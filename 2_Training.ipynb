{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computer Vision Nanodegree\n",
    "\n",
    "## Project: Image Captioning\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, you will train your CNN-RNN model.  \n",
    "\n",
    "You are welcome and encouraged to try out many different architectures and hyperparameters when searching for a good model.\n",
    "\n",
    "This does have the potential to make the project quite messy!  Before submitting your project, make sure that you clean up:\n",
    "- the code you write in this notebook.  The notebook should describe how to train a single CNN-RNN architecture, corresponding to your final choice of hyperparameters.  You should structure the notebook so that the reviewer can replicate your results by running the code in this notebook.  \n",
    "- the output of the code cell in **Step 2**.  The output should show the output obtained when training the model from scratch.\n",
    "\n",
    "This notebook **will be graded**.  \n",
    "\n",
    "Feel free to use the links below to navigate the notebook:\n",
    "- [Step 1](#step1): Training Setup\n",
    "- [Step 2](#step2): Train your Model\n",
    "- [Step 3](#step3): (Optional) Validate your Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step1'></a>\n",
    "## Step 1: Training Setup\n",
    "\n",
    "In this step of the notebook, you will customize the training of your CNN-RNN model by specifying hyperparameters and setting other options that are important to the training procedure.  The values you set now will be used when training your model in **Step 2** below.\n",
    "\n",
    "You should only amend blocks of code that are preceded by a `TODO` statement.  **Any code blocks that are not preceded by a `TODO` statement should not be modified**.\n",
    "\n",
    "### Task #1\n",
    "\n",
    "Begin by setting the following variables:\n",
    "- `batch_size` - the batch size of each training batch.  It is the number of image-caption pairs used to amend the model weights in each training step. \n",
    "- `vocab_threshold` - the minimum word count threshold.  Note that a larger threshold will result in a smaller vocabulary, whereas a smaller threshold will include rarer words and result in a larger vocabulary.  \n",
    "- `vocab_from_file` - a Boolean that decides whether to load the vocabulary from file. \n",
    "- `embed_size` - the dimensionality of the image and word embeddings.  \n",
    "- `hidden_size` - the number of features in the hidden state of the RNN decoder.  \n",
    "- `num_epochs` - the number of epochs to train the model.  We recommend that you set `num_epochs=3`, but feel free to increase or decrease this number as you wish.  [This paper](https://arxiv.org/pdf/1502.03044.pdf) trained a captioning model on a single state-of-the-art GPU for 3 days, but you'll soon see that you can get reasonable results in a matter of a few hours!  (_But of course, if you want your model to compete with current research, you will have to train for much longer._)\n",
    "- `save_every` - determines how often to save the model weights.  We recommend that you set `save_every=1`, to save the model weights after each epoch.  This way, after the `i`th epoch, the encoder and decoder weights will be saved in the `models/` folder as `encoder-i.pkl` and `decoder-i.pkl`, respectively.\n",
    "- `print_every` - determines how often to print the batch loss to the Jupyter notebook while training.  Note that you **will not** observe a monotonic decrease in the loss function while training - this is perfectly fine and completely expected!  You are encouraged to keep this at its default value of `100` to avoid clogging the notebook, but feel free to change it.\n",
    "- `log_file` - the name of the text file containing - for every step - how the loss and perplexity evolved during training.\n",
    "\n",
    "If you're not sure where to begin to set some of the values above, you can peruse [this paper](https://arxiv.org/pdf/1502.03044.pdf) and [this paper](https://arxiv.org/pdf/1411.4555.pdf) for useful guidance!  **To avoid spending too long on this notebook**, you are encouraged to consult these suggested research papers to obtain a strong initial guess for which hyperparameters are likely to work best.  Then, train a single model, and proceed to the next notebook (**3_Inference.ipynb**).  If you are unhappy with your performance, you can return to this notebook to tweak the hyperparameters (and/or the architecture in **model.py**) and re-train your model.\n",
    "\n",
    "### Question 1\n",
    "\n",
    "**Question:** Describe your CNN-RNN architecture in detail.  With this architecture in mind, how did you select the values of the variables in Task 1?  If you consulted a research paper detailing a successful implementation of an image captioning model, please provide the reference.\n",
    "\n",
    "**Answer:** \n",
    "\n",
    "\n",
    "### (Optional) Task #2\n",
    "\n",
    "Note that we have provided a recommended image transform `transform_train` for pre-processing the training images, but you are welcome (and encouraged!) to modify it as you wish.  When modifying this transform, keep in mind that:\n",
    "- the images in the dataset have varying heights and widths, and \n",
    "- if using a pre-trained model, you must perform the corresponding appropriate normalization.\n",
    "\n",
    "### Question 2\n",
    "\n",
    "**Question:** How did you select the transform in `transform_train`?  If you left the transform at its provided value, why do you think that it is a good choice for your CNN architecture?\n",
    "\n",
    "**Answer:** The default looks reasonable, although I have some doubts as to whether it makes sense to discard the data around the edge in this case. \n",
    "\n",
    "### Task #3\n",
    "\n",
    "Next, you will specify a Python list containing the learnable parameters of the model.  For instance, if you decide to make all weights in the decoder trainable, but only want to train the weights in the embedding layer of the encoder, then you should set `params` to something like:\n",
    "```\n",
    "params = list(decoder.parameters()) + list(encoder.embed.parameters()) \n",
    "```\n",
    "\n",
    "### Question 3\n",
    "\n",
    "**Question:** How did you select the trainable parameters of your architecture?  Why do you think this is a good choice?\n",
    "\n",
    "**Answer:** \n",
    "\n",
    "### Task #4\n",
    "\n",
    "Finally, you will select an [optimizer](http://pytorch.org/docs/master/optim.html#torch.optim.Optimizer).\n",
    "\n",
    "### Question 4\n",
    "\n",
    "**Question:** How did you select the optimizer used to train your model?\n",
    "\n",
    "**Answer:** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/sthenc/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary successfully loaded from vocab.pkl file!\n",
      "loading annotations into memory...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 840/414113 [00:00<00:49, 8396.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done (t=0.53s)\n",
      "creating index...\n",
      "index created!\n",
      "Obtaining caption lengths...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 414113/414113 [00:42<00:00, 9752.18it/s] \n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "import sys\n",
    "sys.path.append('/opt/cocoapi/PythonAPI')\n",
    "from pycocotools.coco import COCO\n",
    "from data_loader import get_loader\n",
    "from model import EncoderCNN, DecoderRNN\n",
    "import math\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "## TODO #1: Select appropriate values for the Python variables below.\n",
    "#batch_size=1\n",
    "batch_size=10\n",
    "#batch_size = 64          # batch size, we have 10GB of GPU memory, let's use it\n",
    "vocab_threshold = 5        # minimum word count threshold\n",
    "vocab_from_file = True    # if True, load existing vocab file\n",
    "embed_size = 512           # dimensionality of image and word embeddings\n",
    "hidden_size = 512          # number of features in hidden state of the RNN decoder\n",
    "num_epochs = 3             # number of training epochs\n",
    "save_every = 1             # determines frequency of saving model weights\n",
    "print_every = 100          # determines window for printing average loss\n",
    "log_file = 'training_log.txt'       # name of file with saved training loss and perplexity\n",
    "\n",
    "# (Optional) TODO #2: Amend the image transform below.\n",
    "transform_train = transforms.Compose([ \n",
    "    transforms.Resize(256),                          # smaller edge of image resized to 256\n",
    "    transforms.RandomCrop(224),                      # get 224x224 crop from random location\n",
    "    transforms.RandomHorizontalFlip(),               # horizontally flip image with probability=0.5\n",
    "    transforms.ToTensor(),                           # convert the PIL Image to a tensor\n",
    "    transforms.Normalize((0.485, 0.456, 0.406),      # normalize image for pre-trained model\n",
    "                         (0.229, 0.224, 0.225))])\n",
    "\n",
    "# Build data loader.\n",
    "data_loader = get_loader(transform=transform_train,\n",
    "                         mode='train',\n",
    "                         batch_size=batch_size,\n",
    "                         vocab_threshold=vocab_threshold,\n",
    "                         vocab_from_file=vocab_from_file)\n",
    "\n",
    "# The size of the vocabulary.\n",
    "vocab_size = len(data_loader.dataset.vocab)\n",
    "\n",
    "# Initialize the encoder and decoder. \n",
    "encoder = EncoderCNN(embed_size)\n",
    "decoder = DecoderRNN(embed_size, hidden_size, vocab_size, max_batch_size=batch_size)\n",
    "\n",
    "# Move models to GPU if CUDA is available. \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "encoder.to(device)\n",
    "decoder.to(device)\n",
    "\n",
    "# Define the loss function. \n",
    "criterion = nn.CrossEntropyLoss().cuda() if torch.cuda.is_available() else nn.CrossEntropyLoss()\n",
    "\n",
    "# TODO #3: Specify the learnable parameters of the model.\n",
    "params = list(decoder.parameters()) + list(encoder.embed.parameters()) + list(encoder.bn.parameters())\n",
    "\n",
    "# TODO #4: Define the optimizer.\n",
    "#optimizer = torch.optim.SGD(params, lr=0.01)\n",
    "\n",
    "# this data is probably pretty sparse, and defaults are probably ok\n",
    "#http://ruder.io/optimizing-gradient-descent/\n",
    "optimizer = torch.optim.Adam(params, lr=0.001)\n",
    "\n",
    "# Set the total number of training steps per epoch.\n",
    "total_step = math.ceil(len(data_loader.dataset.caption_lengths) / data_loader.batch_sampler.batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step2'></a>\n",
    "## Step 2: Train your Model\n",
    "\n",
    "Once you have executed the code cell in **Step 1**, the training procedure below should run without issue.  \n",
    "\n",
    "It is completely fine to leave the code cell below as-is without modifications to train your model.  However, if you would like to modify the code used to train the model below, you must ensure that your changes are easily parsed by your reviewer.  In other words, make sure to provide appropriate comments to describe how your code works!  \n",
    "\n",
    "You may find it useful to load saved weights to resume training.  In that case, note the names of the files containing the encoder and decoder weights that you'd like to load (`encoder_file` and `decoder_file`).  Then you can load the weights by using the lines below:\n",
    "\n",
    "```python\n",
    "# Load pre-trained weights before resuming training.\n",
    "encoder.load_state_dict(torch.load(os.path.join('./models', encoder_file)))\n",
    "decoder.load_state_dict(torch.load(os.path.join('./models', decoder_file)))\n",
    "```\n",
    "\n",
    "While trying out parameters, make sure to take extensive notes and record the settings that you used in your various training runs.  In particular, you don't want to encounter a situation where you've trained a model for several hours but can't remember what settings you used :).\n",
    "\n",
    "### A Note on Tuning Hyperparameters\n",
    "\n",
    "To figure out how well your model is doing, you can look at how the training loss and perplexity evolve during training - and for the purposes of this project, you are encouraged to amend the hyperparameters based on this information.  \n",
    "\n",
    "However, this will not tell you if your model is overfitting to the training data, and, unfortunately, overfitting is a problem that is commonly encountered when training image captioning models.  \n",
    "\n",
    "For this project, you need not worry about overfitting. **This project does not have strict requirements regarding the performance of your model**, and you just need to demonstrate that your model has learned **_something_** when you generate captions on the test data.  For now, we strongly encourage you to train your model for the suggested 3 epochs without worrying about performance; then, you should immediately transition to the next notebook in the sequence (**3_Inference.ipynb**) to see how your model performs on the test data.  If your model needs to be changed, you can come back to this notebook, amend hyperparameters (if necessary), and re-train the model.\n",
    "\n",
    "That said, if you would like to go above and beyond in this project, you can read about some approaches to minimizing overfitting in section 4.3.1 of [this paper](http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7505636).  In the next (optional) step of this notebook, we provide some guidance for assessing the performance on the validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/3], Step [100/41412], Loss: 4.1199, Perplexity: 61.5559\n",
      "Epoch [1/3], Step [200/41412], Loss: 4.0104, Perplexity: 55.16937\n",
      "Epoch [1/3], Step [300/41412], Loss: 3.7798, Perplexity: 43.80658\n",
      "Epoch [1/3], Step [400/41412], Loss: 3.9953, Perplexity: 54.34267\n",
      "Epoch [1/3], Step [500/41412], Loss: 3.6032, Perplexity: 36.7172\n",
      "Epoch [1/3], Step [600/41412], Loss: 4.8716, Perplexity: 130.5340\n",
      "Epoch [1/3], Step [700/41412], Loss: 3.5671, Perplexity: 35.41515\n",
      "Epoch [1/3], Step [800/41412], Loss: 3.5726, Perplexity: 35.6105\n",
      "Epoch [1/3], Step [900/41412], Loss: 3.9666, Perplexity: 52.8025\n",
      "Epoch [1/3], Step [1000/41412], Loss: 3.5417, Perplexity: 34.5268\n",
      "Epoch [1/3], Step [1100/41412], Loss: 3.6389, Perplexity: 38.04975\n",
      "Epoch [1/3], Step [1200/41412], Loss: 2.9625, Perplexity: 19.3459\n",
      "Epoch [1/3], Step [1300/41412], Loss: 2.9654, Perplexity: 19.40297\n",
      "Epoch [1/3], Step [1400/41412], Loss: 3.5232, Perplexity: 33.89393\n",
      "Epoch [1/3], Step [1500/41412], Loss: 3.2550, Perplexity: 25.9184\n",
      "Epoch [1/3], Step [1600/41412], Loss: 3.1236, Perplexity: 22.7276\n",
      "Epoch [1/3], Step [1700/41412], Loss: 2.8026, Perplexity: 16.4870\n",
      "Epoch [1/3], Step [1800/41412], Loss: 2.7602, Perplexity: 15.8033\n",
      "Epoch [1/3], Step [1900/41412], Loss: 3.1752, Perplexity: 23.9321\n",
      "Epoch [1/3], Step [2000/41412], Loss: 2.7721, Perplexity: 15.9921\n",
      "Epoch [1/3], Step [2100/41412], Loss: 3.1717, Perplexity: 23.8469\n",
      "Epoch [1/3], Step [2200/41412], Loss: 2.8856, Perplexity: 17.9150\n",
      "Epoch [1/3], Step [2300/41412], Loss: 3.3970, Perplexity: 29.8742\n",
      "Epoch [1/3], Step [2400/41412], Loss: 3.1620, Perplexity: 23.6178\n",
      "Epoch [1/3], Step [2500/41412], Loss: 3.4151, Perplexity: 30.4207\n",
      "Epoch [1/3], Step [2600/41412], Loss: 2.9578, Perplexity: 19.2562\n",
      "Epoch [1/3], Step [2700/41412], Loss: 3.2145, Perplexity: 24.8897\n",
      "Epoch [1/3], Step [2800/41412], Loss: 2.9182, Perplexity: 18.5074\n",
      "Epoch [1/3], Step [2900/41412], Loss: 3.4081, Perplexity: 30.2078\n",
      "Epoch [1/3], Step [3000/41412], Loss: 2.9055, Perplexity: 18.2743\n",
      "Epoch [1/3], Step [3100/41412], Loss: 3.4447, Perplexity: 31.3328\n",
      "Epoch [1/3], Step [3200/41412], Loss: 4.1910, Perplexity: 66.0901\n",
      "Epoch [1/3], Step [3300/41412], Loss: 2.5917, Perplexity: 13.3525\n",
      "Epoch [1/3], Step [3400/41412], Loss: 3.3230, Perplexity: 27.74324\n",
      "Epoch [1/3], Step [3500/41412], Loss: 2.7786, Perplexity: 16.0969\n",
      "Epoch [1/3], Step [3600/41412], Loss: 3.1761, Perplexity: 23.9524\n",
      "Epoch [1/3], Step [3700/41412], Loss: 2.7140, Perplexity: 15.0894\n",
      "Epoch [1/3], Step [3800/41412], Loss: 3.4839, Perplexity: 32.5853\n",
      "Epoch [1/3], Step [3900/41412], Loss: 2.8368, Perplexity: 17.0604\n",
      "Epoch [1/3], Step [4000/41412], Loss: 2.5705, Perplexity: 13.0728\n",
      "Epoch [1/3], Step [4100/41412], Loss: 2.8014, Perplexity: 16.4683\n",
      "Epoch [1/3], Step [4200/41412], Loss: 2.8329, Perplexity: 16.9940\n",
      "Epoch [1/3], Step [4300/41412], Loss: 2.8316, Perplexity: 16.9723\n",
      "Epoch [1/3], Step [4400/41412], Loss: 2.7057, Perplexity: 14.9649\n",
      "Epoch [1/3], Step [4500/41412], Loss: 2.9838, Perplexity: 19.7620\n",
      "Epoch [1/3], Step [4600/41412], Loss: 2.9331, Perplexity: 18.7864\n",
      "Epoch [1/3], Step [4700/41412], Loss: 2.8878, Perplexity: 17.95376\n",
      "Epoch [1/3], Step [4800/41412], Loss: 3.4740, Perplexity: 32.2662\n",
      "Epoch [1/3], Step [4900/41412], Loss: 2.8375, Perplexity: 17.0732\n",
      "Epoch [1/3], Step [5000/41412], Loss: 2.9322, Perplexity: 18.7691\n",
      "Epoch [1/3], Step [5100/41412], Loss: 2.8640, Perplexity: 17.5316\n",
      "Epoch [1/3], Step [5200/41412], Loss: 2.9268, Perplexity: 18.6670\n",
      "Epoch [1/3], Step [5300/41412], Loss: 3.8758, Perplexity: 48.22137\n",
      "Epoch [1/3], Step [5400/41412], Loss: 3.1078, Perplexity: 22.3709\n",
      "Epoch [1/3], Step [5500/41412], Loss: 3.3080, Perplexity: 27.3295\n",
      "Epoch [1/3], Step [5600/41412], Loss: 2.6604, Perplexity: 14.3016\n",
      "Epoch [1/3], Step [5700/41412], Loss: 3.1034, Perplexity: 22.2735\n",
      "Epoch [1/3], Step [5800/41412], Loss: 3.2131, Perplexity: 24.8558\n",
      "Epoch [1/3], Step [5900/41412], Loss: 2.8203, Perplexity: 16.7823\n",
      "Epoch [1/3], Step [6000/41412], Loss: 3.1739, Perplexity: 23.9009\n",
      "Epoch [1/3], Step [6100/41412], Loss: 2.9981, Perplexity: 20.04801\n",
      "Epoch [1/3], Step [6200/41412], Loss: 2.3047, Perplexity: 10.0215\n",
      "Epoch [1/3], Step [6300/41412], Loss: 3.5392, Perplexity: 34.4409\n",
      "Epoch [1/3], Step [6400/41412], Loss: 3.1713, Perplexity: 23.8385\n",
      "Epoch [1/3], Step [6500/41412], Loss: 2.5097, Perplexity: 12.3009\n",
      "Epoch [1/3], Step [6600/41412], Loss: 2.8266, Perplexity: 16.8883\n",
      "Epoch [1/3], Step [6700/41412], Loss: 2.9847, Perplexity: 19.7811\n",
      "Epoch [1/3], Step [6800/41412], Loss: 2.7425, Perplexity: 15.5255\n",
      "Epoch [1/3], Step [6900/41412], Loss: 3.0025, Perplexity: 20.1358\n",
      "Epoch [1/3], Step [7000/41412], Loss: 2.4173, Perplexity: 11.2160\n",
      "Epoch [1/3], Step [7100/41412], Loss: 2.6876, Perplexity: 14.6970\n",
      "Epoch [1/3], Step [7200/41412], Loss: 2.5107, Perplexity: 12.3140\n",
      "Epoch [1/3], Step [7300/41412], Loss: 2.7101, Perplexity: 15.0313\n",
      "Epoch [1/3], Step [7400/41412], Loss: 2.1694, Perplexity: 8.75313\n",
      "Epoch [1/3], Step [7500/41412], Loss: 2.7039, Perplexity: 14.9385\n",
      "Epoch [1/3], Step [7600/41412], Loss: 3.0734, Perplexity: 21.6158\n",
      "Epoch [1/3], Step [7700/41412], Loss: 2.2622, Perplexity: 9.60397\n",
      "Epoch [1/3], Step [7800/41412], Loss: 2.5189, Perplexity: 12.4152\n",
      "Epoch [1/3], Step [7900/41412], Loss: 2.7465, Perplexity: 15.5886\n",
      "Epoch [1/3], Step [8000/41412], Loss: 2.7977, Perplexity: 16.4068\n",
      "Epoch [1/3], Step [8100/41412], Loss: 3.5643, Perplexity: 35.3131\n",
      "Epoch [1/3], Step [8200/41412], Loss: 2.7194, Perplexity: 15.1719\n",
      "Epoch [1/3], Step [8300/41412], Loss: 2.1660, Perplexity: 8.72344\n",
      "Epoch [1/3], Step [8400/41412], Loss: 2.3171, Perplexity: 10.1465\n",
      "Epoch [1/3], Step [8500/41412], Loss: 2.7153, Perplexity: 15.1088\n",
      "Epoch [1/3], Step [8600/41412], Loss: 2.7040, Perplexity: 14.9399\n",
      "Epoch [1/3], Step [8700/41412], Loss: 2.8544, Perplexity: 17.3644\n",
      "Epoch [1/3], Step [8800/41412], Loss: 3.4701, Perplexity: 32.1415\n",
      "Epoch [1/3], Step [8900/41412], Loss: 2.6366, Perplexity: 13.9657\n",
      "Epoch [1/3], Step [9000/41412], Loss: 2.4532, Perplexity: 11.6255\n",
      "Epoch [1/3], Step [9100/41412], Loss: 3.0567, Perplexity: 21.25800\n",
      "Epoch [1/3], Step [9200/41412], Loss: 3.0336, Perplexity: 20.7727\n",
      "Epoch [1/3], Step [9300/41412], Loss: 2.8942, Perplexity: 18.0687\n",
      "Epoch [1/3], Step [9400/41412], Loss: 2.4260, Perplexity: 11.3133\n",
      "Epoch [1/3], Step [9500/41412], Loss: 2.3431, Perplexity: 10.4137\n",
      "Epoch [1/3], Step [9600/41412], Loss: 2.8409, Perplexity: 17.1312\n",
      "Epoch [1/3], Step [9700/41412], Loss: 2.6955, Perplexity: 14.8130\n",
      "Epoch [1/3], Step [9800/41412], Loss: 2.7184, Perplexity: 15.1558\n",
      "Epoch [1/3], Step [9900/41412], Loss: 2.5475, Perplexity: 12.7747\n",
      "Epoch [1/3], Step [10000/41412], Loss: 2.5192, Perplexity: 12.4186\n",
      "Epoch [1/3], Step [10100/41412], Loss: 3.1147, Perplexity: 22.5256\n",
      "Epoch [1/3], Step [10200/41412], Loss: 2.2325, Perplexity: 9.32322\n",
      "Epoch [1/3], Step [10300/41412], Loss: 2.7451, Perplexity: 15.5661\n",
      "Epoch [1/3], Step [10400/41412], Loss: 2.4712, Perplexity: 11.8368\n",
      "Epoch [1/3], Step [10500/41412], Loss: 3.1837, Perplexity: 24.1364\n",
      "Epoch [1/3], Step [10600/41412], Loss: 2.6607, Perplexity: 14.3059\n",
      "Epoch [1/3], Step [10700/41412], Loss: 2.4926, Perplexity: 12.0932\n",
      "Epoch [1/3], Step [10800/41412], Loss: 3.0072, Perplexity: 20.2311\n",
      "Epoch [1/3], Step [10900/41412], Loss: 3.4828, Perplexity: 32.5522\n",
      "Epoch [1/3], Step [11000/41412], Loss: 2.7108, Perplexity: 15.0416\n",
      "Epoch [1/3], Step [11100/41412], Loss: 2.6795, Perplexity: 14.5782\n",
      "Epoch [1/3], Step [11200/41412], Loss: 2.3912, Perplexity: 10.9271\n",
      "Epoch [1/3], Step [11300/41412], Loss: 2.9206, Perplexity: 18.5527\n",
      "Epoch [1/3], Step [11400/41412], Loss: 3.0696, Perplexity: 21.5334\n",
      "Epoch [1/3], Step [11500/41412], Loss: 2.8107, Perplexity: 16.6215\n",
      "Epoch [1/3], Step [11600/41412], Loss: 2.5027, Perplexity: 12.2151\n",
      "Epoch [1/3], Step [11700/41412], Loss: 3.2063, Perplexity: 24.6876\n",
      "Epoch [1/3], Step [11800/41412], Loss: 2.3577, Perplexity: 10.5668\n",
      "Epoch [1/3], Step [11900/41412], Loss: 2.6242, Perplexity: 13.7937\n",
      "Epoch [1/3], Step [12000/41412], Loss: 2.9514, Perplexity: 19.1331\n",
      "Epoch [1/3], Step [12100/41412], Loss: 1.9997, Perplexity: 7.38674\n",
      "Epoch [1/3], Step [12200/41412], Loss: 2.4609, Perplexity: 11.7155\n",
      "Epoch [1/3], Step [12300/41412], Loss: 2.4067, Perplexity: 11.09710\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/3], Step [12400/41412], Loss: 3.0213, Perplexity: 20.5173\n",
      "Epoch [1/3], Step [12500/41412], Loss: 2.9569, Perplexity: 19.2390\n",
      "Epoch [1/3], Step [12600/41412], Loss: 3.6420, Perplexity: 38.1699\n",
      "Epoch [1/3], Step [12700/41412], Loss: 2.7116, Perplexity: 15.0538\n",
      "Epoch [1/3], Step [12800/41412], Loss: 2.1634, Perplexity: 8.70090\n",
      "Epoch [1/3], Step [12900/41412], Loss: 2.1891, Perplexity: 8.92743\n",
      "Epoch [1/3], Step [13000/41412], Loss: 3.0762, Perplexity: 21.6756\n",
      "Epoch [1/3], Step [13100/41412], Loss: 2.5384, Perplexity: 12.6594\n",
      "Epoch [1/3], Step [13200/41412], Loss: 2.5622, Perplexity: 12.9647\n",
      "Epoch [1/3], Step [13300/41412], Loss: 2.7362, Perplexity: 15.4288\n",
      "Epoch [1/3], Step [13400/41412], Loss: 2.2519, Perplexity: 9.50590\n",
      "Epoch [1/3], Step [13500/41412], Loss: 2.9834, Perplexity: 19.7549\n",
      "Epoch [1/3], Step [13600/41412], Loss: 2.7495, Perplexity: 15.6350\n",
      "Epoch [1/3], Step [13700/41412], Loss: 2.5245, Perplexity: 12.4843\n",
      "Epoch [1/3], Step [13800/41412], Loss: 2.7083, Perplexity: 15.0030\n",
      "Epoch [1/3], Step [13900/41412], Loss: 2.3940, Perplexity: 10.95694\n",
      "Epoch [1/3], Step [14000/41412], Loss: 2.4538, Perplexity: 11.63208\n",
      "Epoch [1/3], Step [14100/41412], Loss: 3.2213, Perplexity: 25.0606\n",
      "Epoch [1/3], Step [14200/41412], Loss: 2.7486, Perplexity: 15.6212\n",
      "Epoch [1/3], Step [14300/41412], Loss: 3.2200, Perplexity: 25.0270\n",
      "Epoch [1/3], Step [14400/41412], Loss: 2.2948, Perplexity: 9.92214\n",
      "Epoch [1/3], Step [14500/41412], Loss: 2.1902, Perplexity: 8.93739\n",
      "Epoch [1/3], Step [14600/41412], Loss: 2.2567, Perplexity: 9.55152\n",
      "Epoch [1/3], Step [14700/41412], Loss: 2.6692, Perplexity: 14.4290\n",
      "Epoch [1/3], Step [14800/41412], Loss: 2.1307, Perplexity: 8.42049\n",
      "Epoch [1/3], Step [14900/41412], Loss: 2.1444, Perplexity: 8.53705\n",
      "Epoch [1/3], Step [15000/41412], Loss: 2.5680, Perplexity: 13.0399\n",
      "Epoch [1/3], Step [15100/41412], Loss: 2.6771, Perplexity: 14.5432\n",
      "Epoch [1/3], Step [15200/41412], Loss: 2.0411, Perplexity: 7.69898\n",
      "Epoch [1/3], Step [15300/41412], Loss: 2.1967, Perplexity: 8.99545\n",
      "Epoch [1/3], Step [15400/41412], Loss: 3.0372, Perplexity: 20.8469\n",
      "Epoch [1/3], Step [15500/41412], Loss: 2.0080, Perplexity: 7.44823\n",
      "Epoch [1/3], Step [15600/41412], Loss: 2.7691, Perplexity: 15.9437\n",
      "Epoch [1/3], Step [15700/41412], Loss: 2.2292, Perplexity: 9.29285\n",
      "Epoch [1/3], Step [15800/41412], Loss: 2.0052, Perplexity: 7.42788\n",
      "Epoch [1/3], Step [15900/41412], Loss: 2.8333, Perplexity: 17.0018\n",
      "Epoch [1/3], Step [16000/41412], Loss: 3.2284, Perplexity: 25.2399\n",
      "Epoch [1/3], Step [16100/41412], Loss: 2.7829, Perplexity: 16.1660\n",
      "Epoch [1/3], Step [16200/41412], Loss: 2.6131, Perplexity: 13.6411\n",
      "Epoch [1/3], Step [16300/41412], Loss: 2.6310, Perplexity: 13.8880\n",
      "Epoch [1/3], Step [16400/41412], Loss: 2.8896, Perplexity: 17.9865\n",
      "Epoch [1/3], Step [16500/41412], Loss: 2.5023, Perplexity: 12.2107\n",
      "Epoch [1/3], Step [16600/41412], Loss: 2.7755, Perplexity: 16.0470\n",
      "Epoch [1/3], Step [16700/41412], Loss: 2.2407, Perplexity: 9.39953\n",
      "Epoch [1/3], Step [16800/41412], Loss: 2.8194, Perplexity: 16.76636\n",
      "Epoch [1/3], Step [16900/41412], Loss: 2.8044, Perplexity: 16.5176\n",
      "Epoch [1/3], Step [17000/41412], Loss: 2.3499, Perplexity: 10.4843\n",
      "Epoch [1/3], Step [17100/41412], Loss: 2.6913, Perplexity: 14.7504\n",
      "Epoch [1/3], Step [17200/41412], Loss: 2.1788, Perplexity: 8.83599\n",
      "Epoch [1/3], Step [17300/41412], Loss: 2.7431, Perplexity: 15.5348\n",
      "Epoch [1/3], Step [17400/41412], Loss: 2.9636, Perplexity: 19.3671\n",
      "Epoch [1/3], Step [17500/41412], Loss: 2.4431, Perplexity: 11.5090\n",
      "Epoch [1/3], Step [17600/41412], Loss: 2.5458, Perplexity: 12.7531\n",
      "Epoch [1/3], Step [17700/41412], Loss: 2.5799, Perplexity: 13.1953\n",
      "Epoch [1/3], Step [17800/41412], Loss: 2.9462, Perplexity: 19.0327\n",
      "Epoch [1/3], Step [17900/41412], Loss: 2.6431, Perplexity: 14.0563\n",
      "Epoch [1/3], Step [18000/41412], Loss: 2.5840, Perplexity: 13.2506\n",
      "Epoch [1/3], Step [18100/41412], Loss: 2.8770, Perplexity: 17.7610\n",
      "Epoch [1/3], Step [18200/41412], Loss: 2.4306, Perplexity: 11.36585\n",
      "Epoch [1/3], Step [18300/41412], Loss: 2.9849, Perplexity: 19.7848\n",
      "Epoch [1/3], Step [18400/41412], Loss: 2.1665, Perplexity: 8.72790\n",
      "Epoch [1/3], Step [18500/41412], Loss: 2.7918, Perplexity: 16.3099\n",
      "Epoch [1/3], Step [18600/41412], Loss: 2.8409, Perplexity: 17.1318\n",
      "Epoch [1/3], Step [18700/41412], Loss: 2.6649, Perplexity: 14.3661\n",
      "Epoch [1/3], Step [18800/41412], Loss: 2.6239, Perplexity: 13.7893\n",
      "Epoch [1/3], Step [18900/41412], Loss: 2.5787, Perplexity: 13.1804\n",
      "Epoch [1/3], Step [19000/41412], Loss: 2.9432, Perplexity: 18.9755\n",
      "Epoch [1/3], Step [19100/41412], Loss: 2.5827, Perplexity: 13.2326\n",
      "Epoch [1/3], Step [19200/41412], Loss: 2.2990, Perplexity: 9.96449\n",
      "Epoch [1/3], Step [19300/41412], Loss: 2.5317, Perplexity: 12.5744\n",
      "Epoch [1/3], Step [19400/41412], Loss: 2.5162, Perplexity: 12.3819\n",
      "Epoch [1/3], Step [19500/41412], Loss: 2.7816, Perplexity: 16.1452\n",
      "Epoch [1/3], Step [19600/41412], Loss: 2.1513, Perplexity: 8.59636\n",
      "Epoch [1/3], Step [19700/41412], Loss: 2.7010, Perplexity: 14.8940\n",
      "Epoch [1/3], Step [19800/41412], Loss: 2.0876, Perplexity: 8.06542\n",
      "Epoch [1/3], Step [19900/41412], Loss: 2.2760, Perplexity: 9.73768\n",
      "Epoch [1/3], Step [20000/41412], Loss: 1.9010, Perplexity: 6.69244\n",
      "Epoch [1/3], Step [20100/41412], Loss: 2.0758, Perplexity: 7.97117\n",
      "Epoch [1/3], Step [20200/41412], Loss: 1.9365, Perplexity: 6.93424\n",
      "Epoch [1/3], Step [20300/41412], Loss: 2.5641, Perplexity: 12.9888\n",
      "Epoch [1/3], Step [20400/41412], Loss: 2.7154, Perplexity: 15.1111\n",
      "Epoch [1/3], Step [20500/41412], Loss: 2.1690, Perplexity: 8.75003\n",
      "Epoch [1/3], Step [20600/41412], Loss: 2.4908, Perplexity: 12.0713\n",
      "Epoch [1/3], Step [20700/41412], Loss: 2.8049, Perplexity: 16.5255\n",
      "Epoch [1/3], Step [20800/41412], Loss: 3.2431, Perplexity: 25.6141\n",
      "Epoch [1/3], Step [20900/41412], Loss: 2.3462, Perplexity: 10.4459\n",
      "Epoch [1/3], Step [21000/41412], Loss: 2.7766, Perplexity: 16.0650\n",
      "Epoch [1/3], Step [21100/41412], Loss: 2.9344, Perplexity: 18.8103\n",
      "Epoch [1/3], Step [21200/41412], Loss: 2.6044, Perplexity: 13.5226\n",
      "Epoch [1/3], Step [21300/41412], Loss: 2.7690, Perplexity: 15.9430\n",
      "Epoch [1/3], Step [21400/41412], Loss: 2.3510, Perplexity: 10.4961\n",
      "Epoch [1/3], Step [21500/41412], Loss: 2.4283, Perplexity: 11.3391\n",
      "Epoch [1/3], Step [21600/41412], Loss: 2.1894, Perplexity: 8.92999\n",
      "Epoch [1/3], Step [21700/41412], Loss: 3.2155, Perplexity: 24.9149\n",
      "Epoch [1/3], Step [21800/41412], Loss: 2.3610, Perplexity: 10.6018\n",
      "Epoch [1/3], Step [21900/41412], Loss: 2.9713, Perplexity: 19.5169\n",
      "Epoch [1/3], Step [22000/41412], Loss: 1.8330, Perplexity: 6.25251\n",
      "Epoch [1/3], Step [22100/41412], Loss: 2.6657, Perplexity: 14.3784\n",
      "Epoch [1/3], Step [22200/41412], Loss: 2.6756, Perplexity: 14.5216\n",
      "Epoch [1/3], Step [22300/41412], Loss: 2.6753, Perplexity: 14.5170\n",
      "Epoch [1/3], Step [22400/41412], Loss: 3.0448, Perplexity: 21.0063\n",
      "Epoch [1/3], Step [22500/41412], Loss: 2.3398, Perplexity: 10.3793\n",
      "Epoch [1/3], Step [22600/41412], Loss: 1.9692, Perplexity: 7.16513\n",
      "Epoch [1/3], Step [22700/41412], Loss: 2.7336, Perplexity: 15.3886\n",
      "Epoch [1/3], Step [22800/41412], Loss: 2.3172, Perplexity: 10.1477\n",
      "Epoch [1/3], Step [22900/41412], Loss: 2.0341, Perplexity: 7.64507\n",
      "Epoch [1/3], Step [23000/41412], Loss: 3.4277, Perplexity: 30.8067\n",
      "Epoch [1/3], Step [23100/41412], Loss: 2.8695, Perplexity: 17.6276\n",
      "Epoch [1/3], Step [23200/41412], Loss: 2.6701, Perplexity: 14.4413\n",
      "Epoch [1/3], Step [23300/41412], Loss: 2.2950, Perplexity: 9.92406\n",
      "Epoch [1/3], Step [23400/41412], Loss: 2.9673, Perplexity: 19.4395\n",
      "Epoch [1/3], Step [23500/41412], Loss: 2.2720, Perplexity: 9.69870\n",
      "Epoch [1/3], Step [23600/41412], Loss: 3.3285, Perplexity: 27.8957\n",
      "Epoch [1/3], Step [23700/41412], Loss: 2.4897, Perplexity: 12.0577\n",
      "Epoch [1/3], Step [23800/41412], Loss: 2.6475, Perplexity: 14.1183\n",
      "Epoch [1/3], Step [23900/41412], Loss: 2.5172, Perplexity: 12.3944\n",
      "Epoch [1/3], Step [24000/41412], Loss: 2.0930, Perplexity: 8.10921\n",
      "Epoch [1/3], Step [24100/41412], Loss: 2.2967, Perplexity: 9.94151\n",
      "Epoch [1/3], Step [24200/41412], Loss: 2.7504, Perplexity: 15.6486\n",
      "Epoch [1/3], Step [24300/41412], Loss: 2.4755, Perplexity: 11.88722\n",
      "Epoch [1/3], Step [24400/41412], Loss: 2.3988, Perplexity: 11.0097\n",
      "Epoch [1/3], Step [24500/41412], Loss: 2.5537, Perplexity: 12.8542\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/3], Step [24600/41412], Loss: 2.6545, Perplexity: 14.2178\n",
      "Epoch [1/3], Step [24700/41412], Loss: 2.7086, Perplexity: 15.0079\n",
      "Epoch [1/3], Step [24800/41412], Loss: 2.6039, Perplexity: 13.5166\n",
      "Epoch [1/3], Step [24900/41412], Loss: 3.0708, Perplexity: 21.5590\n",
      "Epoch [1/3], Step [25000/41412], Loss: 2.9393, Perplexity: 18.9035\n",
      "Epoch [1/3], Step [25100/41412], Loss: 2.4355, Perplexity: 11.4215\n",
      "Epoch [1/3], Step [25200/41412], Loss: 2.4402, Perplexity: 11.4748\n",
      "Epoch [1/3], Step [25300/41412], Loss: 3.2792, Perplexity: 26.5550\n",
      "Epoch [1/3], Step [25400/41412], Loss: 2.4509, Perplexity: 11.5993\n",
      "Epoch [1/3], Step [25500/41412], Loss: 2.9802, Perplexity: 19.6924\n",
      "Epoch [1/3], Step [25600/41412], Loss: 2.5052, Perplexity: 12.2463\n",
      "Epoch [1/3], Step [25700/41412], Loss: 2.8135, Perplexity: 16.6682\n",
      "Epoch [1/3], Step [25800/41412], Loss: 3.0100, Perplexity: 20.2870\n",
      "Epoch [1/3], Step [25900/41412], Loss: 2.3896, Perplexity: 10.9090\n",
      "Epoch [1/3], Step [26000/41412], Loss: 2.5906, Perplexity: 13.3379\n",
      "Epoch [1/3], Step [26100/41412], Loss: 2.9014, Perplexity: 18.1992\n",
      "Epoch [1/3], Step [26200/41412], Loss: 2.5887, Perplexity: 13.3127\n",
      "Epoch [1/3], Step [26300/41412], Loss: 2.9812, Perplexity: 19.7121\n",
      "Epoch [1/3], Step [26400/41412], Loss: 2.5401, Perplexity: 12.6810\n",
      "Epoch [1/3], Step [26500/41412], Loss: 2.5373, Perplexity: 12.6460\n",
      "Epoch [1/3], Step [26600/41412], Loss: 3.0218, Perplexity: 20.5286\n",
      "Epoch [1/3], Step [26700/41412], Loss: 2.6705, Perplexity: 14.4469\n",
      "Epoch [1/3], Step [26800/41412], Loss: 2.5127, Perplexity: 12.3384\n",
      "Epoch [1/3], Step [26900/41412], Loss: 2.8584, Perplexity: 17.4343\n",
      "Epoch [1/3], Step [27000/41412], Loss: 2.1554, Perplexity: 8.63178\n",
      "Epoch [1/3], Step [27100/41412], Loss: 3.0908, Perplexity: 21.9946\n",
      "Epoch [1/3], Step [27200/41412], Loss: 2.5502, Perplexity: 12.8102\n",
      "Epoch [1/3], Step [27300/41412], Loss: 2.7342, Perplexity: 15.3982\n",
      "Epoch [1/3], Step [27400/41412], Loss: 1.9583, Perplexity: 7.08738\n",
      "Epoch [1/3], Step [27500/41412], Loss: 3.0352, Perplexity: 20.8049\n",
      "Epoch [1/3], Step [27600/41412], Loss: 2.6571, Perplexity: 14.2550\n",
      "Epoch [1/3], Step [27700/41412], Loss: 2.2022, Perplexity: 9.04508\n",
      "Epoch [1/3], Step [27800/41412], Loss: 2.5825, Perplexity: 13.2307\n",
      "Epoch [1/3], Step [27900/41412], Loss: 2.3691, Perplexity: 10.6881\n",
      "Epoch [1/3], Step [28000/41412], Loss: 2.8467, Perplexity: 17.2305\n",
      "Epoch [1/3], Step [28100/41412], Loss: 2.4962, Perplexity: 12.1365\n",
      "Epoch [1/3], Step [28200/41412], Loss: 2.4675, Perplexity: 11.7924\n",
      "Epoch [1/3], Step [28300/41412], Loss: 2.7375, Perplexity: 15.4486\n",
      "Epoch [1/3], Step [28400/41412], Loss: 2.6480, Perplexity: 14.1252\n",
      "Epoch [1/3], Step [28500/41412], Loss: 3.0435, Perplexity: 20.9782\n",
      "Epoch [1/3], Step [28600/41412], Loss: 2.3574, Perplexity: 10.5636\n",
      "Epoch [1/3], Step [28700/41412], Loss: 2.3646, Perplexity: 10.6397\n",
      "Epoch [1/3], Step [28800/41412], Loss: 2.5316, Perplexity: 12.5738\n",
      "Epoch [1/3], Step [28900/41412], Loss: 2.4268, Perplexity: 11.3225\n",
      "Epoch [1/3], Step [29000/41412], Loss: 2.2818, Perplexity: 9.79484\n",
      "Epoch [1/3], Step [29100/41412], Loss: 2.3727, Perplexity: 10.7258\n",
      "Epoch [1/3], Step [29200/41412], Loss: 2.6855, Perplexity: 14.6656\n",
      "Epoch [1/3], Step [29300/41412], Loss: 2.5176, Perplexity: 12.3991\n",
      "Epoch [1/3], Step [29400/41412], Loss: 2.8765, Perplexity: 17.7525\n",
      "Epoch [1/3], Step [29500/41412], Loss: 2.8388, Perplexity: 17.0950\n",
      "Epoch [1/3], Step [29600/41412], Loss: 2.7265, Perplexity: 15.2795\n",
      "Epoch [1/3], Step [29700/41412], Loss: 2.6650, Perplexity: 14.3683\n",
      "Epoch [1/3], Step [29800/41412], Loss: 2.5445, Perplexity: 12.7371\n",
      "Epoch [1/3], Step [29900/41412], Loss: 3.5862, Perplexity: 36.0957\n",
      "Epoch [1/3], Step [30000/41412], Loss: 2.7710, Perplexity: 15.9738\n",
      "Epoch [1/3], Step [30100/41412], Loss: 2.2304, Perplexity: 9.30356\n",
      "Epoch [1/3], Step [30200/41412], Loss: 2.0944, Perplexity: 8.12027\n",
      "Epoch [1/3], Step [30300/41412], Loss: 2.6722, Perplexity: 14.4724\n",
      "Epoch [1/3], Step [30400/41412], Loss: 2.7146, Perplexity: 15.0987\n",
      "Epoch [1/3], Step [30500/41412], Loss: 2.2223, Perplexity: 9.22832\n",
      "Epoch [1/3], Step [30600/41412], Loss: 1.9732, Perplexity: 7.19384\n",
      "Epoch [1/3], Step [30700/41412], Loss: 2.7051, Perplexity: 14.9565\n",
      "Epoch [1/3], Step [30800/41412], Loss: 2.3011, Perplexity: 9.98522\n",
      "Epoch [1/3], Step [30900/41412], Loss: 2.7087, Perplexity: 15.0104\n",
      "Epoch [1/3], Step [31000/41412], Loss: 2.4462, Perplexity: 11.5449\n",
      "Epoch [1/3], Step [31100/41412], Loss: 2.2754, Perplexity: 9.73183\n",
      "Epoch [1/3], Step [31200/41412], Loss: 2.2929, Perplexity: 9.90411\n",
      "Epoch [1/3], Step [31300/41412], Loss: 2.0159, Perplexity: 7.50766\n",
      "Epoch [1/3], Step [31400/41412], Loss: 2.2137, Perplexity: 9.14945\n",
      "Epoch [1/3], Step [31500/41412], Loss: 2.6660, Perplexity: 14.3830\n",
      "Epoch [1/3], Step [31600/41412], Loss: 2.4585, Perplexity: 11.6868\n",
      "Epoch [1/3], Step [31700/41412], Loss: 2.8610, Perplexity: 17.4787\n",
      "Epoch [1/3], Step [31800/41412], Loss: 2.1455, Perplexity: 8.54656\n",
      "Epoch [1/3], Step [31900/41412], Loss: 2.7677, Perplexity: 15.9216\n",
      "Epoch [1/3], Step [32000/41412], Loss: 2.8332, Perplexity: 16.9993\n",
      "Epoch [1/3], Step [32100/41412], Loss: 2.7866, Perplexity: 16.2257\n",
      "Epoch [1/3], Step [32200/41412], Loss: 2.8507, Perplexity: 17.3005\n",
      "Epoch [1/3], Step [32300/41412], Loss: 2.6306, Perplexity: 13.8825\n",
      "Epoch [1/3], Step [32400/41412], Loss: 2.5034, Perplexity: 12.2236\n",
      "Epoch [1/3], Step [32500/41412], Loss: 2.1420, Perplexity: 8.51615\n",
      "Epoch [1/3], Step [32600/41412], Loss: 2.7398, Perplexity: 15.4839\n",
      "Epoch [1/3], Step [32700/41412], Loss: 2.8017, Perplexity: 16.4733\n",
      "Epoch [1/3], Step [32800/41412], Loss: 2.4604, Perplexity: 11.7099\n",
      "Epoch [1/3], Step [32900/41412], Loss: 2.5178, Perplexity: 12.4013\n",
      "Epoch [1/3], Step [33000/41412], Loss: 2.4914, Perplexity: 12.0781\n",
      "Epoch [1/3], Step [33100/41412], Loss: 2.8016, Perplexity: 16.4709\n",
      "Epoch [1/3], Step [33200/41412], Loss: 2.3160, Perplexity: 10.1351\n",
      "Epoch [1/3], Step [33300/41412], Loss: 2.4537, Perplexity: 11.6318\n",
      "Epoch [1/3], Step [33400/41412], Loss: 2.4929, Perplexity: 12.0961\n",
      "Epoch [1/3], Step [33500/41412], Loss: 2.6929, Perplexity: 14.7742\n",
      "Epoch [1/3], Step [33600/41412], Loss: 2.0371, Perplexity: 7.66812\n",
      "Epoch [1/3], Step [33700/41412], Loss: 2.7704, Perplexity: 15.9642\n",
      "Epoch [1/3], Step [33800/41412], Loss: 2.2454, Perplexity: 9.44453\n",
      "Epoch [1/3], Step [33900/41412], Loss: 3.1607, Perplexity: 23.5860\n",
      "Epoch [1/3], Step [34000/41412], Loss: 2.1266, Perplexity: 8.38674\n",
      "Epoch [1/3], Step [34100/41412], Loss: 2.3131, Perplexity: 10.1054\n",
      "Epoch [1/3], Step [34200/41412], Loss: 1.8968, Perplexity: 6.66441\n",
      "Epoch [1/3], Step [34300/41412], Loss: 2.5079, Perplexity: 12.2786\n",
      "Epoch [1/3], Step [34400/41412], Loss: 2.6642, Perplexity: 14.3562\n",
      "Epoch [1/3], Step [34500/41412], Loss: 2.6532, Perplexity: 14.1990\n",
      "Epoch [1/3], Step [34600/41412], Loss: 2.7414, Perplexity: 15.5084\n",
      "Epoch [1/3], Step [34700/41412], Loss: 2.5612, Perplexity: 12.9514\n",
      "Epoch [1/3], Step [34800/41412], Loss: 2.6412, Perplexity: 14.0306\n",
      "Epoch [1/3], Step [34900/41412], Loss: 2.0704, Perplexity: 7.92780\n",
      "Epoch [1/3], Step [35000/41412], Loss: 2.0321, Perplexity: 7.63026\n",
      "Epoch [1/3], Step [35100/41412], Loss: 2.1265, Perplexity: 8.38510\n",
      "Epoch [1/3], Step [35200/41412], Loss: 3.3124, Perplexity: 27.4503\n",
      "Epoch [1/3], Step [35300/41412], Loss: 2.6009, Perplexity: 13.4757\n",
      "Epoch [1/3], Step [35400/41412], Loss: 2.7228, Perplexity: 15.2229\n",
      "Epoch [1/3], Step [35500/41412], Loss: 2.2802, Perplexity: 9.77853\n",
      "Epoch [1/3], Step [35600/41412], Loss: 2.5902, Perplexity: 13.3320\n",
      "Epoch [1/3], Step [35700/41412], Loss: 2.4627, Perplexity: 11.7362\n",
      "Epoch [1/3], Step [35800/41412], Loss: 2.1255, Perplexity: 8.37731\n",
      "Epoch [1/3], Step [35900/41412], Loss: 2.5522, Perplexity: 12.8348\n",
      "Epoch [1/3], Step [36000/41412], Loss: 2.6959, Perplexity: 14.8195\n",
      "Epoch [1/3], Step [36100/41412], Loss: 2.6914, Perplexity: 14.7516\n",
      "Epoch [1/3], Step [36200/41412], Loss: 2.5230, Perplexity: 12.4664\n",
      "Epoch [1/3], Step [36300/41412], Loss: 2.5901, Perplexity: 13.3307\n",
      "Epoch [1/3], Step [36400/41412], Loss: 2.4817, Perplexity: 11.9621\n",
      "Epoch [1/3], Step [36500/41412], Loss: 2.5365, Perplexity: 12.6360\n",
      "Epoch [1/3], Step [36600/41412], Loss: 2.1959, Perplexity: 8.98846\n",
      "Epoch [1/3], Step [36700/41412], Loss: 2.9481, Perplexity: 19.0701\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/3], Step [36800/41412], Loss: 2.5705, Perplexity: 13.0721\n",
      "Epoch [1/3], Step [36900/41412], Loss: 2.4800, Perplexity: 11.9407\n",
      "Epoch [1/3], Step [37000/41412], Loss: 2.4006, Perplexity: 11.0296\n",
      "Epoch [1/3], Step [37100/41412], Loss: 2.1900, Perplexity: 8.93483\n",
      "Epoch [1/3], Step [37200/41412], Loss: 2.2991, Perplexity: 9.96507\n",
      "Epoch [1/3], Step [37300/41412], Loss: 2.4462, Perplexity: 11.5440\n",
      "Epoch [1/3], Step [37400/41412], Loss: 2.4770, Perplexity: 11.9055\n",
      "Epoch [1/3], Step [37500/41412], Loss: 2.7599, Perplexity: 15.7988\n",
      "Epoch [1/3], Step [37600/41412], Loss: 3.2251, Perplexity: 25.1564\n",
      "Epoch [1/3], Step [37700/41412], Loss: 2.3283, Perplexity: 10.2601\n",
      "Epoch [1/3], Step [37800/41412], Loss: 2.5658, Perplexity: 13.0109\n",
      "Epoch [1/3], Step [37900/41412], Loss: 2.2910, Perplexity: 9.88537\n",
      "Epoch [1/3], Step [38000/41412], Loss: 3.2854, Perplexity: 26.7194\n",
      "Epoch [1/3], Step [38100/41412], Loss: 2.8204, Perplexity: 16.7834\n",
      "Epoch [1/3], Step [38200/41412], Loss: 2.6301, Perplexity: 13.8753\n",
      "Epoch [1/3], Step [38300/41412], Loss: 2.4630, Perplexity: 11.7404\n",
      "Epoch [1/3], Step [38400/41412], Loss: 2.4755, Perplexity: 11.8876\n",
      "Epoch [1/3], Step [38500/41412], Loss: 3.0229, Perplexity: 20.5515\n",
      "Epoch [1/3], Step [38600/41412], Loss: 2.2682, Perplexity: 9.66185\n",
      "Epoch [1/3], Step [38700/41412], Loss: 2.6878, Perplexity: 14.6994\n",
      "Epoch [1/3], Step [38800/41412], Loss: 2.4580, Perplexity: 11.6809\n",
      "Epoch [1/3], Step [38900/41412], Loss: 3.2285, Perplexity: 25.2410\n",
      "Epoch [1/3], Step [39000/41412], Loss: 2.5490, Perplexity: 12.7941\n",
      "Epoch [1/3], Step [39100/41412], Loss: 3.1383, Perplexity: 23.0656\n",
      "Epoch [1/3], Step [39200/41412], Loss: 2.3228, Perplexity: 10.2046\n",
      "Epoch [1/3], Step [39300/41412], Loss: 2.4719, Perplexity: 11.8453\n",
      "Epoch [1/3], Step [39400/41412], Loss: 1.9509, Perplexity: 7.03479\n",
      "Epoch [1/3], Step [39500/41412], Loss: 2.9553, Perplexity: 19.2073\n",
      "Epoch [1/3], Step [39600/41412], Loss: 2.5058, Perplexity: 12.2531\n",
      "Epoch [1/3], Step [39700/41412], Loss: 2.9095, Perplexity: 18.3473\n",
      "Epoch [1/3], Step [39800/41412], Loss: 2.1531, Perplexity: 8.61160\n",
      "Epoch [1/3], Step [39900/41412], Loss: 2.6209, Perplexity: 13.7476\n",
      "Epoch [1/3], Step [40000/41412], Loss: 2.7172, Perplexity: 15.1379\n",
      "Epoch [1/3], Step [40100/41412], Loss: 2.8678, Perplexity: 17.5974\n",
      "Epoch [1/3], Step [40200/41412], Loss: 2.2656, Perplexity: 9.63644\n",
      "Epoch [1/3], Step [40300/41412], Loss: 2.4406, Perplexity: 11.4794\n",
      "Epoch [1/3], Step [40400/41412], Loss: 2.0407, Perplexity: 7.69582\n",
      "Epoch [1/3], Step [40500/41412], Loss: 2.3853, Perplexity: 10.8624\n",
      "Epoch [1/3], Step [40600/41412], Loss: 2.5330, Perplexity: 12.5918\n",
      "Epoch [1/3], Step [40700/41412], Loss: 2.4552, Perplexity: 11.6493\n",
      "Epoch [1/3], Step [40800/41412], Loss: 3.3035, Perplexity: 27.2071\n",
      "Epoch [1/3], Step [40900/41412], Loss: 2.9561, Perplexity: 19.2221\n",
      "Epoch [1/3], Step [41000/41412], Loss: 2.7503, Perplexity: 15.6480\n",
      "Epoch [1/3], Step [41100/41412], Loss: 2.3874, Perplexity: 10.8854\n",
      "Epoch [1/3], Step [41200/41412], Loss: 2.7260, Perplexity: 15.2723\n",
      "Epoch [1/3], Step [41300/41412], Loss: 2.4050, Perplexity: 11.0779\n",
      "Epoch [1/3], Step [41400/41412], Loss: 2.9911, Perplexity: 19.9067\n",
      "Epoch [2/3], Step [100/41412], Loss: 2.2234, Perplexity: 9.2383731\n",
      "Epoch [2/3], Step [200/41412], Loss: 2.4218, Perplexity: 11.2661\n",
      "Epoch [2/3], Step [300/41412], Loss: 2.4116, Perplexity: 11.1517\n",
      "Epoch [2/3], Step [400/41412], Loss: 2.5999, Perplexity: 13.4622\n",
      "Epoch [2/3], Step [500/41412], Loss: 1.9767, Perplexity: 7.21902\n",
      "Epoch [2/3], Step [600/41412], Loss: 2.7564, Perplexity: 15.7439\n",
      "Epoch [2/3], Step [700/41412], Loss: 2.8369, Perplexity: 17.0632\n",
      "Epoch [2/3], Step [800/41412], Loss: 2.7509, Perplexity: 15.6566\n",
      "Epoch [2/3], Step [900/41412], Loss: 2.7012, Perplexity: 14.8973\n",
      "Epoch [2/3], Step [1000/41412], Loss: 2.3342, Perplexity: 10.3211\n",
      "Epoch [2/3], Step [1100/41412], Loss: 2.9007, Perplexity: 18.1867\n",
      "Epoch [2/3], Step [1200/41412], Loss: 2.2992, Perplexity: 9.96600\n",
      "Epoch [2/3], Step [1300/41412], Loss: 2.7426, Perplexity: 15.5267\n",
      "Epoch [2/3], Step [1400/41412], Loss: 2.5407, Perplexity: 12.6880\n",
      "Epoch [2/3], Step [1500/41412], Loss: 2.5693, Perplexity: 13.0564\n",
      "Epoch [2/3], Step [1600/41412], Loss: 2.7253, Perplexity: 15.2612\n",
      "Epoch [2/3], Step [1700/41412], Loss: 2.2316, Perplexity: 9.31518\n",
      "Epoch [2/3], Step [1800/41412], Loss: 2.7303, Perplexity: 15.3376\n",
      "Epoch [2/3], Step [1900/41412], Loss: 2.5246, Perplexity: 12.4865\n",
      "Epoch [2/3], Step [2000/41412], Loss: 2.6968, Perplexity: 14.8329\n",
      "Epoch [2/3], Step [2100/41412], Loss: 2.5502, Perplexity: 12.8097\n",
      "Epoch [2/3], Step [2200/41412], Loss: 2.2722, Perplexity: 9.70104\n",
      "Epoch [2/3], Step [2300/41412], Loss: 2.4450, Perplexity: 11.5308\n",
      "Epoch [2/3], Step [2400/41412], Loss: 2.4681, Perplexity: 11.7996\n",
      "Epoch [2/3], Step [2500/41412], Loss: 2.2151, Perplexity: 9.16194\n",
      "Epoch [2/3], Step [2600/41412], Loss: 2.1739, Perplexity: 8.79223\n",
      "Epoch [2/3], Step [2700/41412], Loss: 3.3934, Perplexity: 29.7662\n",
      "Epoch [2/3], Step [2800/41412], Loss: 3.0007, Perplexity: 20.0987\n",
      "Epoch [2/3], Step [2900/41412], Loss: 2.9376, Perplexity: 18.8697\n",
      "Epoch [2/3], Step [3000/41412], Loss: 2.8294, Perplexity: 16.9345\n",
      "Epoch [2/3], Step [3100/41412], Loss: 2.2598, Perplexity: 9.58070\n",
      "Epoch [2/3], Step [3200/41412], Loss: 2.4468, Perplexity: 11.5513\n",
      "Epoch [2/3], Step [3300/41412], Loss: 2.1659, Perplexity: 8.72200\n",
      "Epoch [2/3], Step [3400/41412], Loss: 2.4550, Perplexity: 11.6460\n",
      "Epoch [2/3], Step [3500/41412], Loss: 2.9787, Perplexity: 19.6619\n",
      "Epoch [2/3], Step [3600/41412], Loss: 2.7180, Perplexity: 15.1492\n",
      "Epoch [2/3], Step [3700/41412], Loss: 1.8975, Perplexity: 6.66954\n",
      "Epoch [2/3], Step [3800/41412], Loss: 2.5314, Perplexity: 12.5705\n",
      "Epoch [2/3], Step [3900/41412], Loss: 2.8701, Perplexity: 17.6385\n",
      "Epoch [2/3], Step [4000/41412], Loss: 2.1809, Perplexity: 8.85447\n",
      "Epoch [2/3], Step [4100/41412], Loss: 1.9826, Perplexity: 7.26122\n",
      "Epoch [2/3], Step [4200/41412], Loss: 2.4982, Perplexity: 12.1606\n",
      "Epoch [2/3], Step [4300/41412], Loss: 2.5891, Perplexity: 13.3184\n",
      "Epoch [2/3], Step [4400/41412], Loss: 2.7488, Perplexity: 15.6234\n",
      "Epoch [2/3], Step [4500/41412], Loss: 2.6029, Perplexity: 13.5031\n",
      "Epoch [2/3], Step [4600/41412], Loss: 2.0995, Perplexity: 8.16231\n",
      "Epoch [2/3], Step [4700/41412], Loss: 2.6153, Perplexity: 13.6713\n",
      "Epoch [2/3], Step [4800/41412], Loss: 2.3650, Perplexity: 10.6435\n",
      "Epoch [2/3], Step [4900/41412], Loss: 2.2717, Perplexity: 9.69577\n",
      "Epoch [2/3], Step [5000/41412], Loss: 2.3668, Perplexity: 10.6635\n",
      "Epoch [2/3], Step [5100/41412], Loss: 2.4006, Perplexity: 11.0293\n",
      "Epoch [2/3], Step [5200/41412], Loss: 2.0917, Perplexity: 8.09877\n",
      "Epoch [2/3], Step [5300/41412], Loss: 2.9763, Perplexity: 19.6153\n",
      "Epoch [2/3], Step [5400/41412], Loss: 2.4167, Perplexity: 11.2087\n",
      "Epoch [2/3], Step [5500/41412], Loss: 2.5367, Perplexity: 12.6374\n",
      "Epoch [2/3], Step [5600/41412], Loss: 2.4181, Perplexity: 11.2250\n",
      "Epoch [2/3], Step [5700/41412], Loss: 2.4689, Perplexity: 11.8093\n",
      "Epoch [2/3], Step [5800/41412], Loss: 2.5513, Perplexity: 12.8236\n",
      "Epoch [2/3], Step [5900/41412], Loss: 3.1410, Perplexity: 23.1271\n",
      "Epoch [2/3], Step [6000/41412], Loss: 2.3552, Perplexity: 10.5399\n",
      "Epoch [2/3], Step [6100/41412], Loss: 3.2422, Perplexity: 25.5912\n",
      "Epoch [2/3], Step [6200/41412], Loss: 2.3283, Perplexity: 10.2605\n",
      "Epoch [2/3], Step [6300/41412], Loss: 2.3269, Perplexity: 10.2462\n",
      "Epoch [2/3], Step [6400/41412], Loss: 2.8468, Perplexity: 17.2322\n",
      "Epoch [2/3], Step [6500/41412], Loss: 2.1370, Perplexity: 8.47427\n",
      "Epoch [2/3], Step [6600/41412], Loss: 2.5537, Perplexity: 12.8550\n",
      "Epoch [2/3], Step [6700/41412], Loss: 2.4451, Perplexity: 11.5317\n",
      "Epoch [2/3], Step [6800/41412], Loss: 2.6746, Perplexity: 14.5071\n",
      "Epoch [2/3], Step [6900/41412], Loss: 2.6503, Perplexity: 14.1582\n",
      "Epoch [2/3], Step [7000/41412], Loss: 2.4392, Perplexity: 11.4636\n",
      "Epoch [2/3], Step [7100/41412], Loss: 2.4433, Perplexity: 11.5110\n",
      "Epoch [2/3], Step [7200/41412], Loss: 2.8564, Perplexity: 17.3996\n",
      "Epoch [2/3], Step [7300/41412], Loss: 2.1679, Perplexity: 8.73961\n",
      "Epoch [2/3], Step [7400/41412], Loss: 2.5839, Perplexity: 13.2493\n",
      "Epoch [2/3], Step [7500/41412], Loss: 2.8840, Perplexity: 17.8853\n",
      "Epoch [2/3], Step [7600/41412], Loss: 2.7948, Perplexity: 16.3594\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/3], Step [7700/41412], Loss: 2.0050, Perplexity: 7.42594\n",
      "Epoch [2/3], Step [7800/41412], Loss: 1.8784, Perplexity: 6.54291\n",
      "Epoch [2/3], Step [7900/41412], Loss: 2.7241, Perplexity: 15.2434\n",
      "Epoch [2/3], Step [8000/41412], Loss: 2.4122, Perplexity: 11.1587\n",
      "Epoch [2/3], Step [8100/41412], Loss: 2.5814, Perplexity: 13.2153\n",
      "Epoch [2/3], Step [8200/41412], Loss: 2.5858, Perplexity: 13.2743\n",
      "Epoch [2/3], Step [8300/41412], Loss: 2.3871, Perplexity: 10.8822\n",
      "Epoch [2/3], Step [8400/41412], Loss: 2.7978, Perplexity: 16.4085\n",
      "Epoch [2/3], Step [8500/41412], Loss: 2.5333, Perplexity: 12.5947\n",
      "Epoch [2/3], Step [8600/41412], Loss: 3.2581, Perplexity: 25.99952\n",
      "Epoch [2/3], Step [8700/41412], Loss: 2.3805, Perplexity: 10.8101\n",
      "Epoch [2/3], Step [8800/41412], Loss: 2.3322, Perplexity: 10.3004\n",
      "Epoch [2/3], Step [8900/41412], Loss: 2.4976, Perplexity: 12.1531\n",
      "Epoch [2/3], Step [9000/41412], Loss: 1.9522, Perplexity: 7.04400\n",
      "Epoch [2/3], Step [9100/41412], Loss: 2.2693, Perplexity: 9.672486\n",
      "Epoch [2/3], Step [9200/41412], Loss: 1.9943, Perplexity: 7.34705\n",
      "Epoch [2/3], Step [9300/41412], Loss: 2.3673, Perplexity: 10.6685\n",
      "Epoch [2/3], Step [9400/41412], Loss: 2.2024, Perplexity: 9.04679\n",
      "Epoch [2/3], Step [9500/41412], Loss: 5.2295, Perplexity: 186.6991\n",
      "Epoch [2/3], Step [9600/41412], Loss: 2.7319, Perplexity: 15.3622\n",
      "Epoch [2/3], Step [9700/41412], Loss: 1.8750, Perplexity: 6.52061\n",
      "Epoch [2/3], Step [9800/41412], Loss: 2.9239, Perplexity: 18.6132\n",
      "Epoch [2/3], Step [9900/41412], Loss: 2.6541, Perplexity: 14.2126\n",
      "Epoch [2/3], Step [10000/41412], Loss: 1.9833, Perplexity: 7.2669\n",
      "Epoch [2/3], Step [10100/41412], Loss: 2.7893, Perplexity: 16.2694\n",
      "Epoch [2/3], Step [10200/41412], Loss: 2.4859, Perplexity: 12.0123\n",
      "Epoch [2/3], Step [10300/41412], Loss: 2.3396, Perplexity: 10.3773\n",
      "Epoch [2/3], Step [10400/41412], Loss: 2.0721, Perplexity: 7.94148\n",
      "Epoch [2/3], Step [10500/41412], Loss: 2.8185, Perplexity: 16.7520\n",
      "Epoch [2/3], Step [10600/41412], Loss: 2.3268, Perplexity: 10.2451\n",
      "Epoch [2/3], Step [10700/41412], Loss: 3.1785, Perplexity: 24.0107\n",
      "Epoch [2/3], Step [10800/41412], Loss: 2.4542, Perplexity: 11.6371\n",
      "Epoch [2/3], Step [10900/41412], Loss: 2.1925, Perplexity: 8.95799\n",
      "Epoch [2/3], Step [11000/41412], Loss: 2.4268, Perplexity: 11.3221\n",
      "Epoch [2/3], Step [11100/41412], Loss: 2.1972, Perplexity: 8.99968\n",
      "Epoch [2/3], Step [11200/41412], Loss: 2.0115, Perplexity: 7.47472\n",
      "Epoch [2/3], Step [11300/41412], Loss: 2.3158, Perplexity: 10.1334\n",
      "Epoch [2/3], Step [11400/41412], Loss: 2.6148, Perplexity: 13.6642\n",
      "Epoch [2/3], Step [11500/41412], Loss: 1.9418, Perplexity: 6.97136\n",
      "Epoch [2/3], Step [11600/41412], Loss: 2.8053, Perplexity: 16.5313\n",
      "Epoch [2/3], Step [11700/41412], Loss: 2.0369, Perplexity: 7.66695\n",
      "Epoch [2/3], Step [11800/41412], Loss: 2.1777, Perplexity: 8.82566\n",
      "Epoch [2/3], Step [11900/41412], Loss: 2.3119, Perplexity: 10.0938\n",
      "Epoch [2/3], Step [12000/41412], Loss: 2.6412, Perplexity: 14.0294\n",
      "Epoch [2/3], Step [12100/41412], Loss: 2.3567, Perplexity: 10.5561\n",
      "Epoch [2/3], Step [12200/41412], Loss: 2.1153, Perplexity: 8.29191\n",
      "Epoch [2/3], Step [12300/41412], Loss: 2.1179, Perplexity: 8.31355\n",
      "Epoch [2/3], Step [12400/41412], Loss: 2.5106, Perplexity: 12.3126\n",
      "Epoch [2/3], Step [12500/41412], Loss: 2.0839, Perplexity: 8.03598\n",
      "Epoch [2/3], Step [12600/41412], Loss: 2.3712, Perplexity: 10.7107\n",
      "Epoch [2/3], Step [12700/41412], Loss: 2.6500, Perplexity: 14.1541\n",
      "Epoch [2/3], Step [12800/41412], Loss: 2.3522, Perplexity: 10.5091\n",
      "Epoch [2/3], Step [12900/41412], Loss: 2.1696, Perplexity: 8.75519\n",
      "Epoch [2/3], Step [13000/41412], Loss: 2.3175, Perplexity: 10.1500\n",
      "Epoch [2/3], Step [13100/41412], Loss: 2.7384, Perplexity: 15.4621\n",
      "Epoch [2/3], Step [13200/41412], Loss: 2.3077, Perplexity: 10.0511\n",
      "Epoch [2/3], Step [13300/41412], Loss: 2.0761, Perplexity: 7.97343\n",
      "Epoch [2/3], Step [13400/41412], Loss: 2.5382, Perplexity: 12.6567\n",
      "Epoch [2/3], Step [13500/41412], Loss: 2.8268, Perplexity: 16.8911\n",
      "Epoch [2/3], Step [13600/41412], Loss: 2.3432, Perplexity: 10.4142\n",
      "Epoch [2/3], Step [13700/41412], Loss: 2.6366, Perplexity: 13.9651\n",
      "Epoch [2/3], Step [13800/41412], Loss: 2.2630, Perplexity: 9.61151\n",
      "Epoch [2/3], Step [13900/41412], Loss: 2.8208, Perplexity: 16.7899\n",
      "Epoch [2/3], Step [14000/41412], Loss: 2.6074, Perplexity: 13.5632\n",
      "Epoch [2/3], Step [14100/41412], Loss: 2.7678, Perplexity: 15.9241\n",
      "Epoch [2/3], Step [14200/41412], Loss: 2.6059, Perplexity: 13.5437\n",
      "Epoch [2/3], Step [14300/41412], Loss: 1.8854, Perplexity: 6.58917\n",
      "Epoch [2/3], Step [14400/41412], Loss: 2.0544, Perplexity: 7.80245\n",
      "Epoch [2/3], Step [14500/41412], Loss: 2.0904, Perplexity: 8.08839\n",
      "Epoch [2/3], Step [14600/41412], Loss: 2.6232, Perplexity: 13.7801\n",
      "Epoch [2/3], Step [14700/41412], Loss: 2.3422, Perplexity: 10.4037\n",
      "Epoch [2/3], Step [14800/41412], Loss: 1.9809, Perplexity: 7.24928\n",
      "Epoch [2/3], Step [14900/41412], Loss: 2.3909, Perplexity: 10.9233\n",
      "Epoch [2/3], Step [15000/41412], Loss: 3.0954, Perplexity: 22.0954\n",
      "Epoch [2/3], Step [15100/41412], Loss: 2.6502, Perplexity: 14.1569\n",
      "Epoch [2/3], Step [15200/41412], Loss: 2.1549, Perplexity: 8.62702\n",
      "Epoch [2/3], Step [15300/41412], Loss: 2.3217, Perplexity: 10.1934\n",
      "Epoch [2/3], Step [15400/41412], Loss: 2.2066, Perplexity: 9.08453\n",
      "Epoch [2/3], Step [15500/41412], Loss: 2.3474, Perplexity: 10.4587\n",
      "Epoch [2/3], Step [15600/41412], Loss: 2.6293, Perplexity: 13.8639\n",
      "Epoch [2/3], Step [15700/41412], Loss: 2.3730, Perplexity: 10.7296\n",
      "Epoch [2/3], Step [15800/41412], Loss: 2.6590, Perplexity: 14.2815\n",
      "Epoch [2/3], Step [15900/41412], Loss: 2.9265, Perplexity: 18.6614\n",
      "Epoch [2/3], Step [16000/41412], Loss: 2.8315, Perplexity: 16.9711\n",
      "Epoch [2/3], Step [16100/41412], Loss: 2.3075, Perplexity: 10.0496\n",
      "Epoch [2/3], Step [16200/41412], Loss: 2.2652, Perplexity: 9.63341\n",
      "Epoch [2/3], Step [16300/41412], Loss: 2.4695, Perplexity: 11.8171\n",
      "Epoch [2/3], Step [16400/41412], Loss: 2.2855, Perplexity: 9.83024\n",
      "Epoch [2/3], Step [16500/41412], Loss: 2.1932, Perplexity: 8.96412\n",
      "Epoch [2/3], Step [16600/41412], Loss: 2.1260, Perplexity: 8.38170\n",
      "Epoch [2/3], Step [16700/41412], Loss: 2.0821, Perplexity: 8.021595\n",
      "Epoch [2/3], Step [16800/41412], Loss: 2.6893, Perplexity: 14.7219\n",
      "Epoch [2/3], Step [16900/41412], Loss: 1.8173, Perplexity: 6.15495\n",
      "Epoch [2/3], Step [17000/41412], Loss: 2.7789, Perplexity: 16.1009\n",
      "Epoch [2/3], Step [17100/41412], Loss: 2.6517, Perplexity: 14.1780\n",
      "Epoch [2/3], Step [17200/41412], Loss: 2.7449, Perplexity: 15.5631\n",
      "Epoch [2/3], Step [17300/41412], Loss: 2.3640, Perplexity: 10.6335\n",
      "Epoch [2/3], Step [17400/41412], Loss: 2.7107, Perplexity: 15.0404\n",
      "Epoch [2/3], Step [17500/41412], Loss: 2.4088, Perplexity: 11.1204\n",
      "Epoch [2/3], Step [17600/41412], Loss: 2.2265, Perplexity: 9.26712\n",
      "Epoch [2/3], Step [17700/41412], Loss: 2.9111, Perplexity: 18.3779\n",
      "Epoch [2/3], Step [17800/41412], Loss: 2.4539, Perplexity: 11.6338\n",
      "Epoch [2/3], Step [17900/41412], Loss: 2.2063, Perplexity: 9.08175\n",
      "Epoch [2/3], Step [18000/41412], Loss: 3.0337, Perplexity: 20.7748\n",
      "Epoch [2/3], Step [18100/41412], Loss: 2.8989, Perplexity: 18.1546\n",
      "Epoch [2/3], Step [18200/41412], Loss: 2.9124, Perplexity: 18.4009\n",
      "Epoch [2/3], Step [18300/41412], Loss: 2.7141, Perplexity: 15.0904\n",
      "Epoch [2/3], Step [18400/41412], Loss: 2.1846, Perplexity: 8.88674\n",
      "Epoch [2/3], Step [18500/41412], Loss: 2.4784, Perplexity: 11.92275\n",
      "Epoch [2/3], Step [18600/41412], Loss: 2.1865, Perplexity: 8.90394\n",
      "Epoch [2/3], Step [18700/41412], Loss: 2.5342, Perplexity: 12.6068\n",
      "Epoch [2/3], Step [18800/41412], Loss: 2.2699, Perplexity: 9.67805\n",
      "Epoch [2/3], Step [18900/41412], Loss: 2.5620, Perplexity: 12.9614\n",
      "Epoch [2/3], Step [19000/41412], Loss: 2.4828, Perplexity: 11.9745\n",
      "Epoch [2/3], Step [19100/41412], Loss: 2.2052, Perplexity: 9.07236\n",
      "Epoch [2/3], Step [19200/41412], Loss: 2.2138, Perplexity: 9.15082\n",
      "Epoch [2/3], Step [19300/41412], Loss: 2.6011, Perplexity: 13.4792\n",
      "Epoch [2/3], Step [19400/41412], Loss: 2.0937, Perplexity: 8.11500\n",
      "Epoch [2/3], Step [19500/41412], Loss: 3.0614, Perplexity: 21.3583\n",
      "Epoch [2/3], Step [19600/41412], Loss: 2.3012, Perplexity: 9.98594\n",
      "Epoch [2/3], Step [19700/41412], Loss: 2.8782, Perplexity: 17.7828\n",
      "Epoch [2/3], Step [19800/41412], Loss: 2.7545, Perplexity: 15.7133\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/3], Step [19900/41412], Loss: 2.4672, Perplexity: 11.7900\n",
      "Epoch [2/3], Step [20000/41412], Loss: 2.1920, Perplexity: 8.95302\n",
      "Epoch [2/3], Step [20100/41412], Loss: 2.5003, Perplexity: 12.1867\n",
      "Epoch [2/3], Step [20200/41412], Loss: 2.3481, Perplexity: 10.4660\n",
      "Epoch [2/3], Step [20300/41412], Loss: 2.4564, Perplexity: 11.6625\n",
      "Epoch [2/3], Step [20400/41412], Loss: 2.3600, Perplexity: 10.5913\n",
      "Epoch [2/3], Step [20500/41412], Loss: 2.1503, Perplexity: 8.58766\n",
      "Epoch [2/3], Step [20600/41412], Loss: 2.1603, Perplexity: 8.67346\n",
      "Epoch [2/3], Step [20700/41412], Loss: 2.5691, Perplexity: 13.0536\n",
      "Epoch [2/3], Step [20800/41412], Loss: 1.9496, Perplexity: 7.02566\n",
      "Epoch [2/3], Step [20900/41412], Loss: 2.1419, Perplexity: 8.51568\n",
      "Epoch [2/3], Step [21000/41412], Loss: 2.4234, Perplexity: 11.2844\n",
      "Epoch [2/3], Step [21100/41412], Loss: 2.6118, Perplexity: 13.6231\n",
      "Epoch [2/3], Step [21200/41412], Loss: 2.8459, Perplexity: 17.2173\n",
      "Epoch [2/3], Step [21300/41412], Loss: 2.4955, Perplexity: 12.1275\n",
      "Epoch [2/3], Step [21400/41412], Loss: 2.1723, Perplexity: 8.77817\n",
      "Epoch [2/3], Step [21500/41412], Loss: 2.2810, Perplexity: 9.78697\n",
      "Epoch [2/3], Step [21600/41412], Loss: 2.1271, Perplexity: 8.39056\n",
      "Epoch [2/3], Step [21700/41412], Loss: 2.3517, Perplexity: 10.5037\n",
      "Epoch [2/3], Step [21800/41412], Loss: 2.3098, Perplexity: 10.0724\n",
      "Epoch [2/3], Step [21900/41412], Loss: 2.9081, Perplexity: 18.3218\n",
      "Epoch [2/3], Step [22000/41412], Loss: 2.2429, Perplexity: 9.42060\n",
      "Epoch [2/3], Step [22100/41412], Loss: 2.6433, Perplexity: 14.0599\n",
      "Epoch [2/3], Step [22200/41412], Loss: 2.2152, Perplexity: 9.16334\n",
      "Epoch [2/3], Step [22300/41412], Loss: 2.4647, Perplexity: 11.7598\n",
      "Epoch [2/3], Step [22400/41412], Loss: 2.4908, Perplexity: 12.0709\n",
      "Epoch [2/3], Step [22500/41412], Loss: 2.6904, Perplexity: 14.7376\n",
      "Epoch [2/3], Step [22600/41412], Loss: 2.6605, Perplexity: 14.3035\n",
      "Epoch [2/3], Step [22700/41412], Loss: 2.5075, Perplexity: 12.2746\n",
      "Epoch [2/3], Step [22800/41412], Loss: 2.3480, Perplexity: 10.4643\n",
      "Epoch [2/3], Step [22900/41412], Loss: 2.1625, Perplexity: 8.69299\n",
      "Epoch [2/3], Step [23000/41412], Loss: 2.8474, Perplexity: 17.2432\n",
      "Epoch [2/3], Step [23100/41412], Loss: 2.2239, Perplexity: 9.24332\n",
      "Epoch [2/3], Step [23200/41412], Loss: 2.4672, Perplexity: 11.7899\n",
      "Epoch [2/3], Step [23300/41412], Loss: 2.3616, Perplexity: 10.6075\n",
      "Epoch [2/3], Step [23400/41412], Loss: 2.5724, Perplexity: 13.0978\n",
      "Epoch [2/3], Step [23500/41412], Loss: 2.2342, Perplexity: 9.33944\n",
      "Epoch [2/3], Step [23600/41412], Loss: 2.7163, Perplexity: 15.1246\n",
      "Epoch [2/3], Step [23700/41412], Loss: 2.3694, Perplexity: 10.6913\n",
      "Epoch [2/3], Step [23800/41412], Loss: 1.8844, Perplexity: 6.58211\n",
      "Epoch [2/3], Step [23900/41412], Loss: 2.2113, Perplexity: 9.12786\n",
      "Epoch [2/3], Step [24000/41412], Loss: 2.5453, Perplexity: 12.7474\n",
      "Epoch [2/3], Step [24100/41412], Loss: 2.0165, Perplexity: 7.51202\n",
      "Epoch [2/3], Step [24200/41412], Loss: 2.5106, Perplexity: 12.3119\n",
      "Epoch [2/3], Step [24300/41412], Loss: 2.6513, Perplexity: 14.1723\n",
      "Epoch [2/3], Step [24400/41412], Loss: 2.0684, Perplexity: 7.91196\n",
      "Epoch [2/3], Step [24500/41412], Loss: 2.4199, Perplexity: 11.2448\n",
      "Epoch [2/3], Step [24600/41412], Loss: 2.2102, Perplexity: 9.11783\n",
      "Epoch [2/3], Step [24700/41412], Loss: 2.0866, Perplexity: 8.05752\n",
      "Epoch [2/3], Step [24800/41412], Loss: 2.6700, Perplexity: 14.44035\n",
      "Epoch [2/3], Step [24900/41412], Loss: 2.6888, Perplexity: 14.7137\n",
      "Epoch [2/3], Step [25000/41412], Loss: 2.4063, Perplexity: 11.0930\n",
      "Epoch [2/3], Step [25100/41412], Loss: 2.4757, Perplexity: 11.8901\n",
      "Epoch [2/3], Step [25200/41412], Loss: 2.3249, Perplexity: 10.2252\n",
      "Epoch [2/3], Step [25300/41412], Loss: 2.3425, Perplexity: 10.4076\n",
      "Epoch [2/3], Step [25400/41412], Loss: 2.9359, Perplexity: 18.8392\n",
      "Epoch [2/3], Step [25500/41412], Loss: 2.4094, Perplexity: 11.1273\n",
      "Epoch [2/3], Step [25600/41412], Loss: 2.3599, Perplexity: 10.58949\n",
      "Epoch [2/3], Step [25700/41412], Loss: 2.4513, Perplexity: 11.6037\n",
      "Epoch [2/3], Step [25800/41412], Loss: 2.3001, Perplexity: 9.97562\n",
      "Epoch [2/3], Step [25900/41412], Loss: 2.5385, Perplexity: 12.6611\n",
      "Epoch [2/3], Step [26000/41412], Loss: 2.7062, Perplexity: 14.9724\n",
      "Epoch [2/3], Step [26100/41412], Loss: 2.3413, Perplexity: 10.3948\n",
      "Epoch [2/3], Step [26200/41412], Loss: 2.3991, Perplexity: 11.0134\n",
      "Epoch [2/3], Step [26300/41412], Loss: 2.2791, Perplexity: 9.76797\n",
      "Epoch [2/3], Step [26400/41412], Loss: 2.4607, Perplexity: 11.7135\n",
      "Epoch [2/3], Step [26500/41412], Loss: 2.4457, Perplexity: 11.5388\n",
      "Epoch [2/3], Step [26600/41412], Loss: 2.6681, Perplexity: 14.4121\n",
      "Epoch [2/3], Step [26700/41412], Loss: 2.3837, Perplexity: 10.8446\n",
      "Epoch [2/3], Step [26800/41412], Loss: 2.4682, Perplexity: 11.8016\n",
      "Epoch [2/3], Step [26900/41412], Loss: 2.3308, Perplexity: 10.2861\n",
      "Epoch [2/3], Step [27000/41412], Loss: 2.2820, Perplexity: 9.79652\n",
      "Epoch [2/3], Step [27100/41412], Loss: 2.7346, Perplexity: 15.4030\n",
      "Epoch [2/3], Step [27200/41412], Loss: 3.3884, Perplexity: 29.6187\n",
      "Epoch [2/3], Step [27300/41412], Loss: 2.6326, Perplexity: 13.9093\n",
      "Epoch [2/3], Step [27400/41412], Loss: 1.9465, Perplexity: 7.00409\n",
      "Epoch [2/3], Step [27500/41412], Loss: 2.2634, Perplexity: 9.61571\n",
      "Epoch [2/3], Step [27600/41412], Loss: 2.7595, Perplexity: 15.7924\n",
      "Epoch [2/3], Step [27700/41412], Loss: 2.1609, Perplexity: 8.678629\n",
      "Epoch [2/3], Step [27800/41412], Loss: 2.3456, Perplexity: 10.4398\n",
      "Epoch [2/3], Step [27900/41412], Loss: 2.2765, Perplexity: 9.74298\n",
      "Epoch [2/3], Step [28000/41412], Loss: 2.2157, Perplexity: 9.16818\n",
      "Epoch [2/3], Step [28100/41412], Loss: 1.9076, Perplexity: 6.73670\n",
      "Epoch [2/3], Step [28200/41412], Loss: 2.0136, Perplexity: 7.49008\n",
      "Epoch [2/3], Step [28300/41412], Loss: 2.2421, Perplexity: 9.41346\n",
      "Epoch [2/3], Step [28400/41412], Loss: 2.4193, Perplexity: 11.2376\n",
      "Epoch [2/3], Step [28500/41412], Loss: 2.7863, Perplexity: 16.2201\n",
      "Epoch [2/3], Step [28600/41412], Loss: 2.6412, Perplexity: 14.0307\n",
      "Epoch [2/3], Step [28700/41412], Loss: 2.9732, Perplexity: 19.5534\n",
      "Epoch [2/3], Step [28800/41412], Loss: 2.8577, Perplexity: 17.4220\n",
      "Epoch [2/3], Step [28900/41412], Loss: 2.1095, Perplexity: 8.24423\n",
      "Epoch [2/3], Step [29000/41412], Loss: 2.2769, Perplexity: 9.74619\n",
      "Epoch [2/3], Step [29100/41412], Loss: 2.5413, Perplexity: 12.6965\n",
      "Epoch [2/3], Step [29200/41412], Loss: 2.4005, Perplexity: 11.0292\n",
      "Epoch [2/3], Step [29300/41412], Loss: 2.5540, Perplexity: 12.8586\n",
      "Epoch [2/3], Step [29400/41412], Loss: 2.7677, Perplexity: 15.9221\n",
      "Epoch [2/3], Step [29500/41412], Loss: 3.2632, Perplexity: 26.1325\n",
      "Epoch [2/3], Step [29600/41412], Loss: 2.0221, Perplexity: 7.55397\n",
      "Epoch [2/3], Step [29700/41412], Loss: 2.2997, Perplexity: 9.97091\n",
      "Epoch [2/3], Step [29800/41412], Loss: 2.5429, Perplexity: 12.7159\n",
      "Epoch [2/3], Step [29900/41412], Loss: 2.8261, Perplexity: 16.8787\n",
      "Epoch [2/3], Step [30000/41412], Loss: 2.2497, Perplexity: 9.48496\n",
      "Epoch [2/3], Step [30100/41412], Loss: 2.7207, Perplexity: 15.1916\n",
      "Epoch [2/3], Step [30200/41412], Loss: 2.3878, Perplexity: 10.8891\n",
      "Epoch [2/3], Step [30300/41412], Loss: 2.7309, Perplexity: 15.3467\n",
      "Epoch [2/3], Step [30400/41412], Loss: 2.7393, Perplexity: 15.4756\n",
      "Epoch [2/3], Step [30500/41412], Loss: 2.0732, Perplexity: 7.95005\n",
      "Epoch [2/3], Step [30600/41412], Loss: 2.1410, Perplexity: 8.50771\n",
      "Epoch [2/3], Step [30700/41412], Loss: 2.2704, Perplexity: 9.68377\n",
      "Epoch [2/3], Step [30800/41412], Loss: 2.4580, Perplexity: 11.6817\n",
      "Epoch [2/3], Step [30900/41412], Loss: 2.0805, Perplexity: 8.00812\n",
      "Epoch [2/3], Step [31000/41412], Loss: 2.2872, Perplexity: 9.84749\n",
      "Epoch [2/3], Step [31100/41412], Loss: 2.2900, Perplexity: 9.87523\n",
      "Epoch [2/3], Step [31200/41412], Loss: 3.0399, Perplexity: 20.9037\n",
      "Epoch [2/3], Step [31300/41412], Loss: 2.0160, Perplexity: 7.50821\n",
      "Epoch [2/3], Step [31400/41412], Loss: 1.9931, Perplexity: 7.33847\n",
      "Epoch [2/3], Step [31500/41412], Loss: 2.4703, Perplexity: 11.8258\n",
      "Epoch [2/3], Step [31600/41412], Loss: 2.8636, Perplexity: 17.5242\n",
      "Epoch [2/3], Step [31700/41412], Loss: 2.6420, Perplexity: 14.0411\n",
      "Epoch [2/3], Step [31800/41412], Loss: 2.1204, Perplexity: 8.33435\n",
      "Epoch [2/3], Step [31900/41412], Loss: 2.3834, Perplexity: 10.8415\n",
      "Epoch [2/3], Step [32000/41412], Loss: 2.5393, Perplexity: 12.6711\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/3], Step [32100/41412], Loss: 2.5849, Perplexity: 13.2621\n",
      "Epoch [2/3], Step [32200/41412], Loss: 2.1596, Perplexity: 8.66731\n",
      "Epoch [2/3], Step [32300/41412], Loss: 2.4787, Perplexity: 11.9255\n",
      "Epoch [2/3], Step [32400/41412], Loss: 2.2770, Perplexity: 9.74759\n",
      "Epoch [2/3], Step [32500/41412], Loss: 2.4780, Perplexity: 11.9174\n",
      "Epoch [2/3], Step [32600/41412], Loss: 2.2098, Perplexity: 9.11428\n",
      "Epoch [2/3], Step [32700/41412], Loss: 2.1896, Perplexity: 8.93174\n",
      "Epoch [2/3], Step [32800/41412], Loss: 2.4040, Perplexity: 11.0673\n",
      "Epoch [2/3], Step [32900/41412], Loss: 2.4530, Perplexity: 11.6231\n",
      "Epoch [2/3], Step [33000/41412], Loss: 2.2674, Perplexity: 9.65451\n",
      "Epoch [2/3], Step [33100/41412], Loss: 2.2728, Perplexity: 9.70642\n",
      "Epoch [2/3], Step [33200/41412], Loss: 2.0403, Perplexity: 7.69301\n",
      "Epoch [2/3], Step [33300/41412], Loss: 2.1289, Perplexity: 8.40572\n",
      "Epoch [2/3], Step [33400/41412], Loss: 2.1568, Perplexity: 8.64303\n",
      "Epoch [2/3], Step [33500/41412], Loss: 2.2815, Perplexity: 9.79150\n",
      "Epoch [2/3], Step [33600/41412], Loss: 2.6749, Perplexity: 14.5107\n",
      "Epoch [2/3], Step [33700/41412], Loss: 2.9597, Perplexity: 19.2930\n",
      "Epoch [2/3], Step [33800/41412], Loss: 2.2991, Perplexity: 9.96484\n",
      "Epoch [2/3], Step [33900/41412], Loss: 2.5593, Perplexity: 12.9268\n",
      "Epoch [2/3], Step [34000/41412], Loss: 1.7727, Perplexity: 5.88671\n",
      "Epoch [2/3], Step [34100/41412], Loss: 2.8560, Perplexity: 17.3915\n",
      "Epoch [2/3], Step [34200/41412], Loss: 2.1982, Perplexity: 9.00900\n",
      "Epoch [2/3], Step [34300/41412], Loss: 2.5768, Perplexity: 13.1548\n",
      "Epoch [2/3], Step [34400/41412], Loss: 2.1330, Perplexity: 8.44021\n",
      "Epoch [2/3], Step [34500/41412], Loss: 2.5927, Perplexity: 13.3652\n",
      "Epoch [2/3], Step [34600/41412], Loss: 1.9061, Perplexity: 6.72707\n",
      "Epoch [2/3], Step [34700/41412], Loss: 2.3011, Perplexity: 9.98541\n",
      "Epoch [2/3], Step [34800/41412], Loss: 2.1816, Perplexity: 8.86083\n",
      "Epoch [2/3], Step [34900/41412], Loss: 2.5827, Perplexity: 13.2324\n",
      "Epoch [2/3], Step [35000/41412], Loss: 2.3662, Perplexity: 10.6563\n",
      "Epoch [2/3], Step [35100/41412], Loss: 2.5548, Perplexity: 12.8689\n",
      "Epoch [2/3], Step [35200/41412], Loss: 2.7316, Perplexity: 15.3569\n",
      "Epoch [2/3], Step [35300/41412], Loss: 2.7776, Perplexity: 16.0805\n",
      "Epoch [2/3], Step [35400/41412], Loss: 2.4325, Perplexity: 11.38682\n",
      "Epoch [2/3], Step [35500/41412], Loss: 1.9668, Perplexity: 7.14808\n",
      "Epoch [2/3], Step [35600/41412], Loss: 2.2544, Perplexity: 9.52959\n",
      "Epoch [2/3], Step [35700/41412], Loss: 2.4089, Perplexity: 11.1214\n",
      "Epoch [2/3], Step [35800/41412], Loss: 2.5497, Perplexity: 12.8032\n",
      "Epoch [2/3], Step [35900/41412], Loss: 2.2925, Perplexity: 9.89929\n",
      "Epoch [2/3], Step [36000/41412], Loss: 2.2224, Perplexity: 9.22948\n",
      "Epoch [2/3], Step [36100/41412], Loss: 2.8855, Perplexity: 17.9129\n",
      "Epoch [2/3], Step [36200/41412], Loss: 2.1604, Perplexity: 8.67422\n",
      "Epoch [2/3], Step [36300/41412], Loss: 3.0821, Perplexity: 21.8039\n",
      "Epoch [2/3], Step [36400/41412], Loss: 2.9601, Perplexity: 19.2992\n",
      "Epoch [2/3], Step [36500/41412], Loss: 2.5898, Perplexity: 13.3268\n",
      "Epoch [2/3], Step [36600/41412], Loss: 2.7971, Perplexity: 16.3973\n",
      "Epoch [2/3], Step [36700/41412], Loss: 1.9412, Perplexity: 6.96702\n",
      "Epoch [2/3], Step [36800/41412], Loss: 2.4450, Perplexity: 11.5311\n",
      "Epoch [2/3], Step [36900/41412], Loss: 2.6659, Perplexity: 14.3812\n",
      "Epoch [2/3], Step [37000/41412], Loss: 1.8937, Perplexity: 6.64407\n",
      "Epoch [2/3], Step [37100/41412], Loss: 2.3484, Perplexity: 10.4685\n",
      "Epoch [2/3], Step [37200/41412], Loss: 1.9527, Perplexity: 7.04745\n",
      "Epoch [2/3], Step [37300/41412], Loss: 2.3055, Perplexity: 10.0289\n",
      "Epoch [2/3], Step [37400/41412], Loss: 2.0234, Perplexity: 7.56376\n",
      "Epoch [2/3], Step [37500/41412], Loss: 2.8829, Perplexity: 17.8653\n",
      "Epoch [2/3], Step [37600/41412], Loss: 2.1263, Perplexity: 8.38353\n",
      "Epoch [2/3], Step [37700/41412], Loss: 2.6040, Perplexity: 13.5174\n",
      "Epoch [2/3], Step [37800/41412], Loss: 2.5512, Perplexity: 12.82302\n",
      "Epoch [2/3], Step [37900/41412], Loss: 2.4105, Perplexity: 11.1398\n",
      "Epoch [2/3], Step [38000/41412], Loss: 2.4116, Perplexity: 11.1516\n",
      "Epoch [2/3], Step [38100/41412], Loss: 2.4169, Perplexity: 11.2110\n",
      "Epoch [2/3], Step [38200/41412], Loss: 2.3084, Perplexity: 10.0583\n",
      "Epoch [2/3], Step [38300/41412], Loss: 2.1518, Perplexity: 8.60015\n",
      "Epoch [2/3], Step [38400/41412], Loss: 2.0360, Perplexity: 7.65999\n",
      "Epoch [2/3], Step [38500/41412], Loss: 2.0675, Perplexity: 7.90505\n",
      "Epoch [2/3], Step [38600/41412], Loss: 2.8449, Perplexity: 17.2004\n",
      "Epoch [2/3], Step [38700/41412], Loss: 2.5717, Perplexity: 13.0881\n",
      "Epoch [2/3], Step [38800/41412], Loss: 2.7638, Perplexity: 15.8599\n",
      "Epoch [2/3], Step [38900/41412], Loss: 2.0901, Perplexity: 8.08541\n",
      "Epoch [2/3], Step [39000/41412], Loss: 2.8501, Perplexity: 17.2890\n",
      "Epoch [2/3], Step [39100/41412], Loss: 1.8992, Perplexity: 6.68043\n",
      "Epoch [2/3], Step [39200/41412], Loss: 2.8353, Perplexity: 17.0350\n",
      "Epoch [2/3], Step [39300/41412], Loss: 2.2122, Perplexity: 9.13546\n",
      "Epoch [2/3], Step [39400/41412], Loss: 2.5088, Perplexity: 12.2906\n",
      "Epoch [2/3], Step [39500/41412], Loss: 2.6680, Perplexity: 14.4114\n",
      "Epoch [2/3], Step [39600/41412], Loss: 2.9701, Perplexity: 19.4939\n",
      "Epoch [2/3], Step [39700/41412], Loss: 3.3503, Perplexity: 28.5116\n",
      "Epoch [2/3], Step [39800/41412], Loss: 2.3964, Perplexity: 10.9837\n",
      "Epoch [2/3], Step [39900/41412], Loss: 2.3691, Perplexity: 10.6878\n",
      "Epoch [2/3], Step [40000/41412], Loss: 2.3366, Perplexity: 10.3463\n",
      "Epoch [2/3], Step [40100/41412], Loss: 2.4498, Perplexity: 11.5858\n",
      "Epoch [2/3], Step [40200/41412], Loss: 2.5857, Perplexity: 13.2720\n",
      "Epoch [2/3], Step [40300/41412], Loss: 2.5558, Perplexity: 12.8821\n",
      "Epoch [2/3], Step [40400/41412], Loss: 2.0099, Perplexity: 7.46234\n",
      "Epoch [2/3], Step [40500/41412], Loss: 2.5233, Perplexity: 12.4694\n",
      "Epoch [2/3], Step [40600/41412], Loss: 2.2455, Perplexity: 9.44534\n",
      "Epoch [2/3], Step [40700/41412], Loss: 2.3152, Perplexity: 10.1273\n",
      "Epoch [2/3], Step [40800/41412], Loss: 2.2383, Perplexity: 9.37776\n",
      "Epoch [2/3], Step [40900/41412], Loss: 2.2684, Perplexity: 9.66406\n",
      "Epoch [2/3], Step [41000/41412], Loss: 2.6043, Perplexity: 13.5216\n",
      "Epoch [2/3], Step [41100/41412], Loss: 2.7757, Perplexity: 16.0495\n",
      "Epoch [2/3], Step [41200/41412], Loss: 2.5225, Perplexity: 12.4596\n",
      "Epoch [2/3], Step [41300/41412], Loss: 2.5724, Perplexity: 13.0974\n",
      "Epoch [2/3], Step [41400/41412], Loss: 2.2528, Perplexity: 9.51392\n",
      "Epoch [3/3], Step [100/41412], Loss: 2.7149, Perplexity: 15.103242\n",
      "Epoch [3/3], Step [200/41412], Loss: 2.2534, Perplexity: 9.51973\n",
      "Epoch [3/3], Step [300/41412], Loss: 2.0527, Perplexity: 7.78912\n",
      "Epoch [3/3], Step [400/41412], Loss: 1.9937, Perplexity: 7.34271\n",
      "Epoch [3/3], Step [500/41412], Loss: 2.4187, Perplexity: 11.2311\n",
      "Epoch [3/3], Step [600/41412], Loss: 1.9863, Perplexity: 7.28844\n",
      "Epoch [3/3], Step [700/41412], Loss: 2.7893, Perplexity: 16.2698\n",
      "Epoch [3/3], Step [800/41412], Loss: 2.2496, Perplexity: 9.48387\n",
      "Epoch [3/3], Step [900/41412], Loss: 2.2322, Perplexity: 9.32051\n",
      "Epoch [3/3], Step [1000/41412], Loss: 2.1993, Perplexity: 9.0189\n",
      "Epoch [3/3], Step [1100/41412], Loss: 1.9934, Perplexity: 7.34018\n",
      "Epoch [3/3], Step [1200/41412], Loss: 2.3897, Perplexity: 10.9102\n",
      "Epoch [3/3], Step [1300/41412], Loss: 2.2554, Perplexity: 9.53941\n",
      "Epoch [3/3], Step [1400/41412], Loss: 3.2880, Perplexity: 26.7889\n",
      "Epoch [3/3], Step [1500/41412], Loss: 2.3409, Perplexity: 10.3903\n",
      "Epoch [3/3], Step [1600/41412], Loss: 2.0109, Perplexity: 7.46992\n",
      "Epoch [3/3], Step [1700/41412], Loss: 2.4674, Perplexity: 11.7914\n",
      "Epoch [3/3], Step [1800/41412], Loss: 1.9171, Perplexity: 6.80129\n",
      "Epoch [3/3], Step [1900/41412], Loss: 2.2775, Perplexity: 9.75248\n",
      "Epoch [3/3], Step [2000/41412], Loss: 2.0516, Perplexity: 7.78020\n",
      "Epoch [3/3], Step [2100/41412], Loss: 2.4086, Perplexity: 11.1182\n",
      "Epoch [3/3], Step [2200/41412], Loss: 2.2512, Perplexity: 9.49938\n",
      "Epoch [3/3], Step [2300/41412], Loss: 2.5404, Perplexity: 12.6847\n",
      "Epoch [3/3], Step [2400/41412], Loss: 2.0014, Perplexity: 7.39960\n",
      "Epoch [3/3], Step [2500/41412], Loss: 2.3536, Perplexity: 10.5236\n",
      "Epoch [3/3], Step [2600/41412], Loss: 2.1300, Perplexity: 8.41489\n",
      "Epoch [3/3], Step [2700/41412], Loss: 2.6944, Perplexity: 14.7966\n",
      "Epoch [3/3], Step [2800/41412], Loss: 2.8642, Perplexity: 17.5355\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/3], Step [2900/41412], Loss: 2.0042, Perplexity: 7.42018\n",
      "Epoch [3/3], Step [3000/41412], Loss: 2.2516, Perplexity: 9.50336\n",
      "Epoch [3/3], Step [3100/41412], Loss: 2.0973, Perplexity: 8.14380\n",
      "Epoch [3/3], Step [3200/41412], Loss: 2.4219, Perplexity: 11.2672\n",
      "Epoch [3/3], Step [3300/41412], Loss: 2.4956, Perplexity: 12.1291\n",
      "Epoch [3/3], Step [3400/41412], Loss: 2.3299, Perplexity: 10.2772\n",
      "Epoch [3/3], Step [3500/41412], Loss: 1.9503, Perplexity: 7.03057\n",
      "Epoch [3/3], Step [3600/41412], Loss: 2.8052, Perplexity: 16.5308\n",
      "Epoch [3/3], Step [3700/41412], Loss: 2.1019, Perplexity: 8.18180\n",
      "Epoch [3/3], Step [3800/41412], Loss: 2.3058, Perplexity: 10.0320\n",
      "Epoch [3/3], Step [3900/41412], Loss: 2.4963, Perplexity: 12.1370\n",
      "Epoch [3/3], Step [4000/41412], Loss: 2.4653, Perplexity: 11.7672\n",
      "Epoch [3/3], Step [4100/41412], Loss: 2.1234, Perplexity: 8.35952\n",
      "Epoch [3/3], Step [4200/41412], Loss: 1.9994, Perplexity: 7.38466\n",
      "Epoch [3/3], Step [4300/41412], Loss: 2.4155, Perplexity: 11.1952\n",
      "Epoch [3/3], Step [4400/41412], Loss: 2.0718, Perplexity: 7.93906\n",
      "Epoch [3/3], Step [4500/41412], Loss: 2.3945, Perplexity: 10.9628\n",
      "Epoch [3/3], Step [4600/41412], Loss: 2.6971, Perplexity: 14.83693\n",
      "Epoch [3/3], Step [4700/41412], Loss: 2.3970, Perplexity: 10.9899\n",
      "Epoch [3/3], Step [4800/41412], Loss: 2.3037, Perplexity: 10.0113\n",
      "Epoch [3/3], Step [4900/41412], Loss: 2.6228, Perplexity: 13.7738\n",
      "Epoch [3/3], Step [5000/41412], Loss: 2.5566, Perplexity: 12.8913\n",
      "Epoch [3/3], Step [5100/41412], Loss: 2.7464, Perplexity: 15.5862\n",
      "Epoch [3/3], Step [5200/41412], Loss: 2.6799, Perplexity: 14.5839\n",
      "Epoch [3/3], Step [5300/41412], Loss: 2.5624, Perplexity: 12.9674\n",
      "Epoch [3/3], Step [5400/41412], Loss: 2.5739, Perplexity: 13.1171\n",
      "Epoch [3/3], Step [5500/41412], Loss: 1.9211, Perplexity: 6.82871\n",
      "Epoch [3/3], Step [5600/41412], Loss: 2.2943, Perplexity: 9.91781\n",
      "Epoch [3/3], Step [5700/41412], Loss: 2.2012, Perplexity: 9.03576\n",
      "Epoch [3/3], Step [5800/41412], Loss: 2.0860, Perplexity: 8.052494\n",
      "Epoch [3/3], Step [5900/41412], Loss: 2.0090, Perplexity: 7.45554\n",
      "Epoch [3/3], Step [6000/41412], Loss: 2.7215, Perplexity: 15.2030\n",
      "Epoch [3/3], Step [6100/41412], Loss: 2.2006, Perplexity: 9.03050\n",
      "Epoch [3/3], Step [6200/41412], Loss: 2.6898, Perplexity: 14.7293\n",
      "Epoch [3/3], Step [6300/41412], Loss: 2.2726, Perplexity: 9.70450\n",
      "Epoch [3/3], Step [6400/41412], Loss: 2.5268, Perplexity: 12.5129\n",
      "Epoch [3/3], Step [6500/41412], Loss: 2.5031, Perplexity: 12.2201\n",
      "Epoch [3/3], Step [6600/41412], Loss: 2.4200, Perplexity: 11.2457\n",
      "Epoch [3/3], Step [6700/41412], Loss: 2.7012, Perplexity: 14.8980\n",
      "Epoch [3/3], Step [6800/41412], Loss: 2.4464, Perplexity: 11.5472\n",
      "Epoch [3/3], Step [6900/41412], Loss: 2.6634, Perplexity: 14.3451\n",
      "Epoch [3/3], Step [7000/41412], Loss: 2.2991, Perplexity: 9.96539\n",
      "Epoch [3/3], Step [7100/41412], Loss: 2.2751, Perplexity: 9.72907\n",
      "Epoch [3/3], Step [7200/41412], Loss: 2.3059, Perplexity: 10.0329\n",
      "Epoch [3/3], Step [7300/41412], Loss: 2.2364, Perplexity: 9.35986\n",
      "Epoch [3/3], Step [7400/41412], Loss: 2.0899, Perplexity: 8.08395\n",
      "Epoch [3/3], Step [7500/41412], Loss: 2.4620, Perplexity: 11.7282\n",
      "Epoch [3/3], Step [7600/41412], Loss: 2.5688, Perplexity: 13.0499\n",
      "Epoch [3/3], Step [7700/41412], Loss: 2.5759, Perplexity: 13.1430\n",
      "Epoch [3/3], Step [7800/41412], Loss: 2.7385, Perplexity: 15.4630\n",
      "Epoch [3/3], Step [7900/41412], Loss: 2.5399, Perplexity: 12.6785\n",
      "Epoch [3/3], Step [8000/41412], Loss: 2.2106, Perplexity: 9.12107\n",
      "Epoch [3/3], Step [8100/41412], Loss: 2.9713, Perplexity: 19.5174\n",
      "Epoch [3/3], Step [8200/41412], Loss: 2.8240, Perplexity: 16.8438\n",
      "Epoch [3/3], Step [8300/41412], Loss: 2.4802, Perplexity: 11.9437\n",
      "Epoch [3/3], Step [8400/41412], Loss: 2.1845, Perplexity: 8.88650\n",
      "Epoch [3/3], Step [8500/41412], Loss: 1.6361, Perplexity: 5.13518\n",
      "Epoch [3/3], Step [8600/41412], Loss: 2.1253, Perplexity: 8.37552\n",
      "Epoch [3/3], Step [8700/41412], Loss: 3.3695, Perplexity: 29.0654\n",
      "Epoch [3/3], Step [8800/41412], Loss: 2.5354, Perplexity: 12.6213\n",
      "Epoch [3/3], Step [8900/41412], Loss: 2.3968, Perplexity: 10.9879\n",
      "Epoch [3/3], Step [9000/41412], Loss: 2.4316, Perplexity: 11.3776\n",
      "Epoch [3/3], Step [9100/41412], Loss: 2.2061, Perplexity: 9.08041\n",
      "Epoch [3/3], Step [9200/41412], Loss: 2.1486, Perplexity: 8.57311\n",
      "Epoch [3/3], Step [9300/41412], Loss: 2.3410, Perplexity: 10.3919\n",
      "Epoch [3/3], Step [9400/41412], Loss: 2.1940, Perplexity: 8.97138\n",
      "Epoch [3/3], Step [9500/41412], Loss: 3.1504, Perplexity: 23.3463\n",
      "Epoch [3/3], Step [9600/41412], Loss: 2.8428, Perplexity: 17.1632\n",
      "Epoch [3/3], Step [9700/41412], Loss: 2.2144, Perplexity: 9.15603\n",
      "Epoch [3/3], Step [9800/41412], Loss: 1.8852, Perplexity: 6.58741\n",
      "Epoch [3/3], Step [9900/41412], Loss: 2.8765, Perplexity: 17.7521\n",
      "Epoch [3/3], Step [10000/41412], Loss: 2.2993, Perplexity: 9.9670\n",
      "Epoch [3/3], Step [10100/41412], Loss: 1.9528, Perplexity: 7.04823\n",
      "Epoch [3/3], Step [10200/41412], Loss: 2.5706, Perplexity: 13.0732\n",
      "Epoch [3/3], Step [10300/41412], Loss: 2.5098, Perplexity: 12.3022\n",
      "Epoch [3/3], Step [10400/41412], Loss: 2.3817, Perplexity: 10.8230\n",
      "Epoch [3/3], Step [10500/41412], Loss: 2.5751, Perplexity: 13.1330\n",
      "Epoch [3/3], Step [10600/41412], Loss: 2.5619, Perplexity: 12.9607\n",
      "Epoch [3/3], Step [10700/41412], Loss: 2.6638, Perplexity: 14.3510\n",
      "Epoch [3/3], Step [10800/41412], Loss: 1.8835, Perplexity: 6.57638\n",
      "Epoch [3/3], Step [10900/41412], Loss: 2.9575, Perplexity: 19.2504\n",
      "Epoch [3/3], Step [11000/41412], Loss: 2.2090, Perplexity: 9.10658\n",
      "Epoch [3/3], Step [11100/41412], Loss: 2.3284, Perplexity: 10.2611\n",
      "Epoch [3/3], Step [11200/41412], Loss: 1.9953, Perplexity: 7.35423\n",
      "Epoch [3/3], Step [11300/41412], Loss: 2.1555, Perplexity: 8.63223\n",
      "Epoch [3/3], Step [11400/41412], Loss: 2.8174, Perplexity: 16.7326\n",
      "Epoch [3/3], Step [11500/41412], Loss: 2.3590, Perplexity: 10.5802\n",
      "Epoch [3/3], Step [11600/41412], Loss: 2.2791, Perplexity: 9.76795\n",
      "Epoch [3/3], Step [11700/41412], Loss: 2.7572, Perplexity: 15.7551\n",
      "Epoch [3/3], Step [11800/41412], Loss: 2.1817, Perplexity: 8.86185\n",
      "Epoch [3/3], Step [11900/41412], Loss: 2.0944, Perplexity: 8.12096\n",
      "Epoch [3/3], Step [12000/41412], Loss: 2.1410, Perplexity: 8.50787\n",
      "Epoch [3/3], Step [12100/41412], Loss: 2.9804, Perplexity: 19.6953\n",
      "Epoch [3/3], Step [12200/41412], Loss: 2.2475, Perplexity: 9.46454\n",
      "Epoch [3/3], Step [12300/41412], Loss: 1.9909, Perplexity: 7.32222\n",
      "Epoch [3/3], Step [12400/41412], Loss: 2.1317, Perplexity: 8.42943\n",
      "Epoch [3/3], Step [12500/41412], Loss: 2.0797, Perplexity: 8.00190\n",
      "Epoch [3/3], Step [12600/41412], Loss: 2.3167, Perplexity: 10.1420\n",
      "Epoch [3/3], Step [12700/41412], Loss: 3.0742, Perplexity: 21.6328\n",
      "Epoch [3/3], Step [12800/41412], Loss: 2.6293, Perplexity: 13.8637\n",
      "Epoch [3/3], Step [12900/41412], Loss: 2.2693, Perplexity: 9.67314\n",
      "Epoch [3/3], Step [13000/41412], Loss: 2.0395, Perplexity: 7.68659\n",
      "Epoch [3/3], Step [13100/41412], Loss: 3.3524, Perplexity: 28.5713\n",
      "Epoch [3/3], Step [13200/41412], Loss: 1.8671, Perplexity: 6.46984\n",
      "Epoch [3/3], Step [13300/41412], Loss: 2.5488, Perplexity: 12.7922\n",
      "Epoch [3/3], Step [13400/41412], Loss: 2.6373, Perplexity: 13.9755\n",
      "Epoch [3/3], Step [13500/41412], Loss: 2.7427, Perplexity: 15.5285\n",
      "Epoch [3/3], Step [13600/41412], Loss: 2.3254, Perplexity: 10.2311\n",
      "Epoch [3/3], Step [13700/41412], Loss: 2.2360, Perplexity: 9.35626\n",
      "Epoch [3/3], Step [13800/41412], Loss: 2.6883, Perplexity: 14.7062\n",
      "Epoch [3/3], Step [13900/41412], Loss: 3.0771, Perplexity: 21.6947\n",
      "Epoch [3/3], Step [14000/41412], Loss: 2.3911, Perplexity: 10.9252\n",
      "Epoch [3/3], Step [14100/41412], Loss: 2.1072, Perplexity: 8.22549\n",
      "Epoch [3/3], Step [14200/41412], Loss: 2.7775, Perplexity: 16.0796\n",
      "Epoch [3/3], Step [14300/41412], Loss: 2.0831, Perplexity: 8.02944\n",
      "Epoch [3/3], Step [14400/41412], Loss: 2.2104, Perplexity: 9.11948\n",
      "Epoch [3/3], Step [14500/41412], Loss: 1.9909, Perplexity: 7.32221\n",
      "Epoch [3/3], Step [14600/41412], Loss: 2.3186, Perplexity: 10.1611\n",
      "Epoch [3/3], Step [14700/41412], Loss: 1.9728, Perplexity: 7.19070\n",
      "Epoch [3/3], Step [14800/41412], Loss: 2.1899, Perplexity: 8.93452\n",
      "Epoch [3/3], Step [14900/41412], Loss: 2.4938, Perplexity: 12.1067\n",
      "Epoch [3/3], Step [15000/41412], Loss: 2.2169, Perplexity: 9.17902\n",
      "Epoch [3/3], Step [15100/41412], Loss: 2.6578, Perplexity: 14.2648\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/3], Step [15200/41412], Loss: 2.4275, Perplexity: 11.3307\n",
      "Epoch [3/3], Step [15300/41412], Loss: 2.2613, Perplexity: 9.59550\n",
      "Epoch [3/3], Step [15400/41412], Loss: 2.3364, Perplexity: 10.3437\n",
      "Epoch [3/3], Step [15500/41412], Loss: 2.3403, Perplexity: 10.3839\n",
      "Epoch [3/3], Step [15600/41412], Loss: 2.6334, Perplexity: 13.9217\n",
      "Epoch [3/3], Step [15700/41412], Loss: 2.3800, Perplexity: 10.8050\n",
      "Epoch [3/3], Step [15800/41412], Loss: 2.2646, Perplexity: 9.62767\n",
      "Epoch [3/3], Step [15900/41412], Loss: 2.3495, Perplexity: 10.4799\n",
      "Epoch [3/3], Step [16000/41412], Loss: 1.8489, Perplexity: 6.35265\n",
      "Epoch [3/3], Step [16100/41412], Loss: 2.1272, Perplexity: 8.39171\n",
      "Epoch [3/3], Step [16200/41412], Loss: 2.0219, Perplexity: 7.55302\n",
      "Epoch [3/3], Step [16300/41412], Loss: 2.7451, Perplexity: 15.5658\n",
      "Epoch [3/3], Step [16400/41412], Loss: 3.0833, Perplexity: 21.8302\n",
      "Epoch [3/3], Step [16500/41412], Loss: 2.2275, Perplexity: 9.27692\n",
      "Epoch [3/3], Step [16600/41412], Loss: 2.4525, Perplexity: 11.6175\n",
      "Epoch [3/3], Step [16700/41412], Loss: 2.0532, Perplexity: 7.79251\n",
      "Epoch [3/3], Step [16800/41412], Loss: 2.6985, Perplexity: 14.8576\n",
      "Epoch [3/3], Step [16900/41412], Loss: 2.2825, Perplexity: 9.80079\n",
      "Epoch [3/3], Step [17000/41412], Loss: 2.0445, Perplexity: 7.72533\n",
      "Epoch [3/3], Step [17100/41412], Loss: 2.3558, Perplexity: 10.5470\n",
      "Epoch [3/3], Step [17200/41412], Loss: 2.4237, Perplexity: 11.28796\n",
      "Epoch [3/3], Step [17300/41412], Loss: 2.5691, Perplexity: 13.0539\n",
      "Epoch [3/3], Step [17400/41412], Loss: 1.8383, Perplexity: 6.28572\n",
      "Epoch [3/3], Step [17500/41412], Loss: 1.9330, Perplexity: 6.91049\n",
      "Epoch [3/3], Step [17600/41412], Loss: 2.1980, Perplexity: 9.00748\n",
      "Epoch [3/3], Step [17700/41412], Loss: 2.3282, Perplexity: 10.2591\n",
      "Epoch [3/3], Step [17800/41412], Loss: 2.4319, Perplexity: 11.3805\n",
      "Epoch [3/3], Step [17900/41412], Loss: 2.3444, Perplexity: 10.4265\n",
      "Epoch [3/3], Step [18000/41412], Loss: 2.3487, Perplexity: 10.4721\n",
      "Epoch [3/3], Step [18100/41412], Loss: 2.3981, Perplexity: 11.0018\n",
      "Epoch [3/3], Step [18200/41412], Loss: 2.7742, Perplexity: 16.0266\n",
      "Epoch [3/3], Step [18300/41412], Loss: 2.2880, Perplexity: 9.85492\n",
      "Epoch [3/3], Step [18400/41412], Loss: 2.3841, Perplexity: 10.8492\n",
      "Epoch [3/3], Step [18500/41412], Loss: 2.4230, Perplexity: 11.2795\n",
      "Epoch [3/3], Step [18600/41412], Loss: 2.1793, Perplexity: 8.83991\n",
      "Epoch [3/3], Step [18700/41412], Loss: 1.9691, Perplexity: 7.16433\n",
      "Epoch [3/3], Step [18800/41412], Loss: 2.2305, Perplexity: 9.30483\n",
      "Epoch [3/3], Step [18900/41412], Loss: 1.8275, Perplexity: 6.21857\n",
      "Epoch [3/3], Step [19000/41412], Loss: 2.4884, Perplexity: 12.0418\n",
      "Epoch [3/3], Step [19100/41412], Loss: 2.1179, Perplexity: 8.31395\n",
      "Epoch [3/3], Step [19200/41412], Loss: 2.5658, Perplexity: 13.0112\n",
      "Epoch [3/3], Step [19300/41412], Loss: 1.8065, Perplexity: 6.08929\n",
      "Epoch [3/3], Step [19400/41412], Loss: 2.1350, Perplexity: 8.45688\n",
      "Epoch [3/3], Step [19500/41412], Loss: 2.7208, Perplexity: 15.1928\n",
      "Epoch [3/3], Step [19600/41412], Loss: 2.4661, Perplexity: 11.7770\n",
      "Epoch [3/3], Step [19700/41412], Loss: 2.4267, Perplexity: 11.3218\n",
      "Epoch [3/3], Step [19800/41412], Loss: 2.3400, Perplexity: 10.3813\n",
      "Epoch [3/3], Step [19900/41412], Loss: 2.0524, Perplexity: 7.78663\n",
      "Epoch [3/3], Step [20000/41412], Loss: 2.4583, Perplexity: 11.6843\n",
      "Epoch [3/3], Step [20100/41412], Loss: 2.4290, Perplexity: 11.3479\n",
      "Epoch [3/3], Step [20200/41412], Loss: 2.6806, Perplexity: 14.5944\n",
      "Epoch [3/3], Step [20300/41412], Loss: 2.6018, Perplexity: 13.4877\n",
      "Epoch [3/3], Step [20400/41412], Loss: 3.2999, Perplexity: 27.1098\n",
      "Epoch [3/3], Step [20500/41412], Loss: 2.6338, Perplexity: 13.9267\n",
      "Epoch [3/3], Step [20600/41412], Loss: 2.1820, Perplexity: 8.86438\n",
      "Epoch [3/3], Step [20700/41412], Loss: 2.3177, Perplexity: 10.1523\n",
      "Epoch [3/3], Step [20800/41412], Loss: 2.1293, Perplexity: 8.40883\n",
      "Epoch [3/3], Step [20900/41412], Loss: 2.4881, Perplexity: 12.0385\n",
      "Epoch [3/3], Step [21000/41412], Loss: 2.5417, Perplexity: 12.7016\n",
      "Epoch [3/3], Step [21100/41412], Loss: 2.1572, Perplexity: 8.64696\n",
      "Epoch [3/3], Step [21200/41412], Loss: 2.2214, Perplexity: 9.22030\n",
      "Epoch [3/3], Step [21300/41412], Loss: 2.7110, Perplexity: 15.0438\n",
      "Epoch [3/3], Step [21400/41412], Loss: 2.1547, Perplexity: 8.62546\n",
      "Epoch [3/3], Step [21500/41412], Loss: 2.8794, Perplexity: 17.8032\n",
      "Epoch [3/3], Step [21600/41412], Loss: 1.8604, Perplexity: 6.42658\n",
      "Epoch [3/3], Step [21700/41412], Loss: 2.5861, Perplexity: 13.2773\n",
      "Epoch [3/3], Step [21800/41412], Loss: 2.2933, Perplexity: 9.90748\n",
      "Epoch [3/3], Step [21900/41412], Loss: 2.2588, Perplexity: 9.57169\n",
      "Epoch [3/3], Step [22000/41412], Loss: 2.4583, Perplexity: 11.6854\n",
      "Epoch [3/3], Step [22100/41412], Loss: 2.6353, Perplexity: 13.9471\n",
      "Epoch [3/3], Step [22200/41412], Loss: 2.2861, Perplexity: 9.83657\n",
      "Epoch [3/3], Step [22300/41412], Loss: 2.3473, Perplexity: 10.4573\n",
      "Epoch [3/3], Step [22400/41412], Loss: 2.5371, Perplexity: 12.6434\n",
      "Epoch [3/3], Step [22500/41412], Loss: 2.1873, Perplexity: 8.91113\n",
      "Epoch [3/3], Step [22600/41412], Loss: 2.2252, Perplexity: 9.25552\n",
      "Epoch [3/3], Step [22700/41412], Loss: 2.7607, Perplexity: 15.8113\n",
      "Epoch [3/3], Step [22800/41412], Loss: 2.5995, Perplexity: 13.4567\n",
      "Epoch [3/3], Step [22900/41412], Loss: 2.0552, Perplexity: 7.80853\n",
      "Epoch [3/3], Step [23000/41412], Loss: 2.1430, Perplexity: 8.52486\n",
      "Epoch [3/3], Step [23100/41412], Loss: 1.9218, Perplexity: 6.83330\n",
      "Epoch [3/3], Step [23200/41412], Loss: 2.8541, Perplexity: 17.3581\n",
      "Epoch [3/3], Step [23300/41412], Loss: 1.7674, Perplexity: 5.85568\n",
      "Epoch [3/3], Step [23400/41412], Loss: 2.7917, Perplexity: 16.3089\n",
      "Epoch [3/3], Step [23500/41412], Loss: 2.3998, Perplexity: 11.0212\n",
      "Epoch [3/3], Step [23600/41412], Loss: 2.3621, Perplexity: 10.6137\n",
      "Epoch [3/3], Step [23700/41412], Loss: 2.8783, Perplexity: 17.7846\n",
      "Epoch [3/3], Step [23800/41412], Loss: 2.1082, Perplexity: 8.23321\n",
      "Epoch [3/3], Step [23900/41412], Loss: 2.2366, Perplexity: 9.36157\n",
      "Epoch [3/3], Step [24000/41412], Loss: 2.2949, Perplexity: 9.92381\n",
      "Epoch [3/3], Step [24100/41412], Loss: 2.4898, Perplexity: 12.0593\n",
      "Epoch [3/3], Step [24200/41412], Loss: 2.1524, Perplexity: 8.60556\n",
      "Epoch [3/3], Step [24300/41412], Loss: 2.1481, Perplexity: 8.56882\n",
      "Epoch [3/3], Step [24400/41412], Loss: 2.1705, Perplexity: 8.76259\n",
      "Epoch [3/3], Step [24500/41412], Loss: 2.0538, Perplexity: 7.79734\n",
      "Epoch [3/3], Step [24600/41412], Loss: 2.0591, Perplexity: 7.83938\n",
      "Epoch [3/3], Step [24700/41412], Loss: 2.2263, Perplexity: 9.26520\n",
      "Epoch [3/3], Step [24800/41412], Loss: 2.0022, Perplexity: 7.40541\n",
      "Epoch [3/3], Step [24900/41412], Loss: 2.2024, Perplexity: 9.04679\n",
      "Epoch [3/3], Step [25000/41412], Loss: 2.0076, Perplexity: 7.44534\n",
      "Epoch [3/3], Step [25100/41412], Loss: 2.5850, Perplexity: 13.2633\n",
      "Epoch [3/3], Step [25200/41412], Loss: 2.1005, Perplexity: 8.17037\n",
      "Epoch [3/3], Step [25300/41412], Loss: 2.5776, Perplexity: 13.1650\n",
      "Epoch [3/3], Step [25400/41412], Loss: 2.7204, Perplexity: 15.1866\n",
      "Epoch [3/3], Step [25500/41412], Loss: 2.2299, Perplexity: 9.29888\n",
      "Epoch [3/3], Step [25600/41412], Loss: 2.2092, Perplexity: 9.10839\n",
      "Epoch [3/3], Step [25700/41412], Loss: 2.5750, Perplexity: 13.1317\n",
      "Epoch [3/3], Step [25800/41412], Loss: 2.2994, Perplexity: 9.96807\n",
      "Epoch [3/3], Step [25900/41412], Loss: 2.4132, Perplexity: 11.1699\n",
      "Epoch [3/3], Step [26000/41412], Loss: 2.1712, Perplexity: 8.76874\n",
      "Epoch [3/3], Step [26100/41412], Loss: 2.9746, Perplexity: 19.5823\n",
      "Epoch [3/3], Step [26200/41412], Loss: 2.2668, Perplexity: 9.64836\n",
      "Epoch [3/3], Step [26300/41412], Loss: 2.2388, Perplexity: 9.38210\n",
      "Epoch [3/3], Step [26400/41412], Loss: 2.1068, Perplexity: 8.22164\n",
      "Epoch [3/3], Step [26500/41412], Loss: 2.4202, Perplexity: 11.2487\n",
      "Epoch [3/3], Step [26600/41412], Loss: 2.0611, Perplexity: 7.85436\n",
      "Epoch [3/3], Step [26700/41412], Loss: 2.1702, Perplexity: 8.76011\n",
      "Epoch [3/3], Step [26800/41412], Loss: 2.3847, Perplexity: 10.8563\n",
      "Epoch [3/3], Step [26900/41412], Loss: 2.2220, Perplexity: 9.22619\n",
      "Epoch [3/3], Step [27000/41412], Loss: 2.1961, Perplexity: 8.98952\n",
      "Epoch [3/3], Step [27100/41412], Loss: 1.9499, Perplexity: 7.02782\n",
      "Epoch [3/3], Step [27200/41412], Loss: 4.4122, Perplexity: 82.4511\n",
      "Epoch [3/3], Step [27300/41412], Loss: 2.0714, Perplexity: 7.93629\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/3], Step [27400/41412], Loss: 2.3858, Perplexity: 10.8674\n",
      "Epoch [3/3], Step [27500/41412], Loss: 2.3058, Perplexity: 10.0318\n",
      "Epoch [3/3], Step [27600/41412], Loss: 2.1298, Perplexity: 8.41286\n",
      "Epoch [3/3], Step [27700/41412], Loss: 2.3734, Perplexity: 10.7337\n",
      "Epoch [3/3], Step [27800/41412], Loss: 2.2689, Perplexity: 9.66910\n",
      "Epoch [3/3], Step [27900/41412], Loss: 2.1011, Perplexity: 8.17552\n",
      "Epoch [3/3], Step [28000/41412], Loss: 2.2919, Perplexity: 9.893800\n",
      "Epoch [3/3], Step [28100/41412], Loss: 1.9505, Perplexity: 7.03218\n",
      "Epoch [3/3], Step [28200/41412], Loss: 2.4301, Perplexity: 11.3605\n",
      "Epoch [3/3], Step [28300/41412], Loss: 2.3121, Perplexity: 10.0957\n",
      "Epoch [3/3], Step [28400/41412], Loss: 2.0329, Perplexity: 7.63631\n",
      "Epoch [3/3], Step [28500/41412], Loss: 2.1794, Perplexity: 8.84140\n",
      "Epoch [3/3], Step [28600/41412], Loss: 3.1177, Perplexity: 22.5933\n",
      "Epoch [3/3], Step [28700/41412], Loss: 2.3492, Perplexity: 10.4769\n",
      "Epoch [3/3], Step [28800/41412], Loss: 2.5085, Perplexity: 12.2868\n",
      "Epoch [3/3], Step [28900/41412], Loss: 2.7344, Perplexity: 15.4006\n",
      "Epoch [3/3], Step [29000/41412], Loss: 2.4418, Perplexity: 11.4941\n",
      "Epoch [3/3], Step [29100/41412], Loss: 2.2775, Perplexity: 9.75215\n",
      "Epoch [3/3], Step [29200/41412], Loss: 2.1568, Perplexity: 8.64347\n",
      "Epoch [3/3], Step [29300/41412], Loss: 2.0867, Perplexity: 8.05837\n",
      "Epoch [3/3], Step [29400/41412], Loss: 2.1899, Perplexity: 8.93417\n",
      "Epoch [3/3], Step [29500/41412], Loss: 2.9842, Perplexity: 19.7714\n",
      "Epoch [3/3], Step [29600/41412], Loss: 2.9025, Perplexity: 18.2187\n",
      "Epoch [3/3], Step [29700/41412], Loss: 2.1112, Perplexity: 8.25818\n",
      "Epoch [3/3], Step [29800/41412], Loss: 2.5317, Perplexity: 12.5754\n",
      "Epoch [3/3], Step [29900/41412], Loss: 2.8713, Perplexity: 17.6600\n",
      "Epoch [3/3], Step [30000/41412], Loss: 2.2302, Perplexity: 9.30143\n",
      "Epoch [3/3], Step [30100/41412], Loss: 2.5908, Perplexity: 13.3409\n",
      "Epoch [3/3], Step [30200/41412], Loss: 2.4052, Perplexity: 11.0807\n",
      "Epoch [3/3], Step [30300/41412], Loss: 2.2733, Perplexity: 9.71096\n",
      "Epoch [3/3], Step [30400/41412], Loss: 2.3592, Perplexity: 10.5828\n",
      "Epoch [3/3], Step [30500/41412], Loss: 2.4054, Perplexity: 11.0825\n",
      "Epoch [3/3], Step [30600/41412], Loss: 2.7420, Perplexity: 15.5174\n",
      "Epoch [3/3], Step [30700/41412], Loss: 2.2187, Perplexity: 9.19539\n",
      "Epoch [3/3], Step [30800/41412], Loss: 2.6632, Perplexity: 14.3421\n",
      "Epoch [3/3], Step [30900/41412], Loss: 2.7587, Perplexity: 15.7792\n",
      "Epoch [3/3], Step [31000/41412], Loss: 1.9904, Perplexity: 7.31829\n",
      "Epoch [3/3], Step [31100/41412], Loss: 2.3755, Perplexity: 10.7565\n",
      "Epoch [3/3], Step [31200/41412], Loss: 1.9893, Perplexity: 7.31040\n",
      "Epoch [3/3], Step [31300/41412], Loss: 2.2610, Perplexity: 9.59234\n",
      "Epoch [3/3], Step [31400/41412], Loss: 2.3650, Perplexity: 10.6443\n",
      "Epoch [3/3], Step [31500/41412], Loss: 2.6322, Perplexity: 13.9046\n",
      "Epoch [3/3], Step [31600/41412], Loss: 1.9562, Perplexity: 7.07219\n",
      "Epoch [3/3], Step [31700/41412], Loss: 2.4701, Perplexity: 11.8236\n",
      "Epoch [3/3], Step [31800/41412], Loss: 1.9926, Perplexity: 7.33496\n",
      "Epoch [3/3], Step [31900/41412], Loss: 2.0423, Perplexity: 7.70863\n",
      "Epoch [3/3], Step [32000/41412], Loss: 2.2138, Perplexity: 9.15083\n",
      "Epoch [3/3], Step [32082/41412], Loss: 2.5000, Perplexity: 12.1829"
     ]
    }
   ],
   "source": [
    "import torch.utils.data as data\n",
    "import numpy as np\n",
    "import os\n",
    "import requests\n",
    "import time\n",
    "\n",
    "# temporary\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from model import EncoderCNN, DecoderRNN\n",
    "\n",
    "# Open the training log file.\n",
    "f = open(log_file, 'w')\n",
    "\n",
    "## running the training locally\n",
    "#old_time = time.time()\n",
    "#response = requests.request(\"GET\", \n",
    "#                            \"http://metadata.google.internal/computeMetadata/v1/instance/attributes/keep_alive_token\", \n",
    "#                            headers={\"Metadata-Flavor\":\"Google\"})\n",
    "\n",
    "\n",
    "for epoch in range(1, num_epochs+1):\n",
    "\n",
    "    for i_step in range(1, total_step+1):\n",
    "        \n",
    "        ## running the training locally\n",
    "        #if time.time() - old_time > 60:\n",
    "        #    old_time = time.time()\n",
    "        #    requests.request(\"POST\", \n",
    "        #                     \"https://nebula.udacity.com/api/v1/remote/keep-alive\", \n",
    "        #                     headers={'Authorization': \"STAR \" + response.text})\n",
    "\n",
    "        # Randomly sample a caption length, and sample indices with that length.\n",
    "        indices = data_loader.dataset.get_train_indices()\n",
    "        # Create and assign a batch sampler to retrieve a batch with the sampled indices.\n",
    "        new_sampler = data.sampler.SubsetRandomSampler(indices=indices)\n",
    "        data_loader.batch_sampler.sampler = new_sampler\n",
    "\n",
    "        # Obtain the batch.\n",
    "        images, captions = next(iter(data_loader))\n",
    "\n",
    "        # Move batch of images and captions to GPU if CUDA is available.\n",
    "        images = images.to(device)\n",
    "        captions = captions.to(device)\n",
    "\n",
    "        # Zero the gradients.\n",
    "        decoder.zero_grad()\n",
    "        encoder.zero_grad()\n",
    "\n",
    "        # Pass the inputs through the CNN-RNN model.\n",
    "        features = encoder(images)\n",
    "        outputs = decoder(features, captions)\n",
    "\n",
    "        # Calculate the batch loss.\n",
    "        loss = criterion(outputs.contiguous().view(-1, vocab_size), captions.contiguous().view(-1))\n",
    "\n",
    "        # Backward pass.\n",
    "        loss.backward(retain_graph=True)\n",
    "\n",
    "        # Update the parameters in the optimizer.\n",
    "        optimizer.step()\n",
    "\n",
    "        # Get training statistics.\n",
    "        stats = 'Epoch [%d/%d], Step [%d/%d], Loss: %.4f, Perplexity: %5.4f' % (epoch, num_epochs, i_step, total_step, loss.item(), np.exp(loss.item()))\n",
    "\n",
    "        # Print training statistics (on same line).\n",
    "        print('\\r' + stats, end=\"\")\n",
    "        sys.stdout.flush()\n",
    "\n",
    "        # Print training statistics to file.\n",
    "        f.write(stats + '\\n')\n",
    "        f.flush()\n",
    "\n",
    "        # Print training statistics (on different line).\n",
    "        if i_step % print_every == 0:\n",
    "            print('\\r' + stats)\n",
    "\n",
    "    # Save the weights.\n",
    "    if epoch % save_every == 0:\n",
    "        torch.save(decoder.state_dict(), os.path.join('./models', 'decoder-%d.pkl' % epoch))\n",
    "        torch.save(encoder.state_dict(), os.path.join('./models', 'encoder-%d.pkl' % epoch))\n",
    "\n",
    "# Close the training log file.\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step3'></a>\n",
    "## Step 3: (Optional) Validate your Model\n",
    "\n",
    "To assess potential overfitting, one approach is to assess performance on a validation set.  If you decide to do this **optional** task, you are required to first complete all of the steps in the next notebook in the sequence (**3_Inference.ipynb**); as part of that notebook, you will write and test code (specifically, the `sample` method in the `DecoderRNN` class) that uses your RNN decoder to generate captions.  That code will prove incredibly useful here. \n",
    "\n",
    "If you decide to validate your model, please do not edit the data loader in **data_loader.py**.  Instead, create a new file named **data_loader_val.py** containing the code for obtaining the data loader for the validation data.  You can access:\n",
    "- the validation images at filepath `'/opt/cocoapi/images/train2014/'`, and\n",
    "- the validation image caption annotation file at filepath `'/opt/cocoapi/annotations/captions_val2014.json'`.\n",
    "\n",
    "The suggested approach to validating your model involves creating a json file such as [this one](https://github.com/cocodataset/cocoapi/blob/master/results/captions_val2014_fakecap_results.json) containing your model's predicted captions for the validation images.  Then, you can write your own script or use one that you [find online](https://github.com/tylin/coco-caption) to calculate the BLEU score of your model.  You can read more about the BLEU score, along with other evaluation metrics (such as TEOR and Cider) in section 4.1 of [this paper](https://arxiv.org/pdf/1411.4555.pdf).  For more information about how to use the annotation file, check out the [website](http://cocodataset.org/#download) for the COCO dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Optional) TODO: Validate your model."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
