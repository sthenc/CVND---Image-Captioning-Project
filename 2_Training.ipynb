{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computer Vision Nanodegree\n",
    "\n",
    "## Project: Image Captioning\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, you will train your CNN-RNN model.  \n",
    "\n",
    "You are welcome and encouraged to try out many different architectures and hyperparameters when searching for a good model.\n",
    "\n",
    "This does have the potential to make the project quite messy!  Before submitting your project, make sure that you clean up:\n",
    "- the code you write in this notebook.  The notebook should describe how to train a single CNN-RNN architecture, corresponding to your final choice of hyperparameters.  You should structure the notebook so that the reviewer can replicate your results by running the code in this notebook.  \n",
    "- the output of the code cell in **Step 2**.  The output should show the output obtained when training the model from scratch.\n",
    "\n",
    "This notebook **will be graded**.  \n",
    "\n",
    "Feel free to use the links below to navigate the notebook:\n",
    "- [Step 1](#step1): Training Setup\n",
    "- [Step 2](#step2): Train your Model\n",
    "- [Step 3](#step3): (Optional) Validate your Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step1'></a>\n",
    "## Step 1: Training Setup\n",
    "\n",
    "In this step of the notebook, you will customize the training of your CNN-RNN model by specifying hyperparameters and setting other options that are important to the training procedure.  The values you set now will be used when training your model in **Step 2** below.\n",
    "\n",
    "You should only amend blocks of code that are preceded by a `TODO` statement.  **Any code blocks that are not preceded by a `TODO` statement should not be modified**.\n",
    "\n",
    "### Task #1\n",
    "\n",
    "Begin by setting the following variables:\n",
    "- `batch_size` - the batch size of each training batch.  It is the number of image-caption pairs used to amend the model weights in each training step. \n",
    "- `vocab_threshold` - the minimum word count threshold.  Note that a larger threshold will result in a smaller vocabulary, whereas a smaller threshold will include rarer words and result in a larger vocabulary.  \n",
    "- `vocab_from_file` - a Boolean that decides whether to load the vocabulary from file. \n",
    "- `embed_size` - the dimensionality of the image and word embeddings.  \n",
    "- `hidden_size` - the number of features in the hidden state of the RNN decoder.  \n",
    "- `num_epochs` - the number of epochs to train the model.  We recommend that you set `num_epochs=3`, but feel free to increase or decrease this number as you wish.  [This paper](https://arxiv.org/pdf/1502.03044.pdf) trained a captioning model on a single state-of-the-art GPU for 3 days, but you'll soon see that you can get reasonable results in a matter of a few hours!  (_But of course, if you want your model to compete with current research, you will have to train for much longer._)\n",
    "- `save_every` - determines how often to save the model weights.  We recommend that you set `save_every=1`, to save the model weights after each epoch.  This way, after the `i`th epoch, the encoder and decoder weights will be saved in the `models/` folder as `encoder-i.pkl` and `decoder-i.pkl`, respectively.\n",
    "- `print_every` - determines how often to print the batch loss to the Jupyter notebook while training.  Note that you **will not** observe a monotonic decrease in the loss function while training - this is perfectly fine and completely expected!  You are encouraged to keep this at its default value of `100` to avoid clogging the notebook, but feel free to change it.\n",
    "- `log_file` - the name of the text file containing - for every step - how the loss and perplexity evolved during training.\n",
    "\n",
    "If you're not sure where to begin to set some of the values above, you can peruse [this paper](https://arxiv.org/pdf/1502.03044.pdf) and [this paper](https://arxiv.org/pdf/1411.4555.pdf) for useful guidance!  **To avoid spending too long on this notebook**, you are encouraged to consult these suggested research papers to obtain a strong initial guess for which hyperparameters are likely to work best.  Then, train a single model, and proceed to the next notebook (**3_Inference.ipynb**).  If you are unhappy with your performance, you can return to this notebook to tweak the hyperparameters (and/or the architecture in **model.py**) and re-train your model.\n",
    "\n",
    "### Question 1\n",
    "\n",
    "**Question:** Describe your CNN-RNN architecture in detail.  With this architecture in mind, how did you select the values of the variables in Task 1?  If you consulted a research paper detailing a successful implementation of an image captioning model, please provide the reference.\n",
    "\n",
    "**Answer:** \n",
    "\n",
    "\n",
    "### (Optional) Task #2\n",
    "\n",
    "Note that we have provided a recommended image transform `transform_train` for pre-processing the training images, but you are welcome (and encouraged!) to modify it as you wish.  When modifying this transform, keep in mind that:\n",
    "- the images in the dataset have varying heights and widths, and \n",
    "- if using a pre-trained model, you must perform the corresponding appropriate normalization.\n",
    "\n",
    "### Question 2\n",
    "\n",
    "**Question:** How did you select the transform in `transform_train`?  If you left the transform at its provided value, why do you think that it is a good choice for your CNN architecture?\n",
    "\n",
    "**Answer:** The default looks reasonable, although I have some doubts as to whether it makes sense to discard the data around the edge in this case. \n",
    "\n",
    "### Task #3\n",
    "\n",
    "Next, you will specify a Python list containing the learnable parameters of the model.  For instance, if you decide to make all weights in the decoder trainable, but only want to train the weights in the embedding layer of the encoder, then you should set `params` to something like:\n",
    "```\n",
    "params = list(decoder.parameters()) + list(encoder.embed.parameters()) \n",
    "```\n",
    "\n",
    "### Question 3\n",
    "\n",
    "**Question:** How did you select the trainable parameters of your architecture?  Why do you think this is a good choice?\n",
    "\n",
    "**Answer:** \n",
    "\n",
    "### Task #4\n",
    "\n",
    "Finally, you will select an [optimizer](http://pytorch.org/docs/master/optim.html#torch.optim.Optimizer).\n",
    "\n",
    "### Question 4\n",
    "\n",
    "**Question:** How did you select the optimizer used to train your model?\n",
    "\n",
    "**Answer:** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/sthenc/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary successfully loaded from vocab.pkl file!\n",
      "loading annotations into memory...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 801/414113 [00:00<00:51, 8009.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done (t=0.51s)\n",
      "creating index...\n",
      "index created!\n",
      "Obtaining caption lengths...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 414113/414113 [00:43<00:00, 9555.96it/s] \n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "import sys\n",
    "sys.path.append('/opt/cocoapi/PythonAPI')\n",
    "from pycocotools.coco import COCO\n",
    "from data_loader import get_loader\n",
    "from model import EncoderCNN, DecoderRNN\n",
    "import math\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "## TODO #1: Select appropriate values for the Python variables below.\n",
    "#batch_size=1\n",
    "batch_size=10\n",
    "#batch_size = 64          # batch size, we have 10GB of GPU memory, let's use it\n",
    "vocab_threshold = 5        # minimum word count threshold\n",
    "vocab_from_file = True    # if True, load existing vocab file\n",
    "embed_size = 512           # dimensionality of image and word embeddings\n",
    "hidden_size = 512          # number of features in hidden state of the RNN decoder\n",
    "num_epochs = 25             # number of training epochs\n",
    "save_every = 1             # determines frequency of saving model weights\n",
    "print_every = 100          # determines window for printing average loss\n",
    "log_file = 'training_log.txt'       # name of file with saved training loss and perplexity\n",
    "\n",
    "# (Optional) TODO #2: Amend the image transform below.\n",
    "transform_train = transforms.Compose([ \n",
    "    transforms.Resize(256),                          # smaller edge of image resized to 256\n",
    "    transforms.RandomCrop(224),                      # get 224x224 crop from random location\n",
    "    transforms.RandomHorizontalFlip(),               # horizontally flip image with probability=0.5\n",
    "    transforms.ToTensor(),                           # convert the PIL Image to a tensor\n",
    "    transforms.Normalize((0.485, 0.456, 0.406),      # normalize image for pre-trained model\n",
    "                         (0.229, 0.224, 0.225))])\n",
    "\n",
    "# Build data loader.\n",
    "data_loader = get_loader(transform=transform_train,\n",
    "                         mode='train',\n",
    "                         batch_size=batch_size,\n",
    "                         vocab_threshold=vocab_threshold,\n",
    "                         vocab_from_file=vocab_from_file)\n",
    "\n",
    "# The size of the vocabulary.\n",
    "vocab_size = len(data_loader.dataset.vocab)\n",
    "\n",
    "# Initialize the encoder and decoder. \n",
    "encoder = EncoderCNN(embed_size)\n",
    "decoder = DecoderRNN(embed_size, hidden_size, vocab_size, max_batch_size=batch_size)\n",
    "\n",
    "# Move models to GPU if CUDA is available. \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "encoder.to(device)\n",
    "decoder.to(device)\n",
    "\n",
    "# Define the loss function. \n",
    "criterion = nn.CrossEntropyLoss().cuda() if torch.cuda.is_available() else nn.CrossEntropyLoss()\n",
    "\n",
    "# TODO #3: Specify the learnable parameters of the model.\n",
    "params = list(decoder.parameters()) + list(encoder.embed.parameters()) + list(encoder.bn.parameters())\n",
    "\n",
    "# TODO #4: Define the optimizer.\n",
    "#optimizer = torch.optim.SGD(params, lr=0.01)\n",
    "\n",
    "# this data is probably pretty sparse, and defaults are probably ok\n",
    "#http://ruder.io/optimizing-gradient-descent/\n",
    "#optimizer = torch.optim.Adam(params, lr=0.001)\n",
    "optimizer = torch.optim.Adam(params)\n",
    "\n",
    "# Set the total number of training steps per epoch.\n",
    "total_step = math.ceil(len(data_loader.dataset.caption_lengths) / data_loader.batch_sampler.batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step2'></a>\n",
    "## Step 2: Train your Model\n",
    "\n",
    "Once you have executed the code cell in **Step 1**, the training procedure below should run without issue.  \n",
    "\n",
    "It is completely fine to leave the code cell below as-is without modifications to train your model.  However, if you would like to modify the code used to train the model below, you must ensure that your changes are easily parsed by your reviewer.  In other words, make sure to provide appropriate comments to describe how your code works!  \n",
    "\n",
    "You may find it useful to load saved weights to resume training.  In that case, note the names of the files containing the encoder and decoder weights that you'd like to load (`encoder_file` and `decoder_file`).  Then you can load the weights by using the lines below:\n",
    "\n",
    "```python\n",
    "# Load pre-trained weights before resuming training.\n",
    "encoder.load_state_dict(torch.load(os.path.join('./models', encoder_file)))\n",
    "decoder.load_state_dict(torch.load(os.path.join('./models', decoder_file)))\n",
    "```\n",
    "\n",
    "While trying out parameters, make sure to take extensive notes and record the settings that you used in your various training runs.  In particular, you don't want to encounter a situation where you've trained a model for several hours but can't remember what settings you used :).\n",
    "\n",
    "### A Note on Tuning Hyperparameters\n",
    "\n",
    "To figure out how well your model is doing, you can look at how the training loss and perplexity evolve during training - and for the purposes of this project, you are encouraged to amend the hyperparameters based on this information.  \n",
    "\n",
    "However, this will not tell you if your model is overfitting to the training data, and, unfortunately, overfitting is a problem that is commonly encountered when training image captioning models.  \n",
    "\n",
    "For this project, you need not worry about overfitting. **This project does not have strict requirements regarding the performance of your model**, and you just need to demonstrate that your model has learned **_something_** when you generate captions on the test data.  For now, we strongly encourage you to train your model for the suggested 3 epochs without worrying about performance; then, you should immediately transition to the next notebook in the sequence (**3_Inference.ipynb**) to see how your model performs on the test data.  If your model needs to be changed, you can come back to this notebook, amend hyperparameters (if necessary), and re-train the model.\n",
    "\n",
    "That said, if you would like to go above and beyond in this project, you can read about some approaches to minimizing overfitting in section 4.3.1 of [this paper](http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7505636).  In the next (optional) step of this notebook, we provide some guidance for assessing the performance on the validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Epoch [1/25], Step [100/41412], Loss: 4.1422, Perplexity: 62.94231\n",
      "Epoch [1/25], Step [200/41412], Loss: 4.4320, Perplexity: 84.09834\n",
      "Epoch [1/25], Step [300/41412], Loss: 3.5618, Perplexity: 35.22595\n",
      "Epoch [1/25], Step [400/41412], Loss: 3.8433, Perplexity: 46.67939\n",
      "Epoch [1/25], Step [500/41412], Loss: 4.3513, Perplexity: 77.57604\n",
      "Epoch [1/25], Step [600/41412], Loss: 3.8125, Perplexity: 45.26140\n",
      "Epoch [1/25], Step [700/41412], Loss: 4.0556, Perplexity: 57.7216\n",
      "Epoch [1/25], Step [800/41412], Loss: 3.5645, Perplexity: 35.32114\n",
      "Epoch [1/25], Step [900/41412], Loss: 3.6654, Perplexity: 39.07250\n",
      "Epoch [1/25], Step [1000/41412], Loss: 3.5339, Perplexity: 34.2576\n",
      "Epoch [1/25], Step [1100/41412], Loss: 4.2510, Perplexity: 70.17715\n",
      "Epoch [1/25], Step [1200/41412], Loss: 3.1357, Perplexity: 23.00589\n",
      "Epoch [1/25], Step [1300/41412], Loss: 3.0393, Perplexity: 20.89128\n",
      "Epoch [1/25], Step [1400/41412], Loss: 3.2001, Perplexity: 24.5357\n",
      "Epoch [1/25], Step [1500/41412], Loss: 3.8620, Perplexity: 47.5591\n",
      "Epoch [1/25], Step [1600/41412], Loss: 2.8614, Perplexity: 17.4856\n",
      "Epoch [1/25], Step [1700/41412], Loss: 3.2158, Perplexity: 24.9235\n",
      "Epoch [1/25], Step [1800/41412], Loss: 2.7832, Perplexity: 16.1707\n",
      "Epoch [1/25], Step [1900/41412], Loss: 3.1041, Perplexity: 22.2881\n",
      "Epoch [1/25], Step [2000/41412], Loss: 2.5209, Perplexity: 12.4396\n",
      "Epoch [1/25], Step [2100/41412], Loss: 2.9993, Perplexity: 20.0718\n",
      "Epoch [1/25], Step [2200/41412], Loss: 3.1305, Perplexity: 22.8853\n",
      "Epoch [1/25], Step [2300/41412], Loss: 3.3652, Perplexity: 28.9393\n",
      "Epoch [1/25], Step [2400/41412], Loss: 2.6744, Perplexity: 14.5042\n",
      "Epoch [1/25], Step [2500/41412], Loss: 4.2789, Perplexity: 72.1596\n",
      "Epoch [1/25], Step [2600/41412], Loss: 3.6672, Perplexity: 39.1438\n",
      "Epoch [1/25], Step [2700/41412], Loss: 2.9318, Perplexity: 18.7618\n",
      "Epoch [1/25], Step [2800/41412], Loss: 2.9131, Perplexity: 18.4138\n",
      "Epoch [1/25], Step [2900/41412], Loss: 3.1946, Perplexity: 24.4016\n",
      "Epoch [1/25], Step [3000/41412], Loss: 3.5766, Perplexity: 35.7512\n",
      "Epoch [1/25], Step [3100/41412], Loss: 2.7298, Perplexity: 15.3291\n",
      "Epoch [1/25], Step [3200/41412], Loss: 2.9948, Perplexity: 19.9809\n",
      "Epoch [1/25], Step [3300/41412], Loss: 2.9912, Perplexity: 19.9102\n",
      "Epoch [1/25], Step [3400/41412], Loss: 2.7380, Perplexity: 15.45618\n",
      "Epoch [1/25], Step [3500/41412], Loss: 3.1352, Perplexity: 22.99407\n",
      "Epoch [1/25], Step [3600/41412], Loss: 2.9037, Perplexity: 18.2412\n",
      "Epoch [1/25], Step [3700/41412], Loss: 3.4502, Perplexity: 31.50571\n",
      "Epoch [1/25], Step [3800/41412], Loss: 3.2745, Perplexity: 26.4290\n",
      "Epoch [1/25], Step [3900/41412], Loss: 2.8186, Perplexity: 16.7532\n",
      "Epoch [1/25], Step [4000/41412], Loss: 3.2007, Perplexity: 24.5486\n",
      "Epoch [1/25], Step [4100/41412], Loss: 3.5263, Perplexity: 33.9988\n",
      "Epoch [1/25], Step [4200/41412], Loss: 3.0664, Perplexity: 21.4636\n",
      "Epoch [1/25], Step [4300/41412], Loss: 3.2365, Perplexity: 25.4444\n",
      "Epoch [1/25], Step [4400/41412], Loss: 3.2928, Perplexity: 26.9185\n",
      "Epoch [1/25], Step [4500/41412], Loss: 2.5596, Perplexity: 12.9301\n",
      "Epoch [1/25], Step [4600/41412], Loss: 3.4191, Perplexity: 30.5419\n",
      "Epoch [1/25], Step [4700/41412], Loss: 3.0994, Perplexity: 22.1843\n",
      "Epoch [1/25], Step [4800/41412], Loss: 2.6688, Perplexity: 14.4228\n",
      "Epoch [1/25], Step [4900/41412], Loss: 3.6175, Perplexity: 37.2426\n",
      "Epoch [1/25], Step [5000/41412], Loss: 3.1464, Perplexity: 23.2515\n",
      "Epoch [1/25], Step [5100/41412], Loss: 3.2707, Perplexity: 26.3286\n",
      "Epoch [1/25], Step [5200/41412], Loss: 2.6914, Perplexity: 14.7530\n",
      "Epoch [1/25], Step [5300/41412], Loss: 3.2413, Perplexity: 25.5680\n",
      "Epoch [1/25], Step [5400/41412], Loss: 2.5383, Perplexity: 12.6577\n",
      "Epoch [1/25], Step [5500/41412], Loss: 3.6575, Perplexity: 38.7631\n",
      "Epoch [1/25], Step [5600/41412], Loss: 3.5566, Perplexity: 35.0435\n",
      "Epoch [1/25], Step [5700/41412], Loss: 2.3008, Perplexity: 9.98206\n",
      "Epoch [1/25], Step [5800/41412], Loss: 3.6261, Perplexity: 37.5647\n",
      "Epoch [1/25], Step [5900/41412], Loss: 2.5433, Perplexity: 12.7217\n",
      "Epoch [1/25], Step [6000/41412], Loss: 2.7717, Perplexity: 15.9855\n",
      "Epoch [1/25], Step [6100/41412], Loss: 2.7843, Perplexity: 16.1878\n",
      "Epoch [1/25], Step [6200/41412], Loss: 2.8683, Perplexity: 17.6067\n",
      "Epoch [1/25], Step [6300/41412], Loss: 2.6203, Perplexity: 13.7404\n",
      "Epoch [1/25], Step [6400/41412], Loss: 3.1577, Perplexity: 23.5159\n",
      "Epoch [1/25], Step [6500/41412], Loss: 2.9759, Perplexity: 19.6077\n",
      "Epoch [1/25], Step [6600/41412], Loss: 2.8307, Perplexity: 16.9571\n",
      "Epoch [1/25], Step [6700/41412], Loss: 3.3846, Perplexity: 29.5065\n",
      "Epoch [1/25], Step [6800/41412], Loss: 2.6309, Perplexity: 13.8867\n",
      "Epoch [1/25], Step [6900/41412], Loss: 2.7924, Perplexity: 16.3205\n",
      "Epoch [1/25], Step [7000/41412], Loss: 2.9564, Perplexity: 19.2283\n",
      "Epoch [1/25], Step [7100/41412], Loss: 2.3092, Perplexity: 10.0660\n",
      "Epoch [1/25], Step [7200/41412], Loss: 3.2914, Perplexity: 26.8798\n",
      "Epoch [1/25], Step [7300/41412], Loss: 2.5586, Perplexity: 12.9179\n",
      "Epoch [1/25], Step [7400/41412], Loss: 2.8299, Perplexity: 16.9439\n",
      "Epoch [1/25], Step [7500/41412], Loss: 4.2173, Perplexity: 67.8523\n",
      "Epoch [1/25], Step [7600/41412], Loss: 2.7342, Perplexity: 15.3974\n",
      "Epoch [1/25], Step [7700/41412], Loss: 2.8397, Perplexity: 17.1104\n",
      "Epoch [1/25], Step [7800/41412], Loss: 3.4046, Perplexity: 30.1022\n",
      "Epoch [1/25], Step [7900/41412], Loss: 3.1119, Perplexity: 22.4633\n",
      "Epoch [1/25], Step [8000/41412], Loss: 2.8190, Perplexity: 16.7604\n",
      "Epoch [1/25], Step [8100/41412], Loss: 2.8402, Perplexity: 17.1199\n",
      "Epoch [1/25], Step [8200/41412], Loss: 3.1050, Perplexity: 22.3097\n",
      "Epoch [1/25], Step [8300/41412], Loss: 2.4321, Perplexity: 11.3822\n",
      "Epoch [1/25], Step [8400/41412], Loss: 2.7551, Perplexity: 15.7218\n",
      "Epoch [1/25], Step [8500/41412], Loss: 2.8674, Perplexity: 17.5914\n",
      "Epoch [1/25], Step [8600/41412], Loss: 4.1270, Perplexity: 61.9924\n",
      "Epoch [1/25], Step [8700/41412], Loss: 1.7759, Perplexity: 5.90561\n",
      "Epoch [1/25], Step [8800/41412], Loss: 2.7909, Perplexity: 16.2958\n",
      "Epoch [1/25], Step [8900/41412], Loss: 3.1036, Perplexity: 22.2789\n",
      "Epoch [1/25], Step [9000/41412], Loss: 3.0071, Perplexity: 20.22966\n",
      "Epoch [1/25], Step [9100/41412], Loss: 3.2451, Perplexity: 25.6644\n",
      "Epoch [1/25], Step [9200/41412], Loss: 2.7612, Perplexity: 15.8195\n",
      "Epoch [1/25], Step [9300/41412], Loss: 2.4678, Perplexity: 11.7962\n",
      "Epoch [1/25], Step [9400/41412], Loss: 2.7574, Perplexity: 15.7593\n",
      "Epoch [1/25], Step [9500/41412], Loss: 3.0253, Perplexity: 20.6002\n",
      "Epoch [1/25], Step [9600/41412], Loss: 2.8654, Perplexity: 17.5560\n",
      "Epoch [1/25], Step [9700/41412], Loss: 2.3149, Perplexity: 10.1234\n",
      "Epoch [1/25], Step [9800/41412], Loss: 3.2601, Perplexity: 26.0534\n",
      "Epoch [1/25], Step [9900/41412], Loss: 2.5690, Perplexity: 13.0526\n",
      "Epoch [1/25], Step [10000/41412], Loss: 2.5407, Perplexity: 12.6881\n",
      "Epoch [1/25], Step [10100/41412], Loss: 2.8127, Perplexity: 16.6543\n",
      "Epoch [1/25], Step [10200/41412], Loss: 3.2098, Perplexity: 24.7746\n",
      "Epoch [1/25], Step [10300/41412], Loss: 2.4777, Perplexity: 11.9144\n",
      "Epoch [1/25], Step [10400/41412], Loss: 2.4430, Perplexity: 11.5077\n",
      "Epoch [1/25], Step [10500/41412], Loss: 2.4645, Perplexity: 11.7572\n",
      "Epoch [1/25], Step [10600/41412], Loss: 2.3262, Perplexity: 10.2394\n",
      "Epoch [1/25], Step [10700/41412], Loss: 2.9721, Perplexity: 19.5339\n",
      "Epoch [1/25], Step [10800/41412], Loss: 2.5286, Perplexity: 12.5355\n",
      "Epoch [1/25], Step [10900/41412], Loss: 2.9023, Perplexity: 18.2155\n",
      "Epoch [1/25], Step [11000/41412], Loss: 2.7150, Perplexity: 15.1042\n",
      "Epoch [1/25], Step [11100/41412], Loss: 2.2161, Perplexity: 9.17159\n",
      "Epoch [1/25], Step [11200/41412], Loss: 2.7246, Perplexity: 15.2499\n",
      "Epoch [1/25], Step [11300/41412], Loss: 2.3541, Perplexity: 10.5285\n",
      "Epoch [1/25], Step [11400/41412], Loss: 2.7218, Perplexity: 15.2072\n",
      "Epoch [1/25], Step [11500/41412], Loss: 2.7136, Perplexity: 15.0835\n",
      "Epoch [1/25], Step [11600/41412], Loss: 2.5156, Perplexity: 12.3737\n",
      "Epoch [1/25], Step [11700/41412], Loss: 2.4697, Perplexity: 11.8194\n",
      "Epoch [1/25], Step [11800/41412], Loss: 2.7191, Perplexity: 15.1660\n",
      "Epoch [1/25], Step [11900/41412], Loss: 2.7544, Perplexity: 15.7114\n",
      "Epoch [1/25], Step [12000/41412], Loss: 2.4280, Perplexity: 11.3364\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/25], Step [12100/41412], Loss: 2.4790, Perplexity: 11.9288\n",
      "Epoch [1/25], Step [12200/41412], Loss: 2.3846, Perplexity: 10.85437\n",
      "Epoch [1/25], Step [12300/41412], Loss: 2.3990, Perplexity: 11.0126\n",
      "Epoch [1/25], Step [12400/41412], Loss: 2.3093, Perplexity: 10.0670\n",
      "Epoch [1/25], Step [12500/41412], Loss: 2.6168, Perplexity: 13.6914\n",
      "Epoch [1/25], Step [12600/41412], Loss: 2.5944, Perplexity: 13.3884\n",
      "Epoch [1/25], Step [12700/41412], Loss: 2.6973, Perplexity: 14.8394\n",
      "Epoch [1/25], Step [12800/41412], Loss: 2.5158, Perplexity: 12.3768\n",
      "Epoch [1/25], Step [12900/41412], Loss: 3.8816, Perplexity: 48.5004\n",
      "Epoch [1/25], Step [13000/41412], Loss: 3.4277, Perplexity: 30.8053\n",
      "Epoch [1/25], Step [13100/41412], Loss: 2.7026, Perplexity: 14.9179\n",
      "Epoch [1/25], Step [13200/41412], Loss: 2.3970, Perplexity: 10.99016\n",
      "Epoch [1/25], Step [13300/41412], Loss: 2.6679, Perplexity: 14.4102\n",
      "Epoch [1/25], Step [13400/41412], Loss: 2.5070, Perplexity: 12.2681\n",
      "Epoch [1/25], Step [13500/41412], Loss: 3.7003, Perplexity: 40.4595\n",
      "Epoch [1/25], Step [13600/41412], Loss: 3.2956, Perplexity: 26.9926\n",
      "Epoch [1/25], Step [13700/41412], Loss: 3.0577, Perplexity: 21.2784\n",
      "Epoch [1/25], Step [13800/41412], Loss: 2.4909, Perplexity: 12.0719\n",
      "Epoch [1/25], Step [13900/41412], Loss: 3.0454, Perplexity: 21.0184\n",
      "Epoch [1/25], Step [14000/41412], Loss: 2.7133, Perplexity: 15.0787\n",
      "Epoch [1/25], Step [14100/41412], Loss: 3.2438, Perplexity: 25.6314\n",
      "Epoch [1/25], Step [14200/41412], Loss: 2.4897, Perplexity: 12.0581\n",
      "Epoch [1/25], Step [14300/41412], Loss: 2.7277, Perplexity: 15.2982\n",
      "Epoch [1/25], Step [14400/41412], Loss: 2.7479, Perplexity: 15.6099\n",
      "Epoch [1/25], Step [14500/41412], Loss: 2.5388, Perplexity: 12.6643\n",
      "Epoch [1/25], Step [14600/41412], Loss: 2.3866, Perplexity: 10.87693\n",
      "Epoch [1/25], Step [14700/41412], Loss: 2.7696, Perplexity: 15.9529\n",
      "Epoch [1/25], Step [14800/41412], Loss: 2.6115, Perplexity: 13.6198\n",
      "Epoch [1/25], Step [14900/41412], Loss: 2.8157, Perplexity: 16.7051\n",
      "Epoch [1/25], Step [15000/41412], Loss: 2.9305, Perplexity: 18.7367\n",
      "Epoch [1/25], Step [15100/41412], Loss: 2.7563, Perplexity: 15.7408\n",
      "Epoch [1/25], Step [15200/41412], Loss: 2.5874, Perplexity: 13.2949\n",
      "Epoch [1/25], Step [15300/41412], Loss: 2.5593, Perplexity: 12.9265\n",
      "Epoch [1/25], Step [15400/41412], Loss: 2.7746, Perplexity: 16.0315\n",
      "Epoch [1/25], Step [15500/41412], Loss: 2.8667, Perplexity: 17.5782\n",
      "Epoch [1/25], Step [15600/41412], Loss: 3.2220, Perplexity: 25.0772\n",
      "Epoch [1/25], Step [15700/41412], Loss: 2.4782, Perplexity: 11.9200\n",
      "Epoch [1/25], Step [15800/41412], Loss: 2.3315, Perplexity: 10.29352\n",
      "Epoch [1/25], Step [15900/41412], Loss: 2.7990, Perplexity: 16.4284\n",
      "Epoch [1/25], Step [16000/41412], Loss: 2.5425, Perplexity: 12.7116\n",
      "Epoch [1/25], Step [16100/41412], Loss: 2.4876, Perplexity: 12.0326\n",
      "Epoch [1/25], Step [16200/41412], Loss: 3.0986, Perplexity: 22.1669\n",
      "Epoch [1/25], Step [16300/41412], Loss: 2.6043, Perplexity: 13.5221\n",
      "Epoch [1/25], Step [16400/41412], Loss: 2.9379, Perplexity: 18.8760\n",
      "Epoch [1/25], Step [16500/41412], Loss: 2.5975, Perplexity: 13.4295\n",
      "Epoch [1/25], Step [16600/41412], Loss: 2.4300, Perplexity: 11.3583\n",
      "Epoch [1/25], Step [16700/41412], Loss: 2.5245, Perplexity: 12.4844\n",
      "Epoch [1/25], Step [16800/41412], Loss: 1.9567, Perplexity: 7.076097\n",
      "Epoch [1/25], Step [16900/41412], Loss: 2.4261, Perplexity: 11.3144\n",
      "Epoch [1/25], Step [17000/41412], Loss: 2.3549, Perplexity: 10.5374\n",
      "Epoch [1/25], Step [17100/41412], Loss: 2.7628, Perplexity: 15.8445\n",
      "Epoch [1/25], Step [17200/41412], Loss: 2.9619, Perplexity: 19.3341\n",
      "Epoch [1/25], Step [17300/41412], Loss: 2.5752, Perplexity: 13.1344\n",
      "Epoch [1/25], Step [17400/41412], Loss: 2.9669, Perplexity: 19.4322\n",
      "Epoch [1/25], Step [17500/41412], Loss: 2.6148, Perplexity: 13.6641\n",
      "Epoch [1/25], Step [17600/41412], Loss: 2.5507, Perplexity: 12.8155\n",
      "Epoch [1/25], Step [17700/41412], Loss: 2.5312, Perplexity: 12.5687\n",
      "Epoch [1/25], Step [17800/41412], Loss: 3.2918, Perplexity: 26.8922\n",
      "Epoch [1/25], Step [17900/41412], Loss: 3.0772, Perplexity: 21.6974\n",
      "Epoch [1/25], Step [18000/41412], Loss: 2.8429, Perplexity: 17.1663\n",
      "Epoch [1/25], Step [18100/41412], Loss: 2.6699, Perplexity: 14.4382\n",
      "Epoch [1/25], Step [18200/41412], Loss: 2.9011, Perplexity: 18.1944\n",
      "Epoch [1/25], Step [18300/41412], Loss: 2.4736, Perplexity: 11.8652\n",
      "Epoch [1/25], Step [18400/41412], Loss: 2.2990, Perplexity: 9.96394\n",
      "Epoch [1/25], Step [18500/41412], Loss: 2.8119, Perplexity: 16.6420\n",
      "Epoch [1/25], Step [18600/41412], Loss: 2.5955, Perplexity: 13.4030\n",
      "Epoch [1/25], Step [18700/41412], Loss: 3.0770, Perplexity: 21.6932\n",
      "Epoch [1/25], Step [18800/41412], Loss: 2.8524, Perplexity: 17.3291\n",
      "Epoch [1/25], Step [18900/41412], Loss: 2.4884, Perplexity: 12.0422\n",
      "Epoch [1/25], Step [19000/41412], Loss: 2.4472, Perplexity: 11.5560\n",
      "Epoch [1/25], Step [19100/41412], Loss: 2.7913, Perplexity: 16.3027\n",
      "Epoch [1/25], Step [19200/41412], Loss: 2.8082, Perplexity: 16.5803\n",
      "Epoch [1/25], Step [19300/41412], Loss: 2.8739, Perplexity: 17.7051\n",
      "Epoch [1/25], Step [19400/41412], Loss: 2.9909, Perplexity: 19.9037\n",
      "Epoch [1/25], Step [19500/41412], Loss: 2.6154, Perplexity: 13.6729\n",
      "Epoch [1/25], Step [19600/41412], Loss: 3.4745, Perplexity: 32.2830\n",
      "Epoch [1/25], Step [19700/41412], Loss: 1.9829, Perplexity: 7.26382\n",
      "Epoch [1/25], Step [19800/41412], Loss: 2.5265, Perplexity: 12.50961\n",
      "Epoch [1/25], Step [19900/41412], Loss: 3.0070, Perplexity: 20.2272\n",
      "Epoch [1/25], Step [20000/41412], Loss: 2.9236, Perplexity: 18.6080\n",
      "Epoch [1/25], Step [20100/41412], Loss: 2.5349, Perplexity: 12.6150\n",
      "Epoch [1/25], Step [20200/41412], Loss: 2.4633, Perplexity: 11.7430\n",
      "Epoch [1/25], Step [20300/41412], Loss: 2.7305, Perplexity: 15.3402\n",
      "Epoch [1/25], Step [20400/41412], Loss: 2.7063, Perplexity: 14.9737\n",
      "Epoch [1/25], Step [20500/41412], Loss: 2.1874, Perplexity: 8.91172\n",
      "Epoch [1/25], Step [20600/41412], Loss: 3.0373, Perplexity: 20.8478\n",
      "Epoch [1/25], Step [20700/41412], Loss: 2.2653, Perplexity: 9.63388\n",
      "Epoch [1/25], Step [20800/41412], Loss: 2.6944, Perplexity: 14.7963\n",
      "Epoch [1/25], Step [20900/41412], Loss: 3.6111, Perplexity: 37.0050\n",
      "Epoch [1/25], Step [21000/41412], Loss: 2.5245, Perplexity: 12.4843\n",
      "Epoch [1/25], Step [21100/41412], Loss: 3.0122, Perplexity: 20.3331\n",
      "Epoch [1/25], Step [21200/41412], Loss: 2.3716, Perplexity: 10.7150\n",
      "Epoch [1/25], Step [21300/41412], Loss: 2.8995, Perplexity: 18.1642\n",
      "Epoch [1/25], Step [21400/41412], Loss: 3.3630, Perplexity: 28.8743\n",
      "Epoch [1/25], Step [21500/41412], Loss: 2.5725, Perplexity: 13.0983\n",
      "Epoch [1/25], Step [21600/41412], Loss: 3.0501, Perplexity: 21.1168\n",
      "Epoch [1/25], Step [21700/41412], Loss: 3.0793, Perplexity: 21.7433\n",
      "Epoch [1/25], Step [21800/41412], Loss: 2.7539, Perplexity: 15.7034\n",
      "Epoch [1/25], Step [21900/41412], Loss: 2.7498, Perplexity: 15.6401\n",
      "Epoch [1/25], Step [22000/41412], Loss: 2.6725, Perplexity: 14.4766\n",
      "Epoch [1/25], Step [22100/41412], Loss: 2.4952, Perplexity: 12.1243\n",
      "Epoch [1/25], Step [22200/41412], Loss: 2.6413, Perplexity: 14.0314\n",
      "Epoch [1/25], Step [22300/41412], Loss: 2.2743, Perplexity: 9.721030\n",
      "Epoch [1/25], Step [22400/41412], Loss: 2.9367, Perplexity: 18.8535\n",
      "Epoch [1/25], Step [22500/41412], Loss: 2.6120, Perplexity: 13.6258\n",
      "Epoch [1/25], Step [22600/41412], Loss: 3.1340, Perplexity: 22.9658\n",
      "Epoch [1/25], Step [22700/41412], Loss: 2.6664, Perplexity: 14.3885\n",
      "Epoch [1/25], Step [22800/41412], Loss: 2.3958, Perplexity: 10.9766\n",
      "Epoch [1/25], Step [22900/41412], Loss: 2.5331, Perplexity: 12.5931\n",
      "Epoch [1/25], Step [23000/41412], Loss: 2.7039, Perplexity: 14.9376\n",
      "Epoch [1/25], Step [23100/41412], Loss: 2.1725, Perplexity: 8.78013\n",
      "Epoch [1/25], Step [23200/41412], Loss: 3.1309, Perplexity: 22.8949\n",
      "Epoch [1/25], Step [23300/41412], Loss: 2.5613, Perplexity: 12.9527\n",
      "Epoch [1/25], Step [23400/41412], Loss: 2.7552, Perplexity: 15.7245\n",
      "Epoch [1/25], Step [23500/41412], Loss: 2.2981, Perplexity: 9.95573\n",
      "Epoch [1/25], Step [23600/41412], Loss: 2.4679, Perplexity: 11.7978\n",
      "Epoch [1/25], Step [23700/41412], Loss: 2.3054, Perplexity: 10.0283\n",
      "Epoch [1/25], Step [23800/41412], Loss: 2.7709, Perplexity: 15.9725\n",
      "Epoch [1/25], Step [23900/41412], Loss: 2.8181, Perplexity: 16.7458\n",
      "Epoch [1/25], Step [24000/41412], Loss: 2.4867, Perplexity: 12.0215\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/25], Step [24100/41412], Loss: 2.5130, Perplexity: 12.3416\n",
      "Epoch [1/25], Step [24200/41412], Loss: 2.4657, Perplexity: 11.77215\n",
      "Epoch [1/25], Step [24300/41412], Loss: 2.7268, Perplexity: 15.2833\n",
      "Epoch [1/25], Step [24400/41412], Loss: 2.8717, Perplexity: 17.6673\n",
      "Epoch [1/25], Step [24500/41412], Loss: 2.5214, Perplexity: 12.4458\n",
      "Epoch [1/25], Step [24600/41412], Loss: 2.7297, Perplexity: 15.3282\n",
      "Epoch [1/25], Step [24700/41412], Loss: 2.1357, Perplexity: 8.46278\n",
      "Epoch [1/25], Step [24800/41412], Loss: 2.4040, Perplexity: 11.0679\n",
      "Epoch [1/25], Step [24900/41412], Loss: 2.9559, Perplexity: 19.2193\n",
      "Epoch [1/25], Step [25000/41412], Loss: 2.7671, Perplexity: 15.9130\n",
      "Epoch [1/25], Step [25100/41412], Loss: 2.4866, Perplexity: 12.0200\n",
      "Epoch [1/25], Step [25200/41412], Loss: 2.7681, Perplexity: 15.9285\n",
      "Epoch [1/25], Step [25300/41412], Loss: 2.5384, Perplexity: 12.6597\n",
      "Epoch [1/25], Step [25400/41412], Loss: 2.2283, Perplexity: 9.284347\n",
      "Epoch [1/25], Step [25500/41412], Loss: 2.0421, Perplexity: 7.70713\n",
      "Epoch [1/25], Step [25600/41412], Loss: 2.7885, Perplexity: 16.2573\n",
      "Epoch [1/25], Step [25700/41412], Loss: 1.9001, Perplexity: 6.68664\n",
      "Epoch [1/25], Step [25800/41412], Loss: 2.7004, Perplexity: 14.8863\n",
      "Epoch [1/25], Step [25900/41412], Loss: 2.7366, Perplexity: 15.4339\n",
      "Epoch [1/25], Step [26000/41412], Loss: 2.5849, Perplexity: 13.2616\n",
      "Epoch [1/25], Step [26100/41412], Loss: 3.2791, Perplexity: 26.5510\n",
      "Epoch [1/25], Step [26200/41412], Loss: 3.0481, Perplexity: 21.0752\n",
      "Epoch [1/25], Step [26300/41412], Loss: 2.4990, Perplexity: 12.1707\n",
      "Epoch [1/25], Step [26400/41412], Loss: 2.3632, Perplexity: 10.6253\n",
      "Epoch [1/25], Step [26500/41412], Loss: 2.5726, Perplexity: 13.0996\n",
      "Epoch [1/25], Step [26600/41412], Loss: 2.1965, Perplexity: 8.99356\n",
      "Epoch [1/25], Step [26700/41412], Loss: 2.5278, Perplexity: 12.5258\n",
      "Epoch [1/25], Step [26800/41412], Loss: 2.4843, Perplexity: 11.9928\n",
      "Epoch [1/25], Step [26900/41412], Loss: 2.5953, Perplexity: 13.4001\n",
      "Epoch [1/25], Step [27000/41412], Loss: 2.6477, Perplexity: 14.1208\n",
      "Epoch [1/25], Step [27100/41412], Loss: 2.5033, Perplexity: 12.2228\n",
      "Epoch [1/25], Step [27200/41412], Loss: 2.5797, Perplexity: 13.1929\n",
      "Epoch [1/25], Step [27300/41412], Loss: 2.9894, Perplexity: 19.8729\n",
      "Epoch [1/25], Step [27400/41412], Loss: 2.9520, Perplexity: 19.1442\n",
      "Epoch [1/25], Step [27500/41412], Loss: 1.8132, Perplexity: 6.13019\n",
      "Epoch [1/25], Step [27600/41412], Loss: 2.4498, Perplexity: 11.5860\n",
      "Epoch [1/25], Step [27700/41412], Loss: 2.3698, Perplexity: 10.6951\n",
      "Epoch [1/25], Step [27800/41412], Loss: 2.3998, Perplexity: 11.0213\n",
      "Epoch [1/25], Step [27900/41412], Loss: 2.5651, Perplexity: 13.0019\n",
      "Epoch [1/25], Step [28000/41412], Loss: 1.9952, Perplexity: 7.35378\n",
      "Epoch [1/25], Step [28100/41412], Loss: 2.7198, Perplexity: 15.1768\n",
      "Epoch [1/25], Step [28200/41412], Loss: 3.2529, Perplexity: 25.8656\n",
      "Epoch [1/25], Step [28300/41412], Loss: 2.4685, Perplexity: 11.8048\n",
      "Epoch [1/25], Step [28400/41412], Loss: 2.8409, Perplexity: 17.1316\n",
      "Epoch [1/25], Step [28500/41412], Loss: 2.7777, Perplexity: 16.0813\n",
      "Epoch [1/25], Step [28600/41412], Loss: 2.0706, Perplexity: 7.92968\n",
      "Epoch [1/25], Step [28700/41412], Loss: 2.1749, Perplexity: 8.80105\n",
      "Epoch [1/25], Step [28800/41412], Loss: 2.3391, Perplexity: 10.3716\n",
      "Epoch [1/25], Step [28900/41412], Loss: 2.2730, Perplexity: 9.70807\n",
      "Epoch [1/25], Step [29000/41412], Loss: 2.8739, Perplexity: 17.7054\n",
      "Epoch [1/25], Step [29100/41412], Loss: 2.4923, Perplexity: 12.0889\n",
      "Epoch [1/25], Step [29200/41412], Loss: 2.3300, Perplexity: 10.2778\n",
      "Epoch [1/25], Step [29300/41412], Loss: 2.4089, Perplexity: 11.1220\n",
      "Epoch [1/25], Step [29400/41412], Loss: 2.1834, Perplexity: 8.87635\n",
      "Epoch [1/25], Step [29500/41412], Loss: 2.2412, Perplexity: 9.40455\n",
      "Epoch [1/25], Step [29600/41412], Loss: 2.2234, Perplexity: 9.23853\n",
      "Epoch [1/25], Step [29700/41412], Loss: 2.3879, Perplexity: 10.8901\n",
      "Epoch [1/25], Step [29800/41412], Loss: 2.3588, Perplexity: 10.5788\n",
      "Epoch [1/25], Step [29900/41412], Loss: 3.0088, Perplexity: 20.2631\n",
      "Epoch [1/25], Step [30000/41412], Loss: 2.5986, Perplexity: 13.4453\n",
      "Epoch [1/25], Step [30100/41412], Loss: 2.6490, Perplexity: 14.1395\n",
      "Epoch [1/25], Step [30200/41412], Loss: 2.2525, Perplexity: 9.511638\n",
      "Epoch [1/25], Step [30300/41412], Loss: 2.2770, Perplexity: 9.74692\n",
      "Epoch [1/25], Step [30400/41412], Loss: 2.3735, Perplexity: 10.7347\n",
      "Epoch [1/25], Step [30500/41412], Loss: 3.0626, Perplexity: 21.3835\n",
      "Epoch [1/25], Step [30600/41412], Loss: 2.7916, Perplexity: 16.3075\n",
      "Epoch [1/25], Step [30700/41412], Loss: 3.1777, Perplexity: 23.9909\n",
      "Epoch [1/25], Step [30800/41412], Loss: 2.5903, Perplexity: 13.33426\n",
      "Epoch [1/25], Step [30900/41412], Loss: 2.5367, Perplexity: 12.6377\n",
      "Epoch [1/25], Step [31000/41412], Loss: 2.2278, Perplexity: 9.27959\n",
      "Epoch [1/25], Step [31100/41412], Loss: 2.2577, Perplexity: 9.56077\n",
      "Epoch [1/25], Step [31200/41412], Loss: 2.6795, Perplexity: 14.5775\n",
      "Epoch [1/25], Step [31300/41412], Loss: 2.7878, Perplexity: 16.2449\n",
      "Epoch [1/25], Step [31400/41412], Loss: 2.4507, Perplexity: 11.5969\n",
      "Epoch [1/25], Step [31500/41412], Loss: 2.5806, Perplexity: 13.2056\n",
      "Epoch [1/25], Step [31600/41412], Loss: 2.3554, Perplexity: 10.5428\n",
      "Epoch [1/25], Step [31700/41412], Loss: 2.6397, Perplexity: 14.0091\n",
      "Epoch [1/25], Step [31800/41412], Loss: 2.6834, Perplexity: 14.6342\n",
      "Epoch [1/25], Step [31900/41412], Loss: 2.4638, Perplexity: 11.7497\n",
      "Epoch [1/25], Step [32000/41412], Loss: 2.7067, Perplexity: 14.9798\n",
      "Epoch [1/25], Step [32100/41412], Loss: 2.4227, Perplexity: 11.2768\n",
      "Epoch [1/25], Step [32200/41412], Loss: 2.5805, Perplexity: 13.2033\n",
      "Epoch [1/25], Step [32300/41412], Loss: 2.6226, Perplexity: 13.7711\n",
      "Epoch [1/25], Step [32400/41412], Loss: 2.0893, Perplexity: 8.07943\n",
      "Epoch [1/25], Step [32500/41412], Loss: 2.4177, Perplexity: 11.2203\n",
      "Epoch [1/25], Step [32600/41412], Loss: 2.7474, Perplexity: 15.6020\n",
      "Epoch [1/25], Step [32700/41412], Loss: 2.0395, Perplexity: 7.68657\n",
      "Epoch [1/25], Step [32800/41412], Loss: 2.4769, Perplexity: 11.9043\n",
      "Epoch [1/25], Step [32900/41412], Loss: 2.8979, Perplexity: 18.1360\n",
      "Epoch [1/25], Step [33000/41412], Loss: 3.1425, Perplexity: 23.1616\n",
      "Epoch [1/25], Step [33100/41412], Loss: 2.6308, Perplexity: 13.8848\n",
      "Epoch [1/25], Step [33200/41412], Loss: 2.9090, Perplexity: 18.3387\n",
      "Epoch [1/25], Step [33300/41412], Loss: 2.1695, Perplexity: 8.75397\n",
      "Epoch [1/25], Step [33400/41412], Loss: 2.6147, Perplexity: 13.6632\n",
      "Epoch [1/25], Step [33500/41412], Loss: 2.9479, Perplexity: 19.0653\n",
      "Epoch [1/25], Step [33600/41412], Loss: 2.6930, Perplexity: 14.7753\n",
      "Epoch [1/25], Step [33700/41412], Loss: 3.0211, Perplexity: 20.51360\n",
      "Epoch [1/25], Step [33800/41412], Loss: 1.9372, Perplexity: 6.93923\n",
      "Epoch [1/25], Step [33900/41412], Loss: 2.2224, Perplexity: 9.22971\n",
      "Epoch [1/25], Step [34000/41412], Loss: 3.0589, Perplexity: 21.3034\n",
      "Epoch [1/25], Step [34100/41412], Loss: 2.9318, Perplexity: 18.7609\n",
      "Epoch [1/25], Step [34200/41412], Loss: 2.2053, Perplexity: 9.07261\n",
      "Epoch [1/25], Step [34300/41412], Loss: 2.4629, Perplexity: 11.7383\n",
      "Epoch [1/25], Step [34400/41412], Loss: 2.3427, Perplexity: 10.4091\n",
      "Epoch [1/25], Step [34500/41412], Loss: 2.1963, Perplexity: 8.99213\n",
      "Epoch [1/25], Step [34600/41412], Loss: 2.4026, Perplexity: 11.0518\n",
      "Epoch [1/25], Step [34700/41412], Loss: 2.4321, Perplexity: 11.3832\n",
      "Epoch [1/25], Step [34800/41412], Loss: 2.7522, Perplexity: 15.6776\n",
      "Epoch [1/25], Step [34900/41412], Loss: 2.7784, Perplexity: 16.0926\n",
      "Epoch [1/25], Step [35000/41412], Loss: 2.4891, Perplexity: 12.0508\n",
      "Epoch [1/25], Step [35100/41412], Loss: 2.2649, Perplexity: 9.63010\n",
      "Epoch [1/25], Step [35200/41412], Loss: 3.3854, Perplexity: 29.5311\n",
      "Epoch [1/25], Step [35300/41412], Loss: 2.4742, Perplexity: 11.8727\n",
      "Epoch [1/25], Step [35400/41412], Loss: 2.5578, Perplexity: 12.9075\n",
      "Epoch [1/25], Step [35500/41412], Loss: 2.7716, Perplexity: 15.9847\n",
      "Epoch [1/25], Step [35600/41412], Loss: 2.5217, Perplexity: 12.4502\n",
      "Epoch [1/25], Step [35700/41412], Loss: 2.5038, Perplexity: 12.2294\n",
      "Epoch [1/25], Step [35800/41412], Loss: 2.6212, Perplexity: 13.7519\n",
      "Epoch [1/25], Step [35900/41412], Loss: 3.5321, Perplexity: 34.1973\n",
      "Epoch [1/25], Step [36000/41412], Loss: 2.0171, Perplexity: 7.51678\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/25], Step [36100/41412], Loss: 2.6301, Perplexity: 13.8749\n",
      "Epoch [1/25], Step [36200/41412], Loss: 2.2044, Perplexity: 9.06452\n",
      "Epoch [1/25], Step [36300/41412], Loss: 2.2119, Perplexity: 9.13306\n",
      "Epoch [1/25], Step [36400/41412], Loss: 2.2575, Perplexity: 9.55937\n",
      "Epoch [1/25], Step [36500/41412], Loss: 2.7144, Perplexity: 15.09546\n",
      "Epoch [1/25], Step [36600/41412], Loss: 2.0421, Perplexity: 7.70726\n",
      "Epoch [1/25], Step [36700/41412], Loss: 2.6440, Perplexity: 14.0687\n",
      "Epoch [1/25], Step [36800/41412], Loss: 2.3597, Perplexity: 10.5874\n",
      "Epoch [1/25], Step [36900/41412], Loss: 2.3868, Perplexity: 10.8785\n",
      "Epoch [1/25], Step [37000/41412], Loss: 2.9265, Perplexity: 18.6616\n",
      "Epoch [1/25], Step [37100/41412], Loss: 2.6153, Perplexity: 13.6715\n",
      "Epoch [1/25], Step [37200/41412], Loss: 2.3157, Perplexity: 10.1317\n",
      "Epoch [1/25], Step [37300/41412], Loss: 3.2043, Perplexity: 24.6392\n",
      "Epoch [1/25], Step [37400/41412], Loss: 2.6260, Perplexity: 13.8180\n",
      "Epoch [1/25], Step [37500/41412], Loss: 2.3844, Perplexity: 10.8524\n",
      "Epoch [1/25], Step [37600/41412], Loss: 2.4922, Perplexity: 12.0874\n",
      "Epoch [1/25], Step [37700/41412], Loss: 3.1858, Perplexity: 24.1876\n",
      "Epoch [1/25], Step [37800/41412], Loss: 2.5392, Perplexity: 12.6689\n",
      "Epoch [1/25], Step [37900/41412], Loss: 3.1139, Perplexity: 22.5097\n",
      "Epoch [1/25], Step [38000/41412], Loss: 2.6244, Perplexity: 13.7967\n",
      "Epoch [1/25], Step [38100/41412], Loss: 2.1199, Perplexity: 8.33024\n",
      "Epoch [1/25], Step [38200/41412], Loss: 2.7477, Perplexity: 15.6065\n",
      "Epoch [1/25], Step [38300/41412], Loss: 2.8697, Perplexity: 17.6316\n",
      "Epoch [1/25], Step [38400/41412], Loss: 2.3709, Perplexity: 10.7073\n",
      "Epoch [1/25], Step [38500/41412], Loss: 2.9279, Perplexity: 18.6876\n",
      "Epoch [1/25], Step [38600/41412], Loss: 2.3064, Perplexity: 10.0383\n",
      "Epoch [1/25], Step [38700/41412], Loss: 3.1047, Perplexity: 22.3023\n",
      "Epoch [1/25], Step [38800/41412], Loss: 2.5039, Perplexity: 12.2306\n",
      "Epoch [1/25], Step [38900/41412], Loss: 2.5481, Perplexity: 12.7825\n",
      "Epoch [1/25], Step [39000/41412], Loss: 2.2287, Perplexity: 9.28784\n",
      "Epoch [1/25], Step [39100/41412], Loss: 2.8820, Perplexity: 17.8503\n",
      "Epoch [1/25], Step [39200/41412], Loss: 2.8611, Perplexity: 17.4812\n",
      "Epoch [1/25], Step [39300/41412], Loss: 2.1782, Perplexity: 8.83026\n",
      "Epoch [1/25], Step [39400/41412], Loss: 2.1486, Perplexity: 8.57308\n",
      "Epoch [1/25], Step [39500/41412], Loss: 2.7025, Perplexity: 14.9169\n",
      "Epoch [1/25], Step [39600/41412], Loss: 2.5410, Perplexity: 12.6921\n",
      "Epoch [1/25], Step [39700/41412], Loss: 2.6504, Perplexity: 14.1603\n",
      "Epoch [1/25], Step [39800/41412], Loss: 2.0087, Perplexity: 7.45359\n",
      "Epoch [1/25], Step [39900/41412], Loss: 3.0935, Perplexity: 22.0547\n",
      "Epoch [1/25], Step [40000/41412], Loss: 2.8062, Perplexity: 16.5462\n",
      "Epoch [1/25], Step [40100/41412], Loss: 2.4609, Perplexity: 11.7159\n",
      "Epoch [1/25], Step [40200/41412], Loss: 2.6035, Perplexity: 13.5106\n",
      "Epoch [1/25], Step [40300/41412], Loss: 2.7185, Perplexity: 15.1581\n",
      "Epoch [1/25], Step [40400/41412], Loss: 3.0481, Perplexity: 21.0754\n",
      "Epoch [1/25], Step [40500/41412], Loss: 2.5923, Perplexity: 13.3604\n",
      "Epoch [1/25], Step [40600/41412], Loss: 2.3139, Perplexity: 10.1133\n",
      "Epoch [1/25], Step [40700/41412], Loss: 2.5893, Perplexity: 13.3207\n",
      "Epoch [1/25], Step [40800/41412], Loss: 2.6997, Perplexity: 14.8752\n",
      "Epoch [1/25], Step [40900/41412], Loss: 2.9207, Perplexity: 18.5548\n",
      "Epoch [1/25], Step [41000/41412], Loss: 2.4557, Perplexity: 11.6541\n",
      "Epoch [1/25], Step [41100/41412], Loss: 2.3338, Perplexity: 10.3167\n",
      "Epoch [1/25], Step [41200/41412], Loss: 2.8439, Perplexity: 17.1831\n",
      "Epoch [1/25], Step [41300/41412], Loss: 2.5676, Perplexity: 13.0345\n",
      "Epoch [1/25], Step [41400/41412], Loss: 2.9172, Perplexity: 18.4892\n",
      "Epoch [2/25], Step [100/41412], Loss: 2.0930, Perplexity: 8.1096904\n",
      "Epoch [2/25], Step [200/41412], Loss: 2.0756, Perplexity: 7.96945\n",
      "Epoch [2/25], Step [300/41412], Loss: 2.2383, Perplexity: 9.37751\n",
      "Epoch [2/25], Step [400/41412], Loss: 2.2619, Perplexity: 9.60102\n",
      "Epoch [2/25], Step [500/41412], Loss: 1.8989, Perplexity: 6.67871\n",
      "Epoch [2/25], Step [600/41412], Loss: 2.5797, Perplexity: 13.1927\n",
      "Epoch [2/25], Step [700/41412], Loss: 2.4582, Perplexity: 11.6840\n",
      "Epoch [2/25], Step [800/41412], Loss: 2.6612, Perplexity: 14.3130\n",
      "Epoch [2/25], Step [900/41412], Loss: 2.6160, Perplexity: 13.6809\n",
      "Epoch [2/25], Step [1000/41412], Loss: 2.6240, Perplexity: 13.7914\n",
      "Epoch [2/25], Step [1100/41412], Loss: 2.5343, Perplexity: 12.6079\n",
      "Epoch [2/25], Step [1200/41412], Loss: 2.0718, Perplexity: 7.93894\n",
      "Epoch [2/25], Step [1300/41412], Loss: 3.1205, Perplexity: 22.6588\n",
      "Epoch [2/25], Step [1400/41412], Loss: 2.3956, Perplexity: 10.9744\n",
      "Epoch [2/25], Step [1500/41412], Loss: 2.5565, Perplexity: 12.8907\n",
      "Epoch [2/25], Step [1600/41412], Loss: 2.6040, Perplexity: 13.5182\n",
      "Epoch [2/25], Step [1700/41412], Loss: 2.1478, Perplexity: 8.56638\n",
      "Epoch [2/25], Step [1800/41412], Loss: 2.7820, Perplexity: 16.1518\n",
      "Epoch [2/25], Step [1900/41412], Loss: 2.4781, Perplexity: 11.9191\n",
      "Epoch [2/25], Step [2000/41412], Loss: 2.1118, Perplexity: 8.26333\n",
      "Epoch [2/25], Step [2100/41412], Loss: 2.3078, Perplexity: 10.0520\n",
      "Epoch [2/25], Step [2200/41412], Loss: 2.5282, Perplexity: 12.5305\n",
      "Epoch [2/25], Step [2300/41412], Loss: 2.5447, Perplexity: 12.7398\n",
      "Epoch [2/25], Step [2400/41412], Loss: 2.4372, Perplexity: 11.4412\n",
      "Epoch [2/25], Step [2500/41412], Loss: 2.4585, Perplexity: 11.6869\n",
      "Epoch [2/25], Step [2600/41412], Loss: 2.6170, Perplexity: 13.6940\n",
      "Epoch [2/25], Step [2700/41412], Loss: 2.5057, Perplexity: 12.2519\n",
      "Epoch [2/25], Step [2800/41412], Loss: 2.5035, Perplexity: 12.2247\n",
      "Epoch [2/25], Step [2900/41412], Loss: 2.6338, Perplexity: 13.9270\n",
      "Epoch [2/25], Step [3000/41412], Loss: 2.6950, Perplexity: 14.8061\n",
      "Epoch [2/25], Step [3100/41412], Loss: 2.1402, Perplexity: 8.50111\n",
      "Epoch [2/25], Step [3200/41412], Loss: 2.5047, Perplexity: 12.2395\n",
      "Epoch [2/25], Step [3300/41412], Loss: 2.3285, Perplexity: 10.2624\n",
      "Epoch [2/25], Step [3400/41412], Loss: 3.1708, Perplexity: 23.8254\n",
      "Epoch [2/25], Step [3500/41412], Loss: 2.5285, Perplexity: 12.5344\n",
      "Epoch [2/25], Step [3600/41412], Loss: 2.6442, Perplexity: 14.0719\n",
      "Epoch [2/25], Step [3700/41412], Loss: 2.3354, Perplexity: 10.3331\n",
      "Epoch [2/25], Step [3800/41412], Loss: 2.7582, Perplexity: 15.77141\n",
      "Epoch [2/25], Step [3900/41412], Loss: 2.4184, Perplexity: 11.2279\n",
      "Epoch [2/25], Step [4000/41412], Loss: 2.0551, Perplexity: 7.80758\n",
      "Epoch [2/25], Step [4100/41412], Loss: 2.5109, Perplexity: 12.3163\n",
      "Epoch [2/25], Step [4200/41412], Loss: 3.0064, Perplexity: 20.2154\n",
      "Epoch [2/25], Step [4300/41412], Loss: 2.6624, Perplexity: 14.3309\n",
      "Epoch [2/25], Step [4400/41412], Loss: 2.3923, Perplexity: 10.9386\n",
      "Epoch [2/25], Step [4500/41412], Loss: 2.7267, Perplexity: 15.2826\n",
      "Epoch [2/25], Step [4600/41412], Loss: 2.8777, Perplexity: 17.7742\n",
      "Epoch [2/25], Step [4700/41412], Loss: 1.9944, Perplexity: 7.34767\n",
      "Epoch [2/25], Step [4800/41412], Loss: 2.2286, Perplexity: 9.28689\n",
      "Epoch [2/25], Step [4900/41412], Loss: 2.6198, Perplexity: 13.7324\n",
      "Epoch [2/25], Step [5000/41412], Loss: 2.6485, Perplexity: 14.1327\n",
      "Epoch [2/25], Step [5100/41412], Loss: 2.7229, Perplexity: 15.2251\n",
      "Epoch [2/25], Step [5200/41412], Loss: 2.6031, Perplexity: 13.5059\n",
      "Epoch [2/25], Step [5300/41412], Loss: 2.6727, Perplexity: 14.4788\n",
      "Epoch [2/25], Step [5400/41412], Loss: 2.9109, Perplexity: 18.3741\n",
      "Epoch [2/25], Step [5500/41412], Loss: 2.7357, Perplexity: 15.4210\n",
      "Epoch [2/25], Step [5600/41412], Loss: 2.7437, Perplexity: 15.5437\n",
      "Epoch [2/25], Step [5700/41412], Loss: 2.4856, Perplexity: 12.0087\n",
      "Epoch [2/25], Step [5800/41412], Loss: 2.2821, Perplexity: 9.79681\n",
      "Epoch [2/25], Step [5900/41412], Loss: 2.9953, Perplexity: 19.99166\n",
      "Epoch [2/25], Step [6000/41412], Loss: 2.1712, Perplexity: 8.76868\n",
      "Epoch [2/25], Step [6100/41412], Loss: 2.5139, Perplexity: 12.3525\n",
      "Epoch [2/25], Step [6200/41412], Loss: 3.0016, Perplexity: 20.1171\n",
      "Epoch [2/25], Step [6300/41412], Loss: 2.2203, Perplexity: 9.21018\n",
      "Epoch [2/25], Step [6400/41412], Loss: 2.8589, Perplexity: 17.4424\n",
      "Epoch [2/25], Step [6500/41412], Loss: 2.5057, Perplexity: 12.2524\n",
      "Epoch [2/25], Step [6600/41412], Loss: 2.0109, Perplexity: 7.47023\n",
      "Epoch [2/25], Step [6700/41412], Loss: 2.6263, Perplexity: 13.8224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/25], Step [6800/41412], Loss: 2.2863, Perplexity: 9.83810\n",
      "Epoch [2/25], Step [6900/41412], Loss: 2.4959, Perplexity: 12.1325\n",
      "Epoch [2/25], Step [7000/41412], Loss: 2.3972, Perplexity: 10.9929\n",
      "Epoch [2/25], Step [7100/41412], Loss: 2.4437, Perplexity: 11.5152\n",
      "Epoch [2/25], Step [7200/41412], Loss: 2.6927, Perplexity: 14.7718\n",
      "Epoch [2/25], Step [7300/41412], Loss: 2.4505, Perplexity: 11.5943\n",
      "Epoch [2/25], Step [7400/41412], Loss: 2.9593, Perplexity: 19.2854\n",
      "Epoch [2/25], Step [7500/41412], Loss: 2.0666, Perplexity: 7.89799\n",
      "Epoch [2/25], Step [7600/41412], Loss: 2.5432, Perplexity: 12.7202\n",
      "Epoch [2/25], Step [7700/41412], Loss: 2.8001, Perplexity: 16.4465\n",
      "Epoch [2/25], Step [7800/41412], Loss: 2.7084, Perplexity: 15.0049\n",
      "Epoch [2/25], Step [7900/41412], Loss: 1.9845, Perplexity: 7.27520\n",
      "Epoch [2/25], Step [8000/41412], Loss: 2.4657, Perplexity: 11.7723\n",
      "Epoch [2/25], Step [8100/41412], Loss: 2.8080, Perplexity: 16.5769\n",
      "Epoch [2/25], Step [8200/41412], Loss: 2.1231, Perplexity: 8.35694\n",
      "Epoch [2/25], Step [8300/41412], Loss: 2.7395, Perplexity: 15.4787\n",
      "Epoch [2/25], Step [8400/41412], Loss: 3.1903, Perplexity: 24.2955\n",
      "Epoch [2/25], Step [8500/41412], Loss: 2.9727, Perplexity: 19.5447\n",
      "Epoch [2/25], Step [8600/41412], Loss: 2.2592, Perplexity: 9.575910\n",
      "Epoch [2/25], Step [8700/41412], Loss: 2.8972, Perplexity: 18.1232\n",
      "Epoch [2/25], Step [8800/41412], Loss: 2.6103, Perplexity: 13.6026\n",
      "Epoch [2/25], Step [8900/41412], Loss: 2.4249, Perplexity: 11.3010\n",
      "Epoch [2/25], Step [9000/41412], Loss: 3.0745, Perplexity: 21.6397\n",
      "Epoch [2/25], Step [9100/41412], Loss: 2.3339, Perplexity: 10.3185\n",
      "Epoch [2/25], Step [9200/41412], Loss: 2.5697, Perplexity: 13.0614\n",
      "Epoch [2/25], Step [9300/41412], Loss: 2.9852, Perplexity: 19.7902\n",
      "Epoch [2/25], Step [9400/41412], Loss: 2.1722, Perplexity: 8.77731\n",
      "Epoch [2/25], Step [9500/41412], Loss: 3.0468, Perplexity: 21.0480\n",
      "Epoch [2/25], Step [9600/41412], Loss: 2.3404, Perplexity: 10.3859\n",
      "Epoch [2/25], Step [9700/41412], Loss: 2.1863, Perplexity: 8.90215\n",
      "Epoch [2/25], Step [9800/41412], Loss: 2.4731, Perplexity: 11.8593\n",
      "Epoch [2/25], Step [9900/41412], Loss: 2.7951, Perplexity: 16.3644\n",
      "Epoch [2/25], Step [10000/41412], Loss: 2.6607, Perplexity: 14.3061\n",
      "Epoch [2/25], Step [10100/41412], Loss: 2.6085, Perplexity: 13.5781\n",
      "Epoch [2/25], Step [10200/41412], Loss: 2.7131, Perplexity: 15.0766\n",
      "Epoch [2/25], Step [10300/41412], Loss: 2.5592, Perplexity: 12.9251\n",
      "Epoch [2/25], Step [10400/41412], Loss: 1.7378, Perplexity: 5.68508\n",
      "Epoch [2/25], Step [10500/41412], Loss: 2.9116, Perplexity: 18.3853\n",
      "Epoch [2/25], Step [10600/41412], Loss: 2.9816, Perplexity: 19.7184\n",
      "Epoch [2/25], Step [10700/41412], Loss: 2.1913, Perplexity: 8.94702\n",
      "Epoch [2/25], Step [10800/41412], Loss: 2.7928, Perplexity: 16.3275\n",
      "Epoch [2/25], Step [10900/41412], Loss: 2.2207, Perplexity: 9.21413\n",
      "Epoch [2/25], Step [11000/41412], Loss: 2.9063, Perplexity: 18.2891\n",
      "Epoch [2/25], Step [11100/41412], Loss: 2.9167, Perplexity: 18.4797\n",
      "Epoch [2/25], Step [11200/41412], Loss: 2.5951, Perplexity: 13.3984\n",
      "Epoch [2/25], Step [11300/41412], Loss: 2.3733, Perplexity: 10.7328\n",
      "Epoch [2/25], Step [11400/41412], Loss: 2.5264, Perplexity: 12.5086\n",
      "Epoch [2/25], Step [11500/41412], Loss: 2.4174, Perplexity: 11.2172\n",
      "Epoch [2/25], Step [11600/41412], Loss: 2.2689, Perplexity: 9.66838\n",
      "Epoch [2/25], Step [11700/41412], Loss: 2.0885, Perplexity: 8.07264\n",
      "Epoch [2/25], Step [11800/41412], Loss: 2.4495, Perplexity: 11.5821\n",
      "Epoch [2/25], Step [11900/41412], Loss: 2.4554, Perplexity: 11.6516\n",
      "Epoch [2/25], Step [12000/41412], Loss: 2.6672, Perplexity: 14.40037\n",
      "Epoch [2/25], Step [12100/41412], Loss: 2.1191, Perplexity: 8.32344\n",
      "Epoch [2/25], Step [12200/41412], Loss: 2.2993, Perplexity: 9.96725\n",
      "Epoch [2/25], Step [12300/41412], Loss: 3.0647, Perplexity: 21.4289\n",
      "Epoch [2/25], Step [12400/41412], Loss: 2.1730, Perplexity: 8.78489\n",
      "Epoch [2/25], Step [12500/41412], Loss: 2.2832, Perplexity: 9.80763\n",
      "Epoch [2/25], Step [12600/41412], Loss: 2.4382, Perplexity: 11.4529\n",
      "Epoch [2/25], Step [12700/41412], Loss: 2.2517, Perplexity: 9.50415\n",
      "Epoch [2/25], Step [12800/41412], Loss: 2.6773, Perplexity: 14.5454\n",
      "Epoch [2/25], Step [12900/41412], Loss: 2.3800, Perplexity: 10.8044\n",
      "Epoch [2/25], Step [13000/41412], Loss: 2.9940, Perplexity: 19.9654\n",
      "Epoch [2/25], Step [13100/41412], Loss: 2.4585, Perplexity: 11.6868\n",
      "Epoch [2/25], Step [13200/41412], Loss: 2.5988, Perplexity: 13.4479\n",
      "Epoch [2/25], Step [13300/41412], Loss: 2.1592, Perplexity: 8.66422\n",
      "Epoch [2/25], Step [13400/41412], Loss: 2.0271, Perplexity: 7.59191\n",
      "Epoch [2/25], Step [13500/41412], Loss: 2.1777, Perplexity: 8.82569\n",
      "Epoch [2/25], Step [13600/41412], Loss: 2.6278, Perplexity: 13.8435\n",
      "Epoch [2/25], Step [13700/41412], Loss: 2.4767, Perplexity: 11.9023\n",
      "Epoch [2/25], Step [13800/41412], Loss: 2.2572, Perplexity: 9.55588\n",
      "Epoch [2/25], Step [13900/41412], Loss: 2.5112, Perplexity: 12.3193\n",
      "Epoch [2/25], Step [14000/41412], Loss: 3.3185, Perplexity: 27.6197\n",
      "Epoch [2/25], Step [14100/41412], Loss: 2.2490, Perplexity: 9.47878\n",
      "Epoch [2/25], Step [14200/41412], Loss: 2.3869, Perplexity: 10.8793\n",
      "Epoch [2/25], Step [14300/41412], Loss: 2.4865, Perplexity: 12.0191\n",
      "Epoch [2/25], Step [14400/41412], Loss: 2.7730, Perplexity: 16.0073\n",
      "Epoch [2/25], Step [14500/41412], Loss: 2.8434, Perplexity: 17.1739\n",
      "Epoch [2/25], Step [14600/41412], Loss: 2.9773, Perplexity: 19.6348\n",
      "Epoch [2/25], Step [14700/41412], Loss: 2.6780, Perplexity: 14.5564\n",
      "Epoch [2/25], Step [14800/41412], Loss: 2.7172, Perplexity: 15.1383\n",
      "Epoch [2/25], Step [14900/41412], Loss: 2.3746, Perplexity: 10.7466\n",
      "Epoch [2/25], Step [15000/41412], Loss: 2.3695, Perplexity: 10.6924\n",
      "Epoch [2/25], Step [15100/41412], Loss: 2.8142, Perplexity: 16.6806\n",
      "Epoch [2/25], Step [15200/41412], Loss: 2.6703, Perplexity: 14.4446\n",
      "Epoch [2/25], Step [15300/41412], Loss: 2.5909, Perplexity: 13.3418\n",
      "Epoch [2/25], Step [15400/41412], Loss: 3.1602, Perplexity: 23.5743\n",
      "Epoch [2/25], Step [15500/41412], Loss: 2.4407, Perplexity: 11.4807\n",
      "Epoch [2/25], Step [15600/41412], Loss: 2.9636, Perplexity: 19.3666\n",
      "Epoch [2/25], Step [15700/41412], Loss: 2.1458, Perplexity: 8.54902\n",
      "Epoch [2/25], Step [15800/41412], Loss: 2.2461, Perplexity: 9.45046\n",
      "Epoch [2/25], Step [15900/41412], Loss: 2.8425, Perplexity: 17.1579\n",
      "Epoch [2/25], Step [16000/41412], Loss: 2.3480, Perplexity: 10.4648\n",
      "Epoch [2/25], Step [16100/41412], Loss: 2.7464, Perplexity: 15.5869\n",
      "Epoch [2/25], Step [16200/41412], Loss: 2.1124, Perplexity: 8.26790\n",
      "Epoch [2/25], Step [16300/41412], Loss: 2.3721, Perplexity: 10.7195\n",
      "Epoch [2/25], Step [16400/41412], Loss: 2.9711, Perplexity: 19.5127\n",
      "Epoch [2/25], Step [16500/41412], Loss: 2.4846, Perplexity: 11.9965\n",
      "Epoch [2/25], Step [16600/41412], Loss: 2.8463, Perplexity: 17.2235\n",
      "Epoch [2/25], Step [16700/41412], Loss: 2.4386, Perplexity: 11.4570\n",
      "Epoch [2/25], Step [16800/41412], Loss: 2.7707, Perplexity: 15.9704\n",
      "Epoch [2/25], Step [16900/41412], Loss: 2.5143, Perplexity: 12.3580\n",
      "Epoch [2/25], Step [17000/41412], Loss: 2.9667, Perplexity: 19.4272\n",
      "Epoch [2/25], Step [17100/41412], Loss: 2.2924, Perplexity: 9.89857\n",
      "Epoch [2/25], Step [17200/41412], Loss: 2.6214, Perplexity: 13.7550\n",
      "Epoch [2/25], Step [17300/41412], Loss: 2.5648, Perplexity: 12.9977\n",
      "Epoch [2/25], Step [17400/41412], Loss: 2.9215, Perplexity: 18.5691\n",
      "Epoch [2/25], Step [17500/41412], Loss: 2.3565, Perplexity: 10.5541\n",
      "Epoch [2/25], Step [17600/41412], Loss: 2.9367, Perplexity: 18.8526\n",
      "Epoch [2/25], Step [17700/41412], Loss: 2.2018, Perplexity: 9.04140\n",
      "Epoch [2/25], Step [17800/41412], Loss: 2.4150, Perplexity: 11.1899\n",
      "Epoch [2/25], Step [17900/41412], Loss: 2.5429, Perplexity: 12.7166\n",
      "Epoch [2/25], Step [18000/41412], Loss: 2.6761, Perplexity: 14.5287\n",
      "Epoch [2/25], Step [18100/41412], Loss: 2.0947, Perplexity: 8.12285\n",
      "Epoch [2/25], Step [18200/41412], Loss: 2.2334, Perplexity: 9.33126\n",
      "Epoch [2/25], Step [18300/41412], Loss: 2.7597, Perplexity: 15.7957\n",
      "Epoch [2/25], Step [18400/41412], Loss: 3.0781, Perplexity: 21.7170\n",
      "Epoch [2/25], Step [18500/41412], Loss: 2.2387, Perplexity: 9.38119\n",
      "Epoch [2/25], Step [18600/41412], Loss: 2.5603, Perplexity: 12.9395\n",
      "Epoch [2/25], Step [18700/41412], Loss: 2.5779, Perplexity: 13.1697\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/25], Step [18800/41412], Loss: 2.1055, Perplexity: 8.21094\n",
      "Epoch [2/25], Step [18900/41412], Loss: 2.6453, Perplexity: 14.0879\n",
      "Epoch [2/25], Step [19000/41412], Loss: 3.6771, Perplexity: 39.5297\n",
      "Epoch [2/25], Step [19100/41412], Loss: 2.2757, Perplexity: 9.73519\n",
      "Epoch [2/25], Step [19200/41412], Loss: 2.5385, Perplexity: 12.6602\n",
      "Epoch [2/25], Step [19300/41412], Loss: 2.3962, Perplexity: 10.9815\n",
      "Epoch [2/25], Step [19400/41412], Loss: 2.1285, Perplexity: 8.40219\n",
      "Epoch [2/25], Step [19500/41412], Loss: 2.3349, Perplexity: 10.3281\n",
      "Epoch [2/25], Step [19600/41412], Loss: 2.6380, Perplexity: 13.9850\n",
      "Epoch [2/25], Step [19700/41412], Loss: 2.7028, Perplexity: 14.9215\n",
      "Epoch [2/25], Step [19800/41412], Loss: 2.5746, Perplexity: 13.1264\n",
      "Epoch [2/25], Step [19900/41412], Loss: 2.4550, Perplexity: 11.6459\n",
      "Epoch [2/25], Step [20000/41412], Loss: 1.9887, Perplexity: 7.30626\n",
      "Epoch [2/25], Step [20100/41412], Loss: 2.6137, Perplexity: 13.6489\n",
      "Epoch [2/25], Step [20200/41412], Loss: 2.2097, Perplexity: 9.11262\n",
      "Epoch [2/25], Step [20300/41412], Loss: 2.2033, Perplexity: 9.05515\n",
      "Epoch [2/25], Step [20400/41412], Loss: 2.7078, Perplexity: 14.9960\n",
      "Epoch [2/25], Step [20500/41412], Loss: 2.8096, Perplexity: 16.6026\n",
      "Epoch [2/25], Step [20600/41412], Loss: 1.9303, Perplexity: 6.89180\n",
      "Epoch [2/25], Step [20700/41412], Loss: 2.7265, Perplexity: 15.2801\n",
      "Epoch [2/25], Step [20800/41412], Loss: 2.6693, Perplexity: 14.4296\n",
      "Epoch [2/25], Step [20900/41412], Loss: 1.9479, Perplexity: 7.01402\n",
      "Epoch [2/25], Step [21000/41412], Loss: 2.4496, Perplexity: 11.5840\n",
      "Epoch [2/25], Step [21100/41412], Loss: 2.5752, Perplexity: 13.1336\n",
      "Epoch [2/25], Step [21200/41412], Loss: 2.5585, Perplexity: 12.9163\n",
      "Epoch [2/25], Step [21300/41412], Loss: 2.7103, Perplexity: 15.0337\n",
      "Epoch [2/25], Step [21400/41412], Loss: 2.9292, Perplexity: 18.7127\n",
      "Epoch [2/25], Step [21500/41412], Loss: 2.4060, Perplexity: 11.0898\n",
      "Epoch [2/25], Step [21600/41412], Loss: 2.4625, Perplexity: 11.7337\n",
      "Epoch [2/25], Step [21700/41412], Loss: 2.3156, Perplexity: 10.1314\n",
      "Epoch [2/25], Step [21800/41412], Loss: 2.8956, Perplexity: 18.0950\n",
      "Epoch [2/25], Step [21900/41412], Loss: 2.4012, Perplexity: 11.0363\n",
      "Epoch [2/25], Step [22000/41412], Loss: 2.7997, Perplexity: 16.4390\n",
      "Epoch [2/25], Step [22100/41412], Loss: 2.2684, Perplexity: 9.66421\n",
      "Epoch [2/25], Step [22200/41412], Loss: 2.3050, Perplexity: 10.0237\n",
      "Epoch [2/25], Step [22300/41412], Loss: 3.2009, Perplexity: 24.5556\n",
      "Epoch [2/25], Step [22400/41412], Loss: 2.3003, Perplexity: 9.97755\n",
      "Epoch [2/25], Step [22500/41412], Loss: 3.0062, Perplexity: 20.2109\n",
      "Epoch [2/25], Step [22600/41412], Loss: 2.3117, Perplexity: 10.0916\n",
      "Epoch [2/25], Step [22700/41412], Loss: 2.2363, Perplexity: 9.35854\n",
      "Epoch [2/25], Step [22800/41412], Loss: 2.2226, Perplexity: 9.23156\n",
      "Epoch [2/25], Step [22900/41412], Loss: 2.6565, Perplexity: 14.2458\n",
      "Epoch [2/25], Step [23000/41412], Loss: 2.4868, Perplexity: 12.0226\n",
      "Epoch [2/25], Step [23100/41412], Loss: 2.0342, Perplexity: 7.64632\n",
      "Epoch [2/25], Step [23200/41412], Loss: 2.8887, Perplexity: 17.9702\n",
      "Epoch [2/25], Step [23300/41412], Loss: 2.3471, Perplexity: 10.4549\n",
      "Epoch [2/25], Step [23400/41412], Loss: 2.5350, Perplexity: 12.6168\n",
      "Epoch [2/25], Step [23500/41412], Loss: 2.3645, Perplexity: 10.6386\n",
      "Epoch [2/25], Step [23600/41412], Loss: 2.3295, Perplexity: 10.2728\n",
      "Epoch [2/25], Step [23700/41412], Loss: 2.5143, Perplexity: 12.3582\n",
      "Epoch [2/25], Step [23800/41412], Loss: 2.6711, Perplexity: 14.4556\n",
      "Epoch [2/25], Step [23900/41412], Loss: 3.3481, Perplexity: 28.4499\n",
      "Epoch [2/25], Step [24000/41412], Loss: 2.4648, Perplexity: 11.7617\n",
      "Epoch [2/25], Step [24100/41412], Loss: 2.9069, Perplexity: 18.3005\n",
      "Epoch [2/25], Step [24200/41412], Loss: 1.9984, Perplexity: 7.37718\n",
      "Epoch [2/25], Step [24300/41412], Loss: 3.1253, Perplexity: 22.7661\n",
      "Epoch [2/25], Step [24400/41412], Loss: 2.6779, Perplexity: 14.5551\n",
      "Epoch [2/25], Step [24500/41412], Loss: 2.9617, Perplexity: 19.3311\n",
      "Epoch [2/25], Step [24600/41412], Loss: 2.0226, Perplexity: 7.55831\n",
      "Epoch [2/25], Step [24700/41412], Loss: 2.9807, Perplexity: 19.7025\n",
      "Epoch [2/25], Step [24800/41412], Loss: 2.2903, Perplexity: 9.87768\n",
      "Epoch [2/25], Step [24900/41412], Loss: 2.9963, Perplexity: 20.0113\n",
      "Epoch [2/25], Step [25000/41412], Loss: 2.3956, Perplexity: 10.9745\n",
      "Epoch [2/25], Step [25100/41412], Loss: 2.5757, Perplexity: 13.1409\n",
      "Epoch [2/25], Step [25200/41412], Loss: 2.4729, Perplexity: 11.8572\n",
      "Epoch [2/25], Step [25300/41412], Loss: 2.3637, Perplexity: 10.6303\n",
      "Epoch [2/25], Step [25400/41412], Loss: 2.2016, Perplexity: 9.03915\n",
      "Epoch [2/25], Step [25500/41412], Loss: 2.8895, Perplexity: 17.98507\n",
      "Epoch [2/25], Step [25600/41412], Loss: 2.5847, Perplexity: 13.2592\n",
      "Epoch [2/25], Step [25700/41412], Loss: 2.5400, Perplexity: 12.6799\n",
      "Epoch [2/25], Step [25800/41412], Loss: 2.4138, Perplexity: 11.1760\n",
      "Epoch [2/25], Step [25900/41412], Loss: 2.2125, Perplexity: 9.13842\n",
      "Epoch [2/25], Step [26000/41412], Loss: 2.4458, Perplexity: 11.5393\n",
      "Epoch [2/25], Step [26100/41412], Loss: 2.7524, Perplexity: 15.6801\n",
      "Epoch [2/25], Step [26200/41412], Loss: 2.4295, Perplexity: 11.3535\n",
      "Epoch [2/25], Step [26300/41412], Loss: 2.8053, Perplexity: 16.5323\n",
      "Epoch [2/25], Step [26400/41412], Loss: 3.1500, Perplexity: 23.3363\n",
      "Epoch [2/25], Step [26500/41412], Loss: 2.3099, Perplexity: 10.0735\n",
      "Epoch [2/25], Step [26600/41412], Loss: 2.1436, Perplexity: 8.53016\n",
      "Epoch [2/25], Step [26700/41412], Loss: 2.7050, Perplexity: 14.9543\n",
      "Epoch [2/25], Step [26800/41412], Loss: 2.4998, Perplexity: 12.1799\n",
      "Epoch [2/25], Step [26900/41412], Loss: 2.1525, Perplexity: 8.60636\n",
      "Epoch [2/25], Step [27000/41412], Loss: 2.2911, Perplexity: 9.88581\n",
      "Epoch [2/25], Step [27100/41412], Loss: 2.6909, Perplexity: 14.7443\n",
      "Epoch [2/25], Step [27200/41412], Loss: 3.0433, Perplexity: 20.9744\n",
      "Epoch [2/25], Step [27300/41412], Loss: 2.5857, Perplexity: 13.2730\n",
      "Epoch [2/25], Step [27400/41412], Loss: 2.5032, Perplexity: 12.2211\n",
      "Epoch [2/25], Step [27500/41412], Loss: 2.0523, Perplexity: 7.78608\n",
      "Epoch [2/25], Step [27600/41412], Loss: 2.5989, Perplexity: 13.4493\n",
      "Epoch [2/25], Step [27700/41412], Loss: 2.1829, Perplexity: 8.87226\n",
      "Epoch [2/25], Step [27800/41412], Loss: 2.4987, Perplexity: 12.1672\n",
      "Epoch [2/25], Step [27900/41412], Loss: 2.6537, Perplexity: 14.2068\n",
      "Epoch [2/25], Step [28000/41412], Loss: 2.5186, Perplexity: 12.4110\n",
      "Epoch [2/25], Step [28100/41412], Loss: 2.2750, Perplexity: 9.72764\n",
      "Epoch [2/25], Step [28200/41412], Loss: 2.1277, Perplexity: 8.39590\n",
      "Epoch [2/25], Step [28300/41412], Loss: 2.6254, Perplexity: 13.8096\n",
      "Epoch [2/25], Step [28400/41412], Loss: 2.0246, Perplexity: 7.57270\n",
      "Epoch [2/25], Step [28500/41412], Loss: 2.9523, Perplexity: 19.1508\n",
      "Epoch [2/25], Step [28600/41412], Loss: 2.3735, Perplexity: 10.7347\n",
      "Epoch [2/25], Step [28700/41412], Loss: 3.1494, Perplexity: 23.3212\n",
      "Epoch [2/25], Step [28800/41412], Loss: 3.2075, Perplexity: 24.7169\n",
      "Epoch [2/25], Step [28900/41412], Loss: 2.0238, Perplexity: 7.56672\n",
      "Epoch [2/25], Step [29000/41412], Loss: 2.6916, Perplexity: 14.7553\n",
      "Epoch [2/25], Step [29100/41412], Loss: 2.5307, Perplexity: 12.5626\n",
      "Epoch [2/25], Step [29200/41412], Loss: 2.7352, Perplexity: 15.4135\n",
      "Epoch [2/25], Step [29300/41412], Loss: 2.2398, Perplexity: 9.39149\n",
      "Epoch [2/25], Step [29400/41412], Loss: 2.9720, Perplexity: 19.5315\n",
      "Epoch [2/25], Step [29500/41412], Loss: 2.3292, Perplexity: 10.2702\n",
      "Epoch [2/25], Step [29600/41412], Loss: 2.8424, Perplexity: 17.1577\n",
      "Epoch [2/25], Step [29700/41412], Loss: 3.1698, Perplexity: 23.8031\n",
      "Epoch [2/25], Step [29800/41412], Loss: 2.9862, Perplexity: 19.8097\n",
      "Epoch [2/25], Step [29900/41412], Loss: 2.6718, Perplexity: 14.4653\n",
      "Epoch [2/25], Step [30000/41412], Loss: 1.9802, Perplexity: 7.24404\n",
      "Epoch [2/25], Step [30100/41412], Loss: 3.0797, Perplexity: 21.7520\n",
      "Epoch [2/25], Step [30200/41412], Loss: 2.1124, Perplexity: 8.26859\n",
      "Epoch [2/25], Step [30300/41412], Loss: 2.3591, Perplexity: 10.5815\n",
      "Epoch [2/25], Step [30400/41412], Loss: 2.2973, Perplexity: 9.94765\n",
      "Epoch [2/25], Step [30500/41412], Loss: 2.8525, Perplexity: 17.3314\n",
      "Epoch [2/25], Step [30600/41412], Loss: 2.4523, Perplexity: 11.6154\n",
      "Epoch [2/25], Step [30700/41412], Loss: 2.3504, Perplexity: 10.4897\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/25], Step [30800/41412], Loss: 2.9986, Perplexity: 20.0569\n",
      "Epoch [2/25], Step [30900/41412], Loss: 2.1948, Perplexity: 8.97807\n",
      "Epoch [2/25], Step [31000/41412], Loss: 2.6140, Perplexity: 13.6538\n",
      "Epoch [2/25], Step [31100/41412], Loss: 3.2992, Perplexity: 27.0919\n",
      "Epoch [2/25], Step [31200/41412], Loss: 2.8705, Perplexity: 17.6451\n",
      "Epoch [2/25], Step [31300/41412], Loss: 2.1391, Perplexity: 8.49202\n",
      "Epoch [2/25], Step [31400/41412], Loss: 2.0564, Perplexity: 7.81812\n",
      "Epoch [2/25], Step [31500/41412], Loss: 2.3156, Perplexity: 10.1305\n",
      "Epoch [2/25], Step [31600/41412], Loss: 2.5668, Perplexity: 13.0237\n",
      "Epoch [2/25], Step [31700/41412], Loss: 1.8847, Perplexity: 6.58446\n",
      "Epoch [2/25], Step [31800/41412], Loss: 2.3046, Perplexity: 10.0206\n",
      "Epoch [2/25], Step [31900/41412], Loss: 2.5140, Perplexity: 12.3545\n",
      "Epoch [2/25], Step [32000/41412], Loss: 2.4963, Perplexity: 12.1369\n",
      "Epoch [2/25], Step [32100/41412], Loss: 2.2558, Perplexity: 9.54299\n",
      "Epoch [2/25], Step [32200/41412], Loss: 2.2658, Perplexity: 9.63896\n",
      "Epoch [2/25], Step [32300/41412], Loss: 2.3873, Perplexity: 10.8838\n",
      "Epoch [2/25], Step [32400/41412], Loss: 2.6665, Perplexity: 14.3902\n",
      "Epoch [2/25], Step [32500/41412], Loss: 2.7835, Perplexity: 16.1755\n",
      "Epoch [2/25], Step [32600/41412], Loss: 2.9048, Perplexity: 18.26253\n",
      "Epoch [2/25], Step [32700/41412], Loss: 2.5765, Perplexity: 13.1511\n",
      "Epoch [2/25], Step [32800/41412], Loss: 2.7679, Perplexity: 15.9257\n",
      "Epoch [2/25], Step [32900/41412], Loss: 2.2723, Perplexity: 9.70180\n",
      "Epoch [2/25], Step [33000/41412], Loss: 1.9273, Perplexity: 6.87090\n",
      "Epoch [2/25], Step [33100/41412], Loss: 2.6022, Perplexity: 13.4930\n",
      "Epoch [2/25], Step [33200/41412], Loss: 2.4789, Perplexity: 11.9276\n",
      "Epoch [2/25], Step [33300/41412], Loss: 2.0334, Perplexity: 7.64006\n",
      "Epoch [2/25], Step [33400/41412], Loss: 2.2233, Perplexity: 9.23828\n",
      "Epoch [2/25], Step [33500/41412], Loss: 2.2096, Perplexity: 9.11258\n",
      "Epoch [2/25], Step [33600/41412], Loss: 2.2214, Perplexity: 9.21984\n",
      "Epoch [2/25], Step [33700/41412], Loss: 2.4375, Perplexity: 11.4440\n",
      "Epoch [2/25], Step [33800/41412], Loss: 2.7349, Perplexity: 15.4087\n",
      "Epoch [2/25], Step [33900/41412], Loss: 2.9269, Perplexity: 18.6703\n",
      "Epoch [2/25], Step [34000/41412], Loss: 2.3967, Perplexity: 10.9864\n",
      "Epoch [2/25], Step [34100/41412], Loss: 2.2440, Perplexity: 9.43120\n",
      "Epoch [2/25], Step [34200/41412], Loss: 2.9239, Perplexity: 18.6145\n",
      "Epoch [2/25], Step [34300/41412], Loss: 2.4775, Perplexity: 11.9114\n",
      "Epoch [2/25], Step [34400/41412], Loss: 2.8572, Perplexity: 17.4121\n",
      "Epoch [2/25], Step [34500/41412], Loss: 2.6093, Perplexity: 13.5891\n",
      "Epoch [2/25], Step [34600/41412], Loss: 2.1412, Perplexity: 8.50996\n",
      "Epoch [2/25], Step [34700/41412], Loss: 2.5677, Perplexity: 13.0364\n",
      "Epoch [2/25], Step [34800/41412], Loss: 2.5907, Perplexity: 13.3391\n",
      "Epoch [2/25], Step [34900/41412], Loss: 2.0281, Perplexity: 7.59981\n",
      "Epoch [2/25], Step [35000/41412], Loss: 2.2699, Perplexity: 9.67870\n",
      "Epoch [2/25], Step [35100/41412], Loss: 2.4760, Perplexity: 11.8931\n",
      "Epoch [2/25], Step [35200/41412], Loss: 2.1348, Perplexity: 8.45529\n",
      "Epoch [2/25], Step [35300/41412], Loss: 2.1861, Perplexity: 8.90021\n",
      "Epoch [2/25], Step [35400/41412], Loss: 2.4320, Perplexity: 11.3816\n",
      "Epoch [2/25], Step [35500/41412], Loss: 3.3400, Perplexity: 28.2183\n",
      "Epoch [2/25], Step [35600/41412], Loss: 2.0892, Perplexity: 8.07846\n",
      "Epoch [2/25], Step [35700/41412], Loss: 2.4905, Perplexity: 12.0671\n",
      "Epoch [2/25], Step [35800/41412], Loss: 2.6220, Perplexity: 13.7634\n",
      "Epoch [2/25], Step [35900/41412], Loss: 2.4595, Perplexity: 11.6988\n",
      "Epoch [2/25], Step [36000/41412], Loss: 2.3081, Perplexity: 10.0555\n",
      "Epoch [2/25], Step [36100/41412], Loss: 2.4487, Perplexity: 11.5728\n",
      "Epoch [2/25], Step [36200/41412], Loss: 2.5639, Perplexity: 12.9864\n",
      "Epoch [2/25], Step [36300/41412], Loss: 2.5983, Perplexity: 13.4403\n",
      "Epoch [2/25], Step [36400/41412], Loss: 2.6860, Perplexity: 14.67253\n",
      "Epoch [2/25], Step [36500/41412], Loss: 2.5284, Perplexity: 12.5329\n",
      "Epoch [2/25], Step [36600/41412], Loss: 2.3402, Perplexity: 10.3831\n",
      "Epoch [2/25], Step [36700/41412], Loss: 2.1632, Perplexity: 8.69933\n",
      "Epoch [2/25], Step [36800/41412], Loss: 2.1971, Perplexity: 8.99927\n",
      "Epoch [2/25], Step [36900/41412], Loss: 2.5644, Perplexity: 12.9926\n",
      "Epoch [2/25], Step [37000/41412], Loss: 2.6458, Perplexity: 14.0945\n",
      "Epoch [2/25], Step [37100/41412], Loss: 2.2979, Perplexity: 9.95320\n",
      "Epoch [2/25], Step [37200/41412], Loss: 2.3801, Perplexity: 10.8055\n",
      "Epoch [2/25], Step [37300/41412], Loss: 2.2526, Perplexity: 9.51208\n",
      "Epoch [2/25], Step [37400/41412], Loss: 2.4966, Perplexity: 12.1416\n",
      "Epoch [2/25], Step [37500/41412], Loss: 2.3547, Perplexity: 10.5347\n",
      "Epoch [2/25], Step [37600/41412], Loss: 2.4380, Perplexity: 11.4497\n",
      "Epoch [2/25], Step [37700/41412], Loss: 2.6518, Perplexity: 14.1795\n",
      "Epoch [2/25], Step [37800/41412], Loss: 2.4540, Perplexity: 11.6345\n",
      "Epoch [2/25], Step [37900/41412], Loss: 3.1521, Perplexity: 23.3853\n",
      "Epoch [2/25], Step [38000/41412], Loss: 2.5247, Perplexity: 12.4869\n",
      "Epoch [2/25], Step [38100/41412], Loss: 2.3067, Perplexity: 10.0407\n",
      "Epoch [2/25], Step [38200/41412], Loss: 2.4891, Perplexity: 12.0506\n",
      "Epoch [2/25], Step [38300/41412], Loss: 2.3511, Perplexity: 10.4966\n",
      "Epoch [2/25], Step [38400/41412], Loss: 2.2620, Perplexity: 9.60208\n",
      "Epoch [2/25], Step [38500/41412], Loss: 2.5255, Perplexity: 12.4967\n",
      "Epoch [2/25], Step [38600/41412], Loss: 1.9688, Perplexity: 7.16216\n",
      "Epoch [2/25], Step [38700/41412], Loss: 2.6853, Perplexity: 14.6628\n",
      "Epoch [2/25], Step [38800/41412], Loss: 2.2537, Perplexity: 9.52261\n",
      "Epoch [2/25], Step [38900/41412], Loss: 3.3577, Perplexity: 28.7223\n",
      "Epoch [2/25], Step [39000/41412], Loss: 2.3040, Perplexity: 10.0146\n",
      "Epoch [2/25], Step [39100/41412], Loss: 2.7107, Perplexity: 15.0403\n",
      "Epoch [2/25], Step [39200/41412], Loss: 2.2233, Perplexity: 9.23763\n",
      "Epoch [2/25], Step [39300/41412], Loss: 2.6558, Perplexity: 14.2366\n",
      "Epoch [2/25], Step [39400/41412], Loss: 2.3440, Perplexity: 10.4230\n",
      "Epoch [2/25], Step [39500/41412], Loss: 2.6816, Perplexity: 14.6089\n",
      "Epoch [2/25], Step [39600/41412], Loss: 2.5347, Perplexity: 12.6129\n",
      "Epoch [2/25], Step [39700/41412], Loss: 2.3198, Perplexity: 10.1736\n",
      "Epoch [2/25], Step [39800/41412], Loss: 2.7852, Perplexity: 16.2023\n",
      "Epoch [2/25], Step [39900/41412], Loss: 2.4125, Perplexity: 11.1615\n",
      "Epoch [2/25], Step [40000/41412], Loss: 2.3838, Perplexity: 10.8460\n",
      "Epoch [2/25], Step [40100/41412], Loss: 2.3120, Perplexity: 10.0945\n",
      "Epoch [2/25], Step [40200/41412], Loss: 2.3544, Perplexity: 10.5323\n",
      "Epoch [2/25], Step [40300/41412], Loss: 3.0706, Perplexity: 21.5551\n",
      "Epoch [2/25], Step [40400/41412], Loss: 2.2521, Perplexity: 9.50810\n",
      "Epoch [2/25], Step [40500/41412], Loss: 2.3627, Perplexity: 10.6196\n",
      "Epoch [2/25], Step [40600/41412], Loss: 2.6353, Perplexity: 13.9481\n",
      "Epoch [2/25], Step [40700/41412], Loss: 2.6682, Perplexity: 14.4142\n",
      "Epoch [2/25], Step [40800/41412], Loss: 2.1151, Perplexity: 8.29057\n",
      "Epoch [2/25], Step [40900/41412], Loss: 2.2545, Perplexity: 9.53091\n",
      "Epoch [2/25], Step [41000/41412], Loss: 2.2384, Perplexity: 9.37867\n",
      "Epoch [2/25], Step [41100/41412], Loss: 2.5141, Perplexity: 12.3555\n",
      "Epoch [2/25], Step [41200/41412], Loss: 2.5233, Perplexity: 12.4693\n",
      "Epoch [2/25], Step [41300/41412], Loss: 2.4003, Perplexity: 11.0266\n",
      "Epoch [2/25], Step [41400/41412], Loss: 2.3931, Perplexity: 10.9471\n",
      "Epoch [3/25], Step [100/41412], Loss: 2.1192, Perplexity: 8.3243395\n",
      "Epoch [3/25], Step [200/41412], Loss: 2.4120, Perplexity: 11.1565\n",
      "Epoch [3/25], Step [300/41412], Loss: 2.6557, Perplexity: 14.2350\n",
      "Epoch [3/25], Step [400/41412], Loss: 2.6545, Perplexity: 14.2172\n",
      "Epoch [3/25], Step [500/41412], Loss: 2.5003, Perplexity: 12.1864\n",
      "Epoch [3/25], Step [600/41412], Loss: 2.2230, Perplexity: 9.23523\n",
      "Epoch [3/25], Step [700/41412], Loss: 2.2884, Perplexity: 9.85920\n",
      "Epoch [3/25], Step [800/41412], Loss: 2.3570, Perplexity: 10.5589\n",
      "Epoch [3/25], Step [900/41412], Loss: 2.3289, Perplexity: 10.2670\n",
      "Epoch [3/25], Step [1000/41412], Loss: 3.0664, Perplexity: 21.4639\n",
      "Epoch [3/25], Step [1100/41412], Loss: 2.0182, Perplexity: 7.52485\n",
      "Epoch [3/25], Step [1200/41412], Loss: 2.7959, Perplexity: 16.3774\n",
      "Epoch [3/25], Step [1300/41412], Loss: 2.3849, Perplexity: 10.8582\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/25], Step [1400/41412], Loss: 2.2484, Perplexity: 9.47272\n",
      "Epoch [3/25], Step [1500/41412], Loss: 2.4711, Perplexity: 11.8354\n",
      "Epoch [3/25], Step [1600/41412], Loss: 2.3987, Perplexity: 11.0085\n",
      "Epoch [3/25], Step [1700/41412], Loss: 2.4147, Perplexity: 11.1868\n",
      "Epoch [3/25], Step [1800/41412], Loss: 2.5181, Perplexity: 12.4050\n",
      "Epoch [3/25], Step [1900/41412], Loss: 2.4255, Perplexity: 11.3076\n",
      "Epoch [3/25], Step [2000/41412], Loss: 2.3321, Perplexity: 10.2998\n",
      "Epoch [3/25], Step [2100/41412], Loss: 2.3255, Perplexity: 10.2314\n",
      "Epoch [3/25], Step [2200/41412], Loss: 2.7334, Perplexity: 15.3847\n",
      "Epoch [3/25], Step [2300/41412], Loss: 2.6093, Perplexity: 13.5901\n",
      "Epoch [3/25], Step [2400/41412], Loss: 2.6693, Perplexity: 14.42954\n",
      "Epoch [3/25], Step [2500/41412], Loss: 2.3864, Perplexity: 10.8746\n",
      "Epoch [3/25], Step [2600/41412], Loss: 2.4499, Perplexity: 11.5870\n",
      "Epoch [3/25], Step [2700/41412], Loss: 1.8057, Perplexity: 6.08439\n",
      "Epoch [3/25], Step [2800/41412], Loss: 2.9459, Perplexity: 19.0276\n",
      "Epoch [3/25], Step [2900/41412], Loss: 2.5163, Perplexity: 12.3832\n",
      "Epoch [3/25], Step [3000/41412], Loss: 2.1804, Perplexity: 8.85022\n",
      "Epoch [3/25], Step [3100/41412], Loss: 2.1655, Perplexity: 8.71949\n",
      "Epoch [3/25], Step [3200/41412], Loss: 2.7217, Perplexity: 15.2060\n",
      "Epoch [3/25], Step [3300/41412], Loss: 2.6762, Perplexity: 14.5305\n",
      "Epoch [3/25], Step [3400/41412], Loss: 2.5155, Perplexity: 12.3727\n",
      "Epoch [3/25], Step [3500/41412], Loss: 2.2131, Perplexity: 9.14422\n",
      "Epoch [3/25], Step [3600/41412], Loss: 2.8962, Perplexity: 18.1050\n",
      "Epoch [3/25], Step [3700/41412], Loss: 2.6855, Perplexity: 14.6649\n",
      "Epoch [3/25], Step [3800/41412], Loss: 2.4501, Perplexity: 11.5892\n",
      "Epoch [3/25], Step [3900/41412], Loss: 2.8484, Perplexity: 17.2594\n",
      "Epoch [3/25], Step [4000/41412], Loss: 2.3244, Perplexity: 10.2207\n",
      "Epoch [3/25], Step [4100/41412], Loss: 2.6031, Perplexity: 13.5059\n",
      "Epoch [3/25], Step [4200/41412], Loss: 2.3792, Perplexity: 10.7958\n",
      "Epoch [3/25], Step [4300/41412], Loss: 1.8381, Perplexity: 6.28460\n",
      "Epoch [3/25], Step [4400/41412], Loss: 2.4386, Perplexity: 11.4570\n",
      "Epoch [3/25], Step [4500/41412], Loss: 2.4372, Perplexity: 11.4413\n",
      "Epoch [3/25], Step [4600/41412], Loss: 2.5454, Perplexity: 12.7483\n",
      "Epoch [3/25], Step [4700/41412], Loss: 2.5506, Perplexity: 12.8151\n",
      "Epoch [3/25], Step [4800/41412], Loss: 2.3579, Perplexity: 10.5688\n",
      "Epoch [3/25], Step [4900/41412], Loss: 2.6248, Perplexity: 13.8015\n",
      "Epoch [3/25], Step [5000/41412], Loss: 2.4522, Perplexity: 11.61433\n",
      "Epoch [3/25], Step [5100/41412], Loss: 2.5816, Perplexity: 13.2181\n",
      "Epoch [3/25], Step [5200/41412], Loss: 2.5438, Perplexity: 12.7284\n",
      "Epoch [3/25], Step [5300/41412], Loss: 2.7750, Perplexity: 16.0389\n",
      "Epoch [3/25], Step [5400/41412], Loss: 2.0131, Perplexity: 7.48623\n",
      "Epoch [3/25], Step [5500/41412], Loss: 2.3637, Perplexity: 10.6298\n",
      "Epoch [3/25], Step [5600/41412], Loss: 2.8355, Perplexity: 17.0389\n",
      "Epoch [3/25], Step [5700/41412], Loss: 2.0378, Perplexity: 7.67410\n",
      "Epoch [3/25], Step [5800/41412], Loss: 1.9886, Perplexity: 7.30544\n",
      "Epoch [3/25], Step [5900/41412], Loss: 2.3585, Perplexity: 10.5754\n",
      "Epoch [3/25], Step [6000/41412], Loss: 2.5234, Perplexity: 12.4704\n",
      "Epoch [3/25], Step [6100/41412], Loss: 2.5407, Perplexity: 12.6882\n",
      "Epoch [3/25], Step [6200/41412], Loss: 2.1124, Perplexity: 8.26818\n",
      "Epoch [3/25], Step [6300/41412], Loss: 2.2410, Perplexity: 9.40305\n",
      "Epoch [3/25], Step [6400/41412], Loss: 2.1583, Perplexity: 8.65655\n",
      "Epoch [3/25], Step [6500/41412], Loss: 2.2854, Perplexity: 9.82928\n",
      "Epoch [3/25], Step [6600/41412], Loss: 2.2709, Perplexity: 9.68808\n",
      "Epoch [3/25], Step [6700/41412], Loss: 2.8813, Perplexity: 17.8372\n",
      "Epoch [3/25], Step [6800/41412], Loss: 2.2765, Perplexity: 9.74296\n",
      "Epoch [3/25], Step [6900/41412], Loss: 2.5291, Perplexity: 12.5425\n",
      "Epoch [3/25], Step [7000/41412], Loss: 2.2938, Perplexity: 9.91215\n",
      "Epoch [3/25], Step [7100/41412], Loss: 1.9986, Perplexity: 7.37892\n",
      "Epoch [3/25], Step [7200/41412], Loss: 2.1517, Perplexity: 8.59917\n",
      "Epoch [3/25], Step [7300/41412], Loss: 2.5800, Perplexity: 13.1968\n",
      "Epoch [3/25], Step [7400/41412], Loss: 2.2188, Perplexity: 9.19600\n",
      "Epoch [3/25], Step [7500/41412], Loss: 2.4681, Perplexity: 11.8004\n",
      "Epoch [3/25], Step [7600/41412], Loss: 1.9454, Perplexity: 6.99653\n",
      "Epoch [3/25], Step [7700/41412], Loss: 2.3846, Perplexity: 10.8552\n",
      "Epoch [3/25], Step [7800/41412], Loss: 2.6578, Perplexity: 14.2655\n",
      "Epoch [3/25], Step [7900/41412], Loss: 2.6978, Perplexity: 14.8478\n",
      "Epoch [3/25], Step [8000/41412], Loss: 2.4362, Perplexity: 11.4295\n",
      "Epoch [3/25], Step [8100/41412], Loss: 2.4982, Perplexity: 12.1610\n",
      "Epoch [3/25], Step [8200/41412], Loss: 2.9618, Perplexity: 19.3334\n",
      "Epoch [3/25], Step [8300/41412], Loss: 2.7045, Perplexity: 14.9464\n",
      "Epoch [3/25], Step [8400/41412], Loss: 2.3499, Perplexity: 10.4846\n",
      "Epoch [3/25], Step [8500/41412], Loss: 2.5158, Perplexity: 12.3760\n",
      "Epoch [3/25], Step [8600/41412], Loss: 2.1785, Perplexity: 8.83272\n",
      "Epoch [3/25], Step [8700/41412], Loss: 2.3335, Perplexity: 10.3139\n",
      "Epoch [3/25], Step [8800/41412], Loss: 2.4088, Perplexity: 11.1205\n",
      "Epoch [3/25], Step [8900/41412], Loss: 2.3876, Perplexity: 10.8868\n",
      "Epoch [3/25], Step [9000/41412], Loss: 2.6103, Perplexity: 13.6029\n",
      "Epoch [3/25], Step [9100/41412], Loss: 2.3509, Perplexity: 10.4947\n",
      "Epoch [3/25], Step [9200/41412], Loss: 2.3082, Perplexity: 10.0561\n",
      "Epoch [3/25], Step [9300/41412], Loss: 2.6943, Perplexity: 14.7956\n",
      "Epoch [3/25], Step [9400/41412], Loss: 2.1832, Perplexity: 8.87462\n",
      "Epoch [3/25], Step [9500/41412], Loss: 3.0872, Perplexity: 21.9156\n",
      "Epoch [3/25], Step [9600/41412], Loss: 2.1430, Perplexity: 8.52494\n",
      "Epoch [3/25], Step [9700/41412], Loss: 2.9013, Perplexity: 18.1979\n",
      "Epoch [3/25], Step [9800/41412], Loss: 2.5367, Perplexity: 12.6373\n",
      "Epoch [3/25], Step [9900/41412], Loss: 2.9853, Perplexity: 19.79281\n",
      "Epoch [3/25], Step [10000/41412], Loss: 2.2757, Perplexity: 9.7346\n",
      "Epoch [3/25], Step [10100/41412], Loss: 2.2471, Perplexity: 9.46067\n",
      "Epoch [3/25], Step [10200/41412], Loss: 2.0547, Perplexity: 7.80411\n",
      "Epoch [3/25], Step [10300/41412], Loss: 2.1926, Perplexity: 8.95890\n",
      "Epoch [3/25], Step [10400/41412], Loss: 3.0859, Perplexity: 21.8873\n",
      "Epoch [3/25], Step [10500/41412], Loss: 2.6719, Perplexity: 14.4678\n",
      "Epoch [3/25], Step [10600/41412], Loss: 2.8802, Perplexity: 17.8181\n",
      "Epoch [3/25], Step [10700/41412], Loss: 2.9811, Perplexity: 19.7092\n",
      "Epoch [3/25], Step [10800/41412], Loss: 2.6816, Perplexity: 14.6088\n",
      "Epoch [3/25], Step [10900/41412], Loss: 2.5798, Perplexity: 13.1941\n",
      "Epoch [3/25], Step [11000/41412], Loss: 2.5536, Perplexity: 12.8537\n",
      "Epoch [3/25], Step [11100/41412], Loss: 2.3025, Perplexity: 9.99879\n",
      "Epoch [3/25], Step [11200/41412], Loss: 1.9558, Perplexity: 7.06958\n",
      "Epoch [3/25], Step [11300/41412], Loss: 2.7378, Perplexity: 15.4528\n",
      "Epoch [3/25], Step [11400/41412], Loss: 2.5372, Perplexity: 12.6440\n",
      "Epoch [3/25], Step [11500/41412], Loss: 2.1762, Perplexity: 8.81319\n",
      "Epoch [3/25], Step [11600/41412], Loss: 2.3784, Perplexity: 10.7875\n",
      "Epoch [3/25], Step [11700/41412], Loss: 2.3009, Perplexity: 9.98365\n",
      "Epoch [3/25], Step [11800/41412], Loss: 2.6983, Perplexity: 14.8552\n",
      "Epoch [3/25], Step [11900/41412], Loss: 2.4314, Perplexity: 11.3744\n",
      "Epoch [3/25], Step [12000/41412], Loss: 2.5403, Perplexity: 12.6832\n",
      "Epoch [3/25], Step [12100/41412], Loss: 2.2983, Perplexity: 9.95776\n",
      "Epoch [3/25], Step [12200/41412], Loss: 2.3394, Perplexity: 10.3754\n",
      "Epoch [3/25], Step [12300/41412], Loss: 2.3683, Perplexity: 10.6795\n",
      "Epoch [3/25], Step [12400/41412], Loss: 2.1902, Perplexity: 8.93676\n",
      "Epoch [3/25], Step [12500/41412], Loss: 2.2511, Perplexity: 9.49811\n",
      "Epoch [3/25], Step [12600/41412], Loss: 2.8074, Perplexity: 16.5670\n",
      "Epoch [3/25], Step [12700/41412], Loss: 2.3934, Perplexity: 10.9511\n",
      "Epoch [3/25], Step [12800/41412], Loss: 2.7594, Perplexity: 15.7908\n",
      "Epoch [3/25], Step [12900/41412], Loss: 2.5762, Perplexity: 13.1475\n",
      "Epoch [3/25], Step [13000/41412], Loss: 2.6494, Perplexity: 14.1455\n",
      "Epoch [3/25], Step [13100/41412], Loss: 2.4206, Perplexity: 11.2522\n",
      "Epoch [3/25], Step [13200/41412], Loss: 2.7633, Perplexity: 15.8523\n",
      "Epoch [3/25], Step [13300/41412], Loss: 3.0276, Perplexity: 20.6484\n",
      "Epoch [3/25], Step [13400/41412], Loss: 1.9731, Perplexity: 7.19278\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/25], Step [13500/41412], Loss: 2.8795, Perplexity: 17.8045\n",
      "Epoch [3/25], Step [13600/41412], Loss: 2.8304, Perplexity: 16.9523\n",
      "Epoch [3/25], Step [13700/41412], Loss: 2.7732, Perplexity: 16.0093\n",
      "Epoch [3/25], Step [13800/41412], Loss: 2.4711, Perplexity: 11.8358\n",
      "Epoch [3/25], Step [13900/41412], Loss: 2.7199, Perplexity: 15.1789\n",
      "Epoch [3/25], Step [14000/41412], Loss: 2.4754, Perplexity: 11.8864\n",
      "Epoch [3/25], Step [14100/41412], Loss: 2.1949, Perplexity: 8.97922\n",
      "Epoch [3/25], Step [14200/41412], Loss: 2.6407, Perplexity: 14.0233\n",
      "Epoch [3/25], Step [14300/41412], Loss: 2.6586, Perplexity: 14.2763\n",
      "Epoch [3/25], Step [14400/41412], Loss: 2.9751, Perplexity: 19.5925\n",
      "Epoch [3/25], Step [14500/41412], Loss: 2.3581, Perplexity: 10.5712\n",
      "Epoch [3/25], Step [14600/41412], Loss: 2.2609, Perplexity: 9.59156\n",
      "Epoch [3/25], Step [14700/41412], Loss: 2.1085, Perplexity: 8.23622\n",
      "Epoch [3/25], Step [14800/41412], Loss: 2.7818, Perplexity: 16.1478\n",
      "Epoch [3/25], Step [14900/41412], Loss: 2.3755, Perplexity: 10.7559\n",
      "Epoch [3/25], Step [15000/41412], Loss: 2.2619, Perplexity: 9.60100\n",
      "Epoch [3/25], Step [15100/41412], Loss: 2.0917, Perplexity: 8.09909\n",
      "Epoch [3/25], Step [15200/41412], Loss: 2.1763, Perplexity: 8.81387\n",
      "Epoch [3/25], Step [15300/41412], Loss: 2.4810, Perplexity: 11.9526\n",
      "Epoch [3/25], Step [15400/41412], Loss: 2.2276, Perplexity: 9.27758\n",
      "Epoch [3/25], Step [15500/41412], Loss: 2.3520, Perplexity: 10.5061\n",
      "Epoch [3/25], Step [15600/41412], Loss: 2.3294, Perplexity: 10.2716\n",
      "Epoch [3/25], Step [15700/41412], Loss: 2.0645, Perplexity: 7.88108\n",
      "Epoch [3/25], Step [15800/41412], Loss: 2.3048, Perplexity: 10.0223\n",
      "Epoch [3/25], Step [15900/41412], Loss: 2.4982, Perplexity: 12.1603\n",
      "Epoch [3/25], Step [16000/41412], Loss: 2.0019, Perplexity: 7.40283\n",
      "Epoch [3/25], Step [16100/41412], Loss: 2.4677, Perplexity: 11.7957\n",
      "Epoch [3/25], Step [16200/41412], Loss: 2.3444, Perplexity: 10.4272\n",
      "Epoch [3/25], Step [16300/41412], Loss: 1.9389, Perplexity: 6.95090\n",
      "Epoch [3/25], Step [16400/41412], Loss: 2.6995, Perplexity: 14.8717\n",
      "Epoch [3/25], Step [16500/41412], Loss: 2.0381, Perplexity: 7.67618\n",
      "Epoch [3/25], Step [16600/41412], Loss: 2.2668, Perplexity: 9.64892\n",
      "Epoch [3/25], Step [16700/41412], Loss: 2.0581, Perplexity: 7.83123\n",
      "Epoch [3/25], Step [16800/41412], Loss: 2.2365, Perplexity: 9.36050\n",
      "Epoch [3/25], Step [16900/41412], Loss: 1.9902, Perplexity: 7.31694\n",
      "Epoch [3/25], Step [17000/41412], Loss: 2.3809, Perplexity: 10.8141\n",
      "Epoch [3/25], Step [17100/41412], Loss: 2.3269, Perplexity: 10.2460\n",
      "Epoch [3/25], Step [17200/41412], Loss: 2.5092, Perplexity: 12.2948\n",
      "Epoch [3/25], Step [17300/41412], Loss: 2.3884, Perplexity: 10.8966\n",
      "Epoch [3/25], Step [17400/41412], Loss: 2.7677, Perplexity: 15.9214\n",
      "Epoch [3/25], Step [17500/41412], Loss: 2.3596, Perplexity: 10.5872\n",
      "Epoch [3/25], Step [17600/41412], Loss: 2.2603, Perplexity: 9.58630\n",
      "Epoch [3/25], Step [17700/41412], Loss: 2.3775, Perplexity: 10.7778\n",
      "Epoch [3/25], Step [17800/41412], Loss: 2.4019, Perplexity: 11.0443\n",
      "Epoch [3/25], Step [17900/41412], Loss: 2.4339, Perplexity: 11.4032\n",
      "Epoch [3/25], Step [18000/41412], Loss: 2.0989, Perplexity: 8.15707\n",
      "Epoch [3/25], Step [18100/41412], Loss: 2.7162, Perplexity: 15.1230\n",
      "Epoch [3/25], Step [18200/41412], Loss: 2.3869, Perplexity: 10.8800\n",
      "Epoch [3/25], Step [18300/41412], Loss: 2.4301, Perplexity: 11.3598\n",
      "Epoch [3/25], Step [18400/41412], Loss: 2.0629, Perplexity: 7.86872\n",
      "Epoch [3/25], Step [18500/41412], Loss: 1.9584, Perplexity: 7.08809\n",
      "Epoch [3/25], Step [18600/41412], Loss: 2.2685, Perplexity: 9.66483\n",
      "Epoch [3/25], Step [18700/41412], Loss: 2.2878, Perplexity: 9.85352\n",
      "Epoch [3/25], Step [18800/41412], Loss: 3.1893, Perplexity: 24.2706\n",
      "Epoch [3/25], Step [18900/41412], Loss: 2.4810, Perplexity: 11.9533\n",
      "Epoch [3/25], Step [19000/41412], Loss: 2.3918, Perplexity: 10.9337\n",
      "Epoch [3/25], Step [19100/41412], Loss: 2.6841, Perplexity: 14.6455\n",
      "Epoch [3/25], Step [19200/41412], Loss: 2.5056, Perplexity: 12.2506\n",
      "Epoch [3/25], Step [19300/41412], Loss: 2.4808, Perplexity: 11.9503\n",
      "Epoch [3/25], Step [19400/41412], Loss: 3.3596, Perplexity: 28.7771\n",
      "Epoch [3/25], Step [19500/41412], Loss: 2.5681, Perplexity: 13.0408\n",
      "Epoch [3/25], Step [19600/41412], Loss: 2.4331, Perplexity: 11.3941\n",
      "Epoch [3/25], Step [19700/41412], Loss: 1.7596, Perplexity: 5.81004\n",
      "Epoch [3/25], Step [19800/41412], Loss: 2.7677, Perplexity: 15.9215\n",
      "Epoch [3/25], Step [19900/41412], Loss: 2.0629, Perplexity: 7.86910\n",
      "Epoch [3/25], Step [20000/41412], Loss: 2.5471, Perplexity: 12.7695\n",
      "Epoch [3/25], Step [20100/41412], Loss: 2.4751, Perplexity: 11.8830\n",
      "Epoch [3/25], Step [20200/41412], Loss: 1.9784, Perplexity: 7.23087\n",
      "Epoch [3/25], Step [20300/41412], Loss: 2.1479, Perplexity: 8.56686\n",
      "Epoch [3/25], Step [20400/41412], Loss: 2.5108, Perplexity: 12.3153\n",
      "Epoch [3/25], Step [20500/41412], Loss: 2.1091, Perplexity: 8.24055\n",
      "Epoch [3/25], Step [20600/41412], Loss: 1.9018, Perplexity: 6.69787\n",
      "Epoch [3/25], Step [20700/41412], Loss: 2.9638, Perplexity: 19.3712\n",
      "Epoch [3/25], Step [20800/41412], Loss: 2.3770, Perplexity: 10.7723\n",
      "Epoch [3/25], Step [20900/41412], Loss: 2.6993, Perplexity: 14.8693\n",
      "Epoch [3/25], Step [21000/41412], Loss: 2.2826, Perplexity: 9.80180\n",
      "Epoch [3/25], Step [21100/41412], Loss: 2.5822, Perplexity: 13.2267\n",
      "Epoch [3/25], Step [21200/41412], Loss: 2.4911, Perplexity: 12.0745\n",
      "Epoch [3/25], Step [21300/41412], Loss: 2.2769, Perplexity: 9.74669\n",
      "Epoch [3/25], Step [21400/41412], Loss: 2.2103, Perplexity: 9.11802\n",
      "Epoch [3/25], Step [21500/41412], Loss: 2.4938, Perplexity: 12.1067\n",
      "Epoch [3/25], Step [21600/41412], Loss: 2.3077, Perplexity: 10.0515\n",
      "Epoch [3/25], Step [21700/41412], Loss: 3.0870, Perplexity: 21.9115\n",
      "Epoch [3/25], Step [21800/41412], Loss: 2.5588, Perplexity: 12.9198\n",
      "Epoch [3/25], Step [21900/41412], Loss: 2.3212, Perplexity: 10.1881\n",
      "Epoch [3/25], Step [22000/41412], Loss: 2.5452, Perplexity: 12.7454\n",
      "Epoch [3/25], Step [22100/41412], Loss: 2.4120, Perplexity: 11.1563\n",
      "Epoch [3/25], Step [22200/41412], Loss: 2.7232, Perplexity: 15.2284\n",
      "Epoch [3/25], Step [22300/41412], Loss: 2.4786, Perplexity: 11.9251\n",
      "Epoch [3/25], Step [22400/41412], Loss: 2.5617, Perplexity: 12.9580\n",
      "Epoch [3/25], Step [22500/41412], Loss: 2.2999, Perplexity: 9.97351\n",
      "Epoch [3/25], Step [22600/41412], Loss: 2.1047, Perplexity: 8.20445\n",
      "Epoch [3/25], Step [22700/41412], Loss: 2.5686, Perplexity: 13.0480\n",
      "Epoch [3/25], Step [22800/41412], Loss: 3.3822, Perplexity: 29.4360\n",
      "Epoch [3/25], Step [22900/41412], Loss: 2.1258, Perplexity: 8.37995\n",
      "Epoch [3/25], Step [23000/41412], Loss: 2.4747, Perplexity: 11.8785\n",
      "Epoch [3/25], Step [23100/41412], Loss: 2.2444, Perplexity: 9.43436\n",
      "Epoch [3/25], Step [23200/41412], Loss: 2.1576, Perplexity: 8.65013\n",
      "Epoch [3/25], Step [23300/41412], Loss: 3.1269, Perplexity: 22.8037\n",
      "Epoch [3/25], Step [23400/41412], Loss: 2.5106, Perplexity: 12.3127\n",
      "Epoch [3/25], Step [23500/41412], Loss: 2.6510, Perplexity: 14.1677\n",
      "Epoch [3/25], Step [23600/41412], Loss: 2.5436, Perplexity: 12.7260\n",
      "Epoch [3/25], Step [23700/41412], Loss: 2.5382, Perplexity: 12.6568\n",
      "Epoch [3/25], Step [23800/41412], Loss: 1.8745, Perplexity: 6.51767\n",
      "Epoch [3/25], Step [23900/41412], Loss: 2.1547, Perplexity: 8.62495\n",
      "Epoch [3/25], Step [24000/41412], Loss: 2.9835, Perplexity: 19.7576\n",
      "Epoch [3/25], Step [24100/41412], Loss: 2.9230, Perplexity: 18.5974\n",
      "Epoch [3/25], Step [24200/41412], Loss: 2.2623, Perplexity: 9.60561\n",
      "Epoch [3/25], Step [24300/41412], Loss: 2.7109, Perplexity: 15.0435\n",
      "Epoch [3/25], Step [24400/41412], Loss: 2.5007, Perplexity: 12.1907\n",
      "Epoch [3/25], Step [24500/41412], Loss: 3.4732, Perplexity: 32.2400\n",
      "Epoch [3/25], Step [24600/41412], Loss: 2.5464, Perplexity: 12.7613\n",
      "Epoch [3/25], Step [24700/41412], Loss: 2.4365, Perplexity: 11.4330\n",
      "Epoch [3/25], Step [24800/41412], Loss: 2.3205, Perplexity: 10.1813\n",
      "Epoch [3/25], Step [24900/41412], Loss: 2.6582, Perplexity: 14.2710\n",
      "Epoch [3/25], Step [25000/41412], Loss: 2.1828, Perplexity: 8.87136\n",
      "Epoch [3/25], Step [25100/41412], Loss: 2.1324, Perplexity: 8.43503\n",
      "Epoch [3/25], Step [25200/41412], Loss: 2.2743, Perplexity: 9.72123\n",
      "Epoch [3/25], Step [25300/41412], Loss: 2.5217, Perplexity: 12.4495\n",
      "Epoch [3/25], Step [25400/41412], Loss: 2.9921, Perplexity: 19.9274\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/25], Step [25500/41412], Loss: 2.7888, Perplexity: 16.2614\n",
      "Epoch [3/25], Step [25600/41412], Loss: 2.6169, Perplexity: 13.6933\n",
      "Epoch [3/25], Step [25700/41412], Loss: 2.4470, Perplexity: 11.55415\n",
      "Epoch [3/25], Step [25800/41412], Loss: 2.3571, Perplexity: 10.5603\n",
      "Epoch [3/25], Step [25900/41412], Loss: 2.8950, Perplexity: 18.0834\n",
      "Epoch [3/25], Step [26000/41412], Loss: 2.1373, Perplexity: 8.47683\n",
      "Epoch [3/25], Step [26100/41412], Loss: 2.4620, Perplexity: 11.7278\n",
      "Epoch [3/25], Step [26200/41412], Loss: 2.6306, Perplexity: 13.8819\n",
      "Epoch [3/25], Step [26300/41412], Loss: 2.4473, Perplexity: 11.5568\n",
      "Epoch [3/25], Step [26400/41412], Loss: 3.9626, Perplexity: 52.5960\n",
      "Epoch [3/25], Step [26500/41412], Loss: 2.1934, Perplexity: 8.96587\n",
      "Epoch [3/25], Step [26600/41412], Loss: 2.1700, Perplexity: 8.75821\n",
      "Epoch [3/25], Step [26700/41412], Loss: 2.0701, Perplexity: 7.92609\n",
      "Epoch [3/25], Step [26800/41412], Loss: 2.2500, Perplexity: 9.48782\n",
      "Epoch [3/25], Step [26900/41412], Loss: 3.0217, Perplexity: 20.5270\n",
      "Epoch [3/25], Step [27000/41412], Loss: 2.2009, Perplexity: 9.03338\n",
      "Epoch [3/25], Step [27100/41412], Loss: 2.2530, Perplexity: 9.51634\n",
      "Epoch [3/25], Step [27200/41412], Loss: 2.5838, Perplexity: 13.2472\n",
      "Epoch [3/25], Step [27300/41412], Loss: 2.5993, Perplexity: 13.4546\n",
      "Epoch [3/25], Step [27400/41412], Loss: 2.3407, Perplexity: 10.3880\n",
      "Epoch [3/25], Step [27500/41412], Loss: 2.4372, Perplexity: 11.4412\n",
      "Epoch [3/25], Step [27600/41412], Loss: 2.3845, Perplexity: 10.8536\n",
      "Epoch [3/25], Step [27700/41412], Loss: 2.6271, Perplexity: 13.8337\n",
      "Epoch [3/25], Step [27800/41412], Loss: 2.3651, Perplexity: 10.6446\n",
      "Epoch [3/25], Step [27900/41412], Loss: 2.5501, Perplexity: 12.8080\n",
      "Epoch [3/25], Step [28000/41412], Loss: 2.5105, Perplexity: 12.3116\n",
      "Epoch [3/25], Step [28100/41412], Loss: 2.4621, Perplexity: 11.7293\n",
      "Epoch [3/25], Step [28200/41412], Loss: 2.2476, Perplexity: 9.46492\n",
      "Epoch [3/25], Step [28300/41412], Loss: 2.4462, Perplexity: 11.5442\n",
      "Epoch [3/25], Step [28400/41412], Loss: 2.5235, Perplexity: 12.4725\n",
      "Epoch [3/25], Step [28500/41412], Loss: 2.6920, Perplexity: 14.7613\n",
      "Epoch [3/25], Step [28600/41412], Loss: 2.7318, Perplexity: 15.3611\n",
      "Epoch [3/25], Step [28700/41412], Loss: 2.3872, Perplexity: 10.8826\n",
      "Epoch [3/25], Step [28800/41412], Loss: 2.6842, Perplexity: 14.6459\n",
      "Epoch [3/25], Step [28900/41412], Loss: 2.0772, Perplexity: 7.98201\n",
      "Epoch [3/25], Step [29000/41412], Loss: 2.2411, Perplexity: 9.40377\n",
      "Epoch [3/25], Step [29100/41412], Loss: 2.4675, Perplexity: 11.7924\n",
      "Epoch [3/25], Step [29200/41412], Loss: 2.5118, Perplexity: 12.3265\n",
      "Epoch [3/25], Step [29300/41412], Loss: 2.4176, Perplexity: 11.2186\n",
      "Epoch [3/25], Step [29400/41412], Loss: 2.6117, Perplexity: 13.6225\n",
      "Epoch [3/25], Step [29500/41412], Loss: 2.6263, Perplexity: 13.8229\n",
      "Epoch [3/25], Step [29600/41412], Loss: 2.2269, Perplexity: 9.27119\n",
      "Epoch [3/25], Step [29700/41412], Loss: 2.3160, Perplexity: 10.1350\n",
      "Epoch [3/25], Step [29800/41412], Loss: 2.5937, Perplexity: 13.3787\n",
      "Epoch [3/25], Step [29900/41412], Loss: 1.9927, Perplexity: 7.33535\n",
      "Epoch [3/25], Step [30000/41412], Loss: 2.0297, Perplexity: 7.61157\n",
      "Epoch [3/25], Step [30100/41412], Loss: 2.5090, Perplexity: 12.2931\n",
      "Epoch [3/25], Step [30200/41412], Loss: 2.2984, Perplexity: 9.95862\n",
      "Epoch [3/25], Step [30300/41412], Loss: 2.5497, Perplexity: 12.8039\n",
      "Epoch [3/25], Step [30400/41412], Loss: 2.3661, Perplexity: 10.6561\n",
      "Epoch [3/25], Step [30500/41412], Loss: 2.6487, Perplexity: 14.1351\n",
      "Epoch [3/25], Step [30600/41412], Loss: 2.3964, Perplexity: 10.9839\n",
      "Epoch [3/25], Step [30700/41412], Loss: 2.9701, Perplexity: 19.4948\n",
      "Epoch [3/25], Step [30800/41412], Loss: 2.7666, Perplexity: 15.9048\n",
      "Epoch [3/25], Step [30900/41412], Loss: 2.1640, Perplexity: 8.70617\n",
      "Epoch [3/25], Step [31000/41412], Loss: 2.2061, Perplexity: 9.07992\n",
      "Epoch [3/25], Step [31100/41412], Loss: 3.7561, Perplexity: 42.7799\n",
      "Epoch [3/25], Step [31200/41412], Loss: 2.4491, Perplexity: 11.5782\n",
      "Epoch [3/25], Step [31300/41412], Loss: 2.8685, Perplexity: 17.6098\n",
      "Epoch [3/25], Step [31400/41412], Loss: 2.4955, Perplexity: 12.1280\n",
      "Epoch [3/25], Step [31500/41412], Loss: 2.2207, Perplexity: 9.21348\n",
      "Epoch [3/25], Step [31600/41412], Loss: 2.2273, Perplexity: 9.27516\n",
      "Epoch [3/25], Step [31700/41412], Loss: 2.7732, Perplexity: 16.0092\n",
      "Epoch [3/25], Step [31800/41412], Loss: 2.2550, Perplexity: 9.53504\n",
      "Epoch [3/25], Step [31900/41412], Loss: 2.1901, Perplexity: 8.93613\n",
      "Epoch [3/25], Step [32000/41412], Loss: 2.3324, Perplexity: 10.3031\n",
      "Epoch [3/25], Step [32100/41412], Loss: 2.4995, Perplexity: 12.1761\n",
      "Epoch [3/25], Step [32200/41412], Loss: 2.6419, Perplexity: 14.0399\n",
      "Epoch [3/25], Step [32300/41412], Loss: 2.3034, Perplexity: 10.0081\n",
      "Epoch [3/25], Step [32400/41412], Loss: 2.7386, Perplexity: 15.4657\n",
      "Epoch [3/25], Step [32500/41412], Loss: 2.7463, Perplexity: 15.5844\n",
      "Epoch [3/25], Step [32600/41412], Loss: 2.3985, Perplexity: 11.0065\n",
      "Epoch [3/25], Step [32700/41412], Loss: 2.9043, Perplexity: 18.2531\n",
      "Epoch [3/25], Step [32800/41412], Loss: 2.2951, Perplexity: 9.92593\n",
      "Epoch [3/25], Step [32900/41412], Loss: 2.3040, Perplexity: 10.0144\n",
      "Epoch [3/25], Step [33000/41412], Loss: 2.4704, Perplexity: 11.8273\n",
      "Epoch [3/25], Step [33100/41412], Loss: 2.4033, Perplexity: 11.0592\n",
      "Epoch [3/25], Step [33200/41412], Loss: 2.7843, Perplexity: 16.1887\n",
      "Epoch [3/25], Step [33300/41412], Loss: 2.3043, Perplexity: 10.0173\n",
      "Epoch [3/25], Step [33400/41412], Loss: 2.2012, Perplexity: 9.03588\n",
      "Epoch [3/25], Step [33500/41412], Loss: 2.1513, Perplexity: 8.59603\n",
      "Epoch [3/25], Step [33600/41412], Loss: 2.4073, Perplexity: 11.1037\n",
      "Epoch [3/25], Step [33700/41412], Loss: 2.6717, Perplexity: 14.4642\n",
      "Epoch [3/25], Step [33800/41412], Loss: 2.5822, Perplexity: 13.2256\n",
      "Epoch [3/25], Step [33900/41412], Loss: 2.4813, Perplexity: 11.9564\n",
      "Epoch [3/25], Step [34000/41412], Loss: 2.8083, Perplexity: 16.5813\n",
      "Epoch [3/25], Step [34100/41412], Loss: 2.2854, Perplexity: 9.82938\n",
      "Epoch [3/25], Step [34200/41412], Loss: 2.1545, Perplexity: 8.62331\n",
      "Epoch [3/25], Step [34300/41412], Loss: 2.4703, Perplexity: 11.8263\n",
      "Epoch [3/25], Step [34400/41412], Loss: 2.6550, Perplexity: 14.2249\n",
      "Epoch [3/25], Step [34500/41412], Loss: 2.4283, Perplexity: 11.3391\n",
      "Epoch [3/25], Step [34600/41412], Loss: 2.4621, Perplexity: 11.7299\n",
      "Epoch [3/25], Step [34700/41412], Loss: 2.5764, Perplexity: 13.1498\n",
      "Epoch [3/25], Step [34800/41412], Loss: 2.6634, Perplexity: 14.3451\n",
      "Epoch [3/25], Step [34900/41412], Loss: 2.3808, Perplexity: 10.8132\n",
      "Epoch [3/25], Step [35000/41412], Loss: 2.1082, Perplexity: 8.23331\n",
      "Epoch [3/25], Step [35100/41412], Loss: 2.3040, Perplexity: 10.0139\n",
      "Epoch [3/25], Step [35200/41412], Loss: 2.8525, Perplexity: 17.3315\n",
      "Epoch [3/25], Step [35300/41412], Loss: 2.2547, Perplexity: 9.53248\n",
      "Epoch [3/25], Step [35400/41412], Loss: 2.5702, Perplexity: 13.0687\n",
      "Epoch [3/25], Step [35500/41412], Loss: 2.2532, Perplexity: 9.51805\n",
      "Epoch [3/25], Step [35600/41412], Loss: 2.5187, Perplexity: 12.4120\n",
      "Epoch [3/25], Step [35700/41412], Loss: 2.4391, Perplexity: 11.4624\n",
      "Epoch [3/25], Step [35800/41412], Loss: 2.7753, Perplexity: 16.0431\n",
      "Epoch [3/25], Step [35900/41412], Loss: 1.8821, Perplexity: 6.56764\n",
      "Epoch [3/25], Step [36000/41412], Loss: 2.1731, Perplexity: 8.78531\n",
      "Epoch [3/25], Step [36100/41412], Loss: 2.4258, Perplexity: 11.3118\n",
      "Epoch [3/25], Step [36200/41412], Loss: 2.0959, Perplexity: 8.13274\n",
      "Epoch [3/25], Step [36300/41412], Loss: 2.4691, Perplexity: 11.8114\n",
      "Epoch [3/25], Step [36400/41412], Loss: 3.4776, Perplexity: 32.3834\n",
      "Epoch [3/25], Step [36500/41412], Loss: 2.0749, Perplexity: 7.96416\n",
      "Epoch [3/25], Step [36600/41412], Loss: 3.2729, Perplexity: 26.3881\n",
      "Epoch [3/25], Step [36700/41412], Loss: 2.5245, Perplexity: 12.4850\n",
      "Epoch [3/25], Step [36800/41412], Loss: 2.7147, Perplexity: 15.1006\n",
      "Epoch [3/25], Step [36900/41412], Loss: 2.7424, Perplexity: 15.5240\n",
      "Epoch [3/25], Step [37000/41412], Loss: 2.8967, Perplexity: 18.1143\n",
      "Epoch [3/25], Step [37100/41412], Loss: 2.5535, Perplexity: 12.8524\n",
      "Epoch [3/25], Step [37200/41412], Loss: 2.9658, Perplexity: 19.4111\n",
      "Epoch [3/25], Step [37300/41412], Loss: 2.0129, Perplexity: 7.48549\n",
      "Epoch [3/25], Step [37400/41412], Loss: 2.5504, Perplexity: 12.8121\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/25], Step [37500/41412], Loss: 2.3822, Perplexity: 10.8289\n",
      "Epoch [3/25], Step [37600/41412], Loss: 2.5925, Perplexity: 13.3636\n",
      "Epoch [3/25], Step [37700/41412], Loss: 2.0378, Perplexity: 7.67349\n",
      "Epoch [3/25], Step [37800/41412], Loss: 2.2043, Perplexity: 9.06356\n",
      "Epoch [3/25], Step [37900/41412], Loss: 2.9560, Perplexity: 19.2216\n",
      "Epoch [3/25], Step [38000/41412], Loss: 2.2462, Perplexity: 9.45205\n",
      "Epoch [3/25], Step [38100/41412], Loss: 2.6027, Perplexity: 13.4996\n",
      "Epoch [3/25], Step [38200/41412], Loss: 2.5518, Perplexity: 12.8301\n",
      "Epoch [3/25], Step [38300/41412], Loss: 2.0543, Perplexity: 7.80128\n",
      "Epoch [3/25], Step [38400/41412], Loss: 2.7086, Perplexity: 15.0077\n",
      "Epoch [3/25], Step [38500/41412], Loss: 2.4575, Perplexity: 11.6753\n",
      "Epoch [3/25], Step [38600/41412], Loss: 2.4888, Perplexity: 12.0471\n",
      "Epoch [3/25], Step [38700/41412], Loss: 2.8279, Perplexity: 16.9097\n",
      "Epoch [3/25], Step [38800/41412], Loss: 2.4316, Perplexity: 11.3772\n",
      "Epoch [3/25], Step [38900/41412], Loss: 2.3986, Perplexity: 11.0072\n",
      "Epoch [3/25], Step [39000/41412], Loss: 2.3363, Perplexity: 10.3434\n",
      "Epoch [3/25], Step [39100/41412], Loss: 2.3655, Perplexity: 10.6496\n",
      "Epoch [3/25], Step [39200/41412], Loss: 2.9230, Perplexity: 18.5972\n",
      "Epoch [3/25], Step [39300/41412], Loss: 2.4357, Perplexity: 11.4239\n",
      "Epoch [3/25], Step [39400/41412], Loss: 2.3302, Perplexity: 10.2805\n",
      "Epoch [3/25], Step [39500/41412], Loss: 2.9530, Perplexity: 19.1634\n",
      "Epoch [3/25], Step [39600/41412], Loss: 2.5846, Perplexity: 13.2583\n",
      "Epoch [3/25], Step [39700/41412], Loss: 2.2487, Perplexity: 9.47575\n",
      "Epoch [3/25], Step [39800/41412], Loss: 2.6430, Perplexity: 14.0559\n",
      "Epoch [3/25], Step [39900/41412], Loss: 2.1512, Perplexity: 8.59491\n",
      "Epoch [3/25], Step [40000/41412], Loss: 2.3805, Perplexity: 10.8104\n",
      "Epoch [3/25], Step [40100/41412], Loss: 3.0437, Perplexity: 20.9836\n",
      "Epoch [3/25], Step [40200/41412], Loss: 2.1948, Perplexity: 8.97782\n",
      "Epoch [3/25], Step [40300/41412], Loss: 2.5373, Perplexity: 12.6456\n",
      "Epoch [3/25], Step [40400/41412], Loss: 2.9943, Perplexity: 19.9707\n",
      "Epoch [3/25], Step [40500/41412], Loss: 2.2294, Perplexity: 9.29446\n",
      "Epoch [3/25], Step [40600/41412], Loss: 2.3811, Perplexity: 10.8167\n",
      "Epoch [3/25], Step [40700/41412], Loss: 2.6350, Perplexity: 13.9430\n",
      "Epoch [3/25], Step [40800/41412], Loss: 2.7451, Perplexity: 15.5668\n",
      "Epoch [3/25], Step [40900/41412], Loss: 1.9790, Perplexity: 7.23536\n",
      "Epoch [3/25], Step [41000/41412], Loss: 2.8597, Perplexity: 17.4568\n",
      "Epoch [3/25], Step [41100/41412], Loss: 2.7820, Perplexity: 16.1506\n",
      "Epoch [3/25], Step [41200/41412], Loss: 2.4000, Perplexity: 11.0236\n",
      "Epoch [3/25], Step [41300/41412], Loss: 2.5606, Perplexity: 12.9438\n",
      "Epoch [3/25], Step [41400/41412], Loss: 2.2803, Perplexity: 9.77938\n",
      "Epoch [4/25], Step [100/41412], Loss: 2.9048, Perplexity: 18.261913\n",
      "Epoch [4/25], Step [200/41412], Loss: 2.0804, Perplexity: 8.00735\n",
      "Epoch [4/25], Step [300/41412], Loss: 2.1407, Perplexity: 8.50570\n",
      "Epoch [4/25], Step [400/41412], Loss: 2.8867, Perplexity: 17.9339\n",
      "Epoch [4/25], Step [500/41412], Loss: 2.1332, Perplexity: 8.44200\n",
      "Epoch [4/25], Step [600/41412], Loss: 2.5033, Perplexity: 12.2226\n",
      "Epoch [4/25], Step [700/41412], Loss: 2.3938, Perplexity: 10.9549\n",
      "Epoch [4/25], Step [800/41412], Loss: 2.5280, Perplexity: 12.5282\n",
      "Epoch [4/25], Step [900/41412], Loss: 2.3333, Perplexity: 10.3123\n",
      "Epoch [4/25], Step [1000/41412], Loss: 2.6104, Perplexity: 13.6049\n",
      "Epoch [4/25], Step [1100/41412], Loss: 2.4462, Perplexity: 11.5439\n",
      "Epoch [4/25], Step [1200/41412], Loss: 2.3526, Perplexity: 10.5127\n",
      "Epoch [4/25], Step [1300/41412], Loss: 2.3771, Perplexity: 10.7732\n",
      "Epoch [4/25], Step [1400/41412], Loss: 2.5242, Perplexity: 12.4809\n",
      "Epoch [4/25], Step [1500/41412], Loss: 2.3723, Perplexity: 10.7222\n",
      "Epoch [4/25], Step [1600/41412], Loss: 2.1008, Perplexity: 8.17275\n",
      "Epoch [4/25], Step [1700/41412], Loss: 2.7926, Perplexity: 16.3238\n",
      "Epoch [4/25], Step [1800/41412], Loss: 2.1683, Perplexity: 8.74351\n",
      "Epoch [4/25], Step [1900/41412], Loss: 2.3365, Perplexity: 10.3445\n",
      "Epoch [4/25], Step [2000/41412], Loss: 2.3382, Perplexity: 10.3631\n",
      "Epoch [4/25], Step [2100/41412], Loss: 2.6641, Perplexity: 14.3550\n",
      "Epoch [4/25], Step [2200/41412], Loss: 2.0811, Perplexity: 8.01307\n",
      "Epoch [4/25], Step [2300/41412], Loss: 2.7115, Perplexity: 15.0522\n",
      "Epoch [4/25], Step [2400/41412], Loss: 3.0019, Perplexity: 20.1230\n",
      "Epoch [4/25], Step [2500/41412], Loss: 2.9008, Perplexity: 18.1882\n",
      "Epoch [4/25], Step [2600/41412], Loss: 2.8500, Perplexity: 17.2883\n",
      "Epoch [4/25], Step [2700/41412], Loss: 1.9573, Perplexity: 7.08010\n",
      "Epoch [4/25], Step [2800/41412], Loss: 2.3997, Perplexity: 11.0202\n",
      "Epoch [4/25], Step [2900/41412], Loss: 2.1077, Perplexity: 8.22909\n",
      "Epoch [4/25], Step [3000/41412], Loss: 2.8298, Perplexity: 16.9423\n",
      "Epoch [4/25], Step [3100/41412], Loss: 2.2462, Perplexity: 9.45206\n",
      "Epoch [4/25], Step [3200/41412], Loss: 3.1623, Perplexity: 23.6244\n",
      "Epoch [4/25], Step [3300/41412], Loss: 2.4241, Perplexity: 11.2921\n",
      "Epoch [4/25], Step [3400/41412], Loss: 2.5747, Perplexity: 13.1274\n",
      "Epoch [4/25], Step [3500/41412], Loss: 2.1817, Perplexity: 8.86113\n",
      "Epoch [4/25], Step [3600/41412], Loss: 2.7047, Perplexity: 14.9497\n",
      "Epoch [4/25], Step [3700/41412], Loss: 2.2797, Perplexity: 9.77351\n",
      "Epoch [4/25], Step [3800/41412], Loss: 2.3360, Perplexity: 10.3394\n",
      "Epoch [4/25], Step [3900/41412], Loss: 2.5070, Perplexity: 12.2680\n",
      "Epoch [4/25], Step [4000/41412], Loss: 2.6001, Perplexity: 13.4654\n",
      "Epoch [4/25], Step [4100/41412], Loss: 2.2197, Perplexity: 9.20439\n",
      "Epoch [4/25], Step [4200/41412], Loss: 1.9850, Perplexity: 7.27888\n",
      "Epoch [4/25], Step [4300/41412], Loss: 2.3444, Perplexity: 10.4274\n",
      "Epoch [4/25], Step [4400/41412], Loss: 2.3921, Perplexity: 10.9359\n",
      "Epoch [4/25], Step [4500/41412], Loss: 2.3165, Perplexity: 10.1406\n",
      "Epoch [4/25], Step [4600/41412], Loss: 2.7992, Perplexity: 16.4308\n",
      "Epoch [4/25], Step [4700/41412], Loss: 2.2845, Perplexity: 9.82046\n",
      "Epoch [4/25], Step [4800/41412], Loss: 3.0894, Perplexity: 21.9647\n",
      "Epoch [4/25], Step [4900/41412], Loss: 2.4138, Perplexity: 11.1767\n",
      "Epoch [4/25], Step [5000/41412], Loss: 2.0595, Perplexity: 7.84218\n",
      "Epoch [4/25], Step [5100/41412], Loss: 2.9545, Perplexity: 19.1924\n",
      "Epoch [4/25], Step [5200/41412], Loss: 2.4679, Perplexity: 11.7982\n",
      "Epoch [4/25], Step [5300/41412], Loss: 2.0952, Perplexity: 8.12679\n",
      "Epoch [4/25], Step [5400/41412], Loss: 2.5306, Perplexity: 12.5610\n",
      "Epoch [4/25], Step [5500/41412], Loss: 2.3693, Perplexity: 10.6900\n",
      "Epoch [4/25], Step [5600/41412], Loss: 2.6300, Perplexity: 13.8738\n",
      "Epoch [4/25], Step [5700/41412], Loss: 2.2290, Perplexity: 9.29096\n",
      "Epoch [4/25], Step [5800/41412], Loss: 2.4585, Perplexity: 11.6875\n",
      "Epoch [4/25], Step [5900/41412], Loss: 2.7191, Perplexity: 15.1670\n",
      "Epoch [4/25], Step [6000/41412], Loss: 2.4920, Perplexity: 12.0855\n",
      "Epoch [4/25], Step [6100/41412], Loss: 2.6092, Perplexity: 13.5880\n",
      "Epoch [4/25], Step [6200/41412], Loss: 2.4642, Perplexity: 11.7544\n",
      "Epoch [4/25], Step [6300/41412], Loss: 2.4147, Perplexity: 11.1866\n",
      "Epoch [4/25], Step [6400/41412], Loss: 2.2723, Perplexity: 9.70190\n",
      "Epoch [4/25], Step [6500/41412], Loss: 2.3587, Perplexity: 10.5770\n",
      "Epoch [4/25], Step [6600/41412], Loss: 2.0634, Perplexity: 7.87312\n",
      "Epoch [4/25], Step [6700/41412], Loss: 2.3477, Perplexity: 10.4617\n",
      "Epoch [4/25], Step [6800/41412], Loss: 2.5028, Perplexity: 12.2171\n",
      "Epoch [4/25], Step [6900/41412], Loss: 2.2386, Perplexity: 9.38022\n",
      "Epoch [4/25], Step [7000/41412], Loss: 2.1930, Perplexity: 8.96206\n",
      "Epoch [4/25], Step [7100/41412], Loss: 2.4115, Perplexity: 11.1503\n",
      "Epoch [4/25], Step [7200/41412], Loss: 2.4105, Perplexity: 11.1393\n",
      "Epoch [4/25], Step [7300/41412], Loss: 2.4212, Perplexity: 11.2593\n",
      "Epoch [4/25], Step [7400/41412], Loss: 2.1910, Perplexity: 8.94418\n",
      "Epoch [4/25], Step [7500/41412], Loss: 2.6191, Perplexity: 13.7234\n",
      "Epoch [4/25], Step [7600/41412], Loss: 2.3689, Perplexity: 10.6851\n",
      "Epoch [4/25], Step [7700/41412], Loss: 2.4746, Perplexity: 11.8772\n",
      "Epoch [4/25], Step [7800/41412], Loss: 2.8402, Perplexity: 17.1188\n",
      "Epoch [4/25], Step [7900/41412], Loss: 2.7457, Perplexity: 15.5762\n",
      "Epoch [4/25], Step [8000/41412], Loss: 2.5462, Perplexity: 12.7589\n",
      "Epoch [4/25], Step [8100/41412], Loss: 3.5778, Perplexity: 35.7931\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/25], Step [8200/41412], Loss: 2.0105, Perplexity: 7.46684\n",
      "Epoch [4/25], Step [8300/41412], Loss: 2.3020, Perplexity: 9.99371\n",
      "Epoch [4/25], Step [8400/41412], Loss: 2.3079, Perplexity: 10.0533\n",
      "Epoch [4/25], Step [8500/41412], Loss: 2.3430, Perplexity: 10.4126\n",
      "Epoch [4/25], Step [8600/41412], Loss: 2.4679, Perplexity: 11.7982\n",
      "Epoch [4/25], Step [8700/41412], Loss: 2.3321, Perplexity: 10.2993\n",
      "Epoch [4/25], Step [8800/41412], Loss: 2.9283, Perplexity: 18.6960\n",
      "Epoch [4/25], Step [8900/41412], Loss: 2.4557, Perplexity: 11.6546\n",
      "Epoch [4/25], Step [9000/41412], Loss: 1.5559, Perplexity: 4.73952\n",
      "Epoch [4/25], Step [9100/41412], Loss: 2.4726, Perplexity: 11.8528\n",
      "Epoch [4/25], Step [9200/41412], Loss: 3.3235, Perplexity: 27.7575\n",
      "Epoch [4/25], Step [9300/41412], Loss: 2.8062, Perplexity: 16.5474\n",
      "Epoch [4/25], Step [9400/41412], Loss: 2.3938, Perplexity: 10.9553\n",
      "Epoch [4/25], Step [9500/41412], Loss: 2.5484, Perplexity: 12.7871\n",
      "Epoch [4/25], Step [9600/41412], Loss: 2.4267, Perplexity: 11.3217\n",
      "Epoch [4/25], Step [9700/41412], Loss: 2.2232, Perplexity: 9.23713\n",
      "Epoch [4/25], Step [9800/41412], Loss: 2.6025, Perplexity: 13.4977\n",
      "Epoch [4/25], Step [9900/41412], Loss: 2.2981, Perplexity: 9.95543\n",
      "Epoch [4/25], Step [10000/41412], Loss: 2.9490, Perplexity: 19.0871\n",
      "Epoch [4/25], Step [10100/41412], Loss: 2.3653, Perplexity: 10.6476\n",
      "Epoch [4/25], Step [10200/41412], Loss: 2.7847, Perplexity: 16.1947\n",
      "Epoch [4/25], Step [10300/41412], Loss: 2.2807, Perplexity: 9.78353\n",
      "Epoch [4/25], Step [10400/41412], Loss: 3.4318, Perplexity: 30.9328\n",
      "Epoch [4/25], Step [10500/41412], Loss: 2.5883, Perplexity: 13.3069\n",
      "Epoch [4/25], Step [10600/41412], Loss: 2.6180, Perplexity: 13.7080\n",
      "Epoch [4/25], Step [10700/41412], Loss: 2.4210, Perplexity: 11.2566\n",
      "Epoch [4/25], Step [10800/41412], Loss: 2.8053, Perplexity: 16.5328\n",
      "Epoch [4/25], Step [10900/41412], Loss: 2.1089, Perplexity: 8.23915\n",
      "Epoch [4/25], Step [11000/41412], Loss: 2.6764, Perplexity: 14.5325\n",
      "Epoch [4/25], Step [11100/41412], Loss: 2.3561, Perplexity: 10.5495\n",
      "Epoch [4/25], Step [11200/41412], Loss: 3.1171, Perplexity: 22.5802\n",
      "Epoch [4/25], Step [11300/41412], Loss: 2.4898, Perplexity: 12.0586\n",
      "Epoch [4/25], Step [11400/41412], Loss: 2.8284, Perplexity: 16.9184\n",
      "Epoch [4/25], Step [11500/41412], Loss: 2.6802, Perplexity: 14.5887\n",
      "Epoch [4/25], Step [11600/41412], Loss: 2.1783, Perplexity: 8.83096\n",
      "Epoch [4/25], Step [11700/41412], Loss: 2.2203, Perplexity: 9.20978\n",
      "Epoch [4/25], Step [11800/41412], Loss: 2.2899, Perplexity: 9.87377\n",
      "Epoch [4/25], Step [11900/41412], Loss: 2.2734, Perplexity: 9.71210\n",
      "Epoch [4/25], Step [12000/41412], Loss: 2.6058, Perplexity: 13.5426\n",
      "Epoch [4/25], Step [12100/41412], Loss: 2.1003, Perplexity: 8.16866\n",
      "Epoch [4/25], Step [12200/41412], Loss: 2.3317, Perplexity: 10.2956\n",
      "Epoch [4/25], Step [12300/41412], Loss: 2.9205, Perplexity: 18.5515\n",
      "Epoch [4/25], Step [12400/41412], Loss: 2.5881, Perplexity: 13.3046\n",
      "Epoch [4/25], Step [12500/41412], Loss: 2.7012, Perplexity: 14.8977\n",
      "Epoch [4/25], Step [12600/41412], Loss: 3.0629, Perplexity: 21.3894\n",
      "Epoch [4/25], Step [12700/41412], Loss: 2.5411, Perplexity: 12.6938\n",
      "Epoch [4/25], Step [12800/41412], Loss: 1.7389, Perplexity: 5.69090\n",
      "Epoch [4/25], Step [12900/41412], Loss: 2.3791, Perplexity: 10.7954\n",
      "Epoch [4/25], Step [13000/41412], Loss: 2.5023, Perplexity: 12.2109\n",
      "Epoch [4/25], Step [13100/41412], Loss: 2.6537, Perplexity: 14.2069\n",
      "Epoch [4/25], Step [13200/41412], Loss: 2.3385, Perplexity: 10.3661\n",
      "Epoch [4/25], Step [13300/41412], Loss: 2.4085, Perplexity: 11.1172\n",
      "Epoch [4/25], Step [13400/41412], Loss: 2.1278, Perplexity: 8.39649\n",
      "Epoch [4/25], Step [13500/41412], Loss: 2.7187, Perplexity: 15.1604\n",
      "Epoch [4/25], Step [13600/41412], Loss: 2.1435, Perplexity: 8.52922\n",
      "Epoch [4/25], Step [13700/41412], Loss: 2.3627, Perplexity: 10.6192\n",
      "Epoch [4/25], Step [13800/41412], Loss: 2.3271, Perplexity: 10.2484\n",
      "Epoch [4/25], Step [13900/41412], Loss: 2.5747, Perplexity: 13.1274\n",
      "Epoch [4/25], Step [14000/41412], Loss: 2.4496, Perplexity: 11.5837\n",
      "Epoch [4/25], Step [14100/41412], Loss: 2.1881, Perplexity: 8.91842\n",
      "Epoch [4/25], Step [14200/41412], Loss: 2.0109, Perplexity: 7.47021\n",
      "Epoch [4/25], Step [14300/41412], Loss: 2.8135, Perplexity: 16.6678\n",
      "Epoch [4/25], Step [14400/41412], Loss: 2.2485, Perplexity: 9.47336\n",
      "Epoch [4/25], Step [14500/41412], Loss: 2.1795, Perplexity: 8.84173\n",
      "Epoch [4/25], Step [14600/41412], Loss: 2.4227, Perplexity: 11.2763\n",
      "Epoch [4/25], Step [14700/41412], Loss: 2.8365, Perplexity: 17.0568\n",
      "Epoch [4/25], Step [14800/41412], Loss: 2.5167, Perplexity: 12.3870\n",
      "Epoch [4/25], Step [14900/41412], Loss: 2.2144, Perplexity: 9.15551\n",
      "Epoch [4/25], Step [15000/41412], Loss: 2.7545, Perplexity: 15.7124\n",
      "Epoch [4/25], Step [15100/41412], Loss: 2.3323, Perplexity: 10.3016\n",
      "Epoch [4/25], Step [15200/41412], Loss: 2.6056, Perplexity: 13.5399\n",
      "Epoch [4/25], Step [15300/41412], Loss: 2.3994, Perplexity: 11.0168\n",
      "Epoch [4/25], Step [15400/41412], Loss: 2.1798, Perplexity: 8.84444\n",
      "Epoch [4/25], Step [15500/41412], Loss: 1.9793, Perplexity: 7.23767\n",
      "Epoch [4/25], Step [15600/41412], Loss: 2.0719, Perplexity: 7.94011\n",
      "Epoch [4/25], Step [15700/41412], Loss: 2.3704, Perplexity: 10.7020\n",
      "Epoch [4/25], Step [15800/41412], Loss: 2.2164, Perplexity: 9.17416\n",
      "Epoch [4/25], Step [15900/41412], Loss: 2.3226, Perplexity: 10.2019\n",
      "Epoch [4/25], Step [16000/41412], Loss: 2.4669, Perplexity: 11.7854\n",
      "Epoch [4/25], Step [16100/41412], Loss: 2.2081, Perplexity: 9.09870\n",
      "Epoch [4/25], Step [16200/41412], Loss: 2.1827, Perplexity: 8.86991\n",
      "Epoch [4/25], Step [16300/41412], Loss: 1.8718, Perplexity: 6.50018\n",
      "Epoch [4/25], Step [16400/41412], Loss: 1.9704, Perplexity: 7.17373\n",
      "Epoch [4/25], Step [16500/41412], Loss: 2.9272, Perplexity: 18.6749\n",
      "Epoch [4/25], Step [16600/41412], Loss: 2.7028, Perplexity: 14.9221\n",
      "Epoch [4/25], Step [16700/41412], Loss: 2.3668, Perplexity: 10.6629\n",
      "Epoch [4/25], Step [16800/41412], Loss: 2.5089, Perplexity: 12.2916\n",
      "Epoch [4/25], Step [16900/41412], Loss: 2.7397, Perplexity: 15.4817\n",
      "Epoch [4/25], Step [17000/41412], Loss: 2.0190, Perplexity: 7.53080\n",
      "Epoch [4/25], Step [17100/41412], Loss: 2.0455, Perplexity: 7.73309\n",
      "Epoch [4/25], Step [17200/41412], Loss: 2.4296, Perplexity: 11.3539\n",
      "Epoch [4/25], Step [17300/41412], Loss: 2.1464, Perplexity: 8.55397\n",
      "Epoch [4/25], Step [17400/41412], Loss: 2.3031, Perplexity: 10.0055\n",
      "Epoch [4/25], Step [17500/41412], Loss: 2.7537, Perplexity: 15.7009\n",
      "Epoch [4/25], Step [17600/41412], Loss: 2.1959, Perplexity: 8.98846\n",
      "Epoch [4/25], Step [17700/41412], Loss: 2.0012, Perplexity: 7.39775\n",
      "Epoch [4/25], Step [17800/41412], Loss: 2.4457, Perplexity: 11.5384\n",
      "Epoch [4/25], Step [17900/41412], Loss: 2.5463, Perplexity: 12.7596\n",
      "Epoch [4/25], Step [18000/41412], Loss: 2.5354, Perplexity: 12.6214\n",
      "Epoch [4/25], Step [18100/41412], Loss: 3.0597, Perplexity: 21.3211\n",
      "Epoch [4/25], Step [18200/41412], Loss: 2.5573, Perplexity: 12.9009\n",
      "Epoch [4/25], Step [18300/41412], Loss: 2.0966, Perplexity: 8.13825\n",
      "Epoch [4/25], Step [18400/41412], Loss: 2.4810, Perplexity: 11.9537\n",
      "Epoch [4/25], Step [18500/41412], Loss: 2.8502, Perplexity: 17.2906\n",
      "Epoch [4/25], Step [18600/41412], Loss: 2.5436, Perplexity: 12.7256\n",
      "Epoch [4/25], Step [18700/41412], Loss: 2.6204, Perplexity: 13.7405\n",
      "Epoch [4/25], Step [18800/41412], Loss: 2.3715, Perplexity: 10.7138\n",
      "Epoch [4/25], Step [18900/41412], Loss: 2.3595, Perplexity: 10.5853\n",
      "Epoch [4/25], Step [19000/41412], Loss: 2.6258, Perplexity: 13.8162\n",
      "Epoch [4/25], Step [19100/41412], Loss: 2.7512, Perplexity: 15.6610\n",
      "Epoch [4/25], Step [19200/41412], Loss: 2.2591, Perplexity: 9.57447\n",
      "Epoch [4/25], Step [19300/41412], Loss: 2.7270, Perplexity: 15.2877\n",
      "Epoch [4/25], Step [19400/41412], Loss: 2.3185, Perplexity: 10.1606\n",
      "Epoch [4/25], Step [19500/41412], Loss: 2.4579, Perplexity: 11.6804\n",
      "Epoch [4/25], Step [19600/41412], Loss: 2.6739, Perplexity: 14.4970\n",
      "Epoch [4/25], Step [19700/41412], Loss: 2.5569, Perplexity: 12.8961\n",
      "Epoch [4/25], Step [19800/41412], Loss: 2.9039, Perplexity: 18.2454\n",
      "Epoch [4/25], Step [19900/41412], Loss: 2.2324, Perplexity: 9.32219\n",
      "Epoch [4/25], Step [20000/41412], Loss: 2.1628, Perplexity: 8.69520\n",
      "Epoch [4/25], Step [20100/41412], Loss: 2.2020, Perplexity: 9.04337\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/25], Step [20200/41412], Loss: 2.7448, Perplexity: 15.5622\n",
      "Epoch [4/25], Step [20300/41412], Loss: 2.2745, Perplexity: 9.72336\n",
      "Epoch [4/25], Step [20400/41412], Loss: 2.5626, Perplexity: 12.9689\n",
      "Epoch [4/25], Step [20500/41412], Loss: 2.3588, Perplexity: 10.5784\n",
      "Epoch [4/25], Step [20600/41412], Loss: 2.6318, Perplexity: 13.8982\n",
      "Epoch [4/25], Step [20700/41412], Loss: 2.7133, Perplexity: 15.0794\n",
      "Epoch [4/25], Step [20800/41412], Loss: 2.4071, Perplexity: 11.1017\n",
      "Epoch [4/25], Step [20900/41412], Loss: 2.2557, Perplexity: 9.54156\n",
      "Epoch [4/25], Step [21000/41412], Loss: 2.6481, Perplexity: 14.1270\n",
      "Epoch [4/25], Step [21100/41412], Loss: 3.0284, Perplexity: 20.6652\n",
      "Epoch [4/25], Step [21200/41412], Loss: 2.1851, Perplexity: 8.89135\n",
      "Epoch [4/25], Step [21300/41412], Loss: 2.3214, Perplexity: 10.1904\n",
      "Epoch [4/25], Step [21400/41412], Loss: 2.2493, Perplexity: 9.48119\n",
      "Epoch [4/25], Step [21500/41412], Loss: 2.5432, Perplexity: 12.7205\n",
      "Epoch [4/25], Step [21600/41412], Loss: 2.9304, Perplexity: 18.7347\n",
      "Epoch [4/25], Step [21700/41412], Loss: 2.5003, Perplexity: 12.1864\n",
      "Epoch [4/25], Step [21800/41412], Loss: 2.2093, Perplexity: 9.10949\n",
      "Epoch [4/25], Step [21900/41412], Loss: 2.3378, Perplexity: 10.3585\n",
      "Epoch [4/25], Step [22000/41412], Loss: 2.2345, Perplexity: 9.34148\n",
      "Epoch [4/25], Step [22100/41412], Loss: 2.4215, Perplexity: 11.2625\n",
      "Epoch [4/25], Step [22200/41412], Loss: 2.6535, Perplexity: 14.2039\n",
      "Epoch [4/25], Step [22300/41412], Loss: 2.3901, Perplexity: 10.9144\n",
      "Epoch [4/25], Step [22400/41412], Loss: 2.0863, Perplexity: 8.05529\n",
      "Epoch [4/25], Step [22500/41412], Loss: 2.5246, Perplexity: 12.4853\n",
      "Epoch [4/25], Step [22600/41412], Loss: 3.0497, Perplexity: 21.1097\n",
      "Epoch [4/25], Step [22700/41412], Loss: 2.2525, Perplexity: 9.51120\n",
      "Epoch [4/25], Step [22800/41412], Loss: 2.0259, Perplexity: 7.58337\n",
      "Epoch [4/25], Step [22900/41412], Loss: 1.8774, Perplexity: 6.53677\n",
      "Epoch [4/25], Step [23000/41412], Loss: 3.7275, Perplexity: 41.5753\n",
      "Epoch [4/25], Step [23100/41412], Loss: 2.7163, Perplexity: 15.1249\n",
      "Epoch [4/25], Step [23200/41412], Loss: 2.0324, Perplexity: 7.63253\n",
      "Epoch [4/25], Step [23300/41412], Loss: 2.7283, Perplexity: 15.3062\n",
      "Epoch [4/25], Step [23400/41412], Loss: 2.8180, Perplexity: 16.7426\n",
      "Epoch [4/25], Step [23500/41412], Loss: 2.4524, Perplexity: 11.6159\n",
      "Epoch [4/25], Step [23600/41412], Loss: 3.3527, Perplexity: 28.5804\n",
      "Epoch [4/25], Step [23700/41412], Loss: 2.4066, Perplexity: 11.0962\n",
      "Epoch [4/25], Step [23800/41412], Loss: 2.3352, Perplexity: 10.3316\n",
      "Epoch [4/25], Step [23900/41412], Loss: 2.1562, Perplexity: 8.63848\n",
      "Epoch [4/25], Step [24000/41412], Loss: 2.2006, Perplexity: 9.03046\n",
      "Epoch [4/25], Step [24100/41412], Loss: 2.0755, Perplexity: 7.96890\n",
      "Epoch [4/25], Step [24200/41412], Loss: 3.3673, Perplexity: 28.9991\n",
      "Epoch [4/25], Step [24300/41412], Loss: 2.4158, Perplexity: 11.1989\n",
      "Epoch [4/25], Step [24400/41412], Loss: 2.4923, Perplexity: 12.0885\n",
      "Epoch [4/25], Step [24500/41412], Loss: 2.5841, Perplexity: 13.2512\n",
      "Epoch [4/25], Step [24600/41412], Loss: 2.5342, Perplexity: 12.6065\n",
      "Epoch [4/25], Step [24700/41412], Loss: 2.2443, Perplexity: 9.43353\n",
      "Epoch [4/25], Step [24800/41412], Loss: 2.9582, Perplexity: 19.2641\n",
      "Epoch [4/25], Step [24900/41412], Loss: 2.0070, Perplexity: 7.44096\n",
      "Epoch [4/25], Step [25000/41412], Loss: 2.6593, Perplexity: 14.2866\n",
      "Epoch [4/25], Step [25100/41412], Loss: 3.1544, Perplexity: 23.4380\n",
      "Epoch [4/25], Step [25200/41412], Loss: 2.8726, Perplexity: 17.6832\n",
      "Epoch [4/25], Step [25300/41412], Loss: 2.1672, Perplexity: 8.73418\n",
      "Epoch [4/25], Step [25400/41412], Loss: 2.5814, Perplexity: 13.2159\n",
      "Epoch [4/25], Step [25500/41412], Loss: 2.0499, Perplexity: 7.76737\n",
      "Epoch [4/25], Step [25600/41412], Loss: 2.6583, Perplexity: 14.2713\n",
      "Epoch [4/25], Step [25700/41412], Loss: 2.0528, Perplexity: 7.78962\n",
      "Epoch [4/25], Step [25800/41412], Loss: 4.8334, Perplexity: 125.6343\n",
      "Epoch [4/25], Step [25900/41412], Loss: 3.6936, Perplexity: 40.1905\n",
      "Epoch [4/25], Step [26000/41412], Loss: 2.7370, Perplexity: 15.4406\n",
      "Epoch [4/25], Step [26100/41412], Loss: 2.6124, Perplexity: 13.6319\n",
      "Epoch [4/25], Step [26200/41412], Loss: 3.4212, Perplexity: 30.6047\n",
      "Epoch [4/25], Step [26300/41412], Loss: 2.5000, Perplexity: 12.1830\n",
      "Epoch [4/25], Step [26400/41412], Loss: 2.2242, Perplexity: 9.24578\n",
      "Epoch [4/25], Step [26500/41412], Loss: 2.2451, Perplexity: 9.44157\n",
      "Epoch [4/25], Step [26600/41412], Loss: 1.9936, Perplexity: 7.34198\n",
      "Epoch [4/25], Step [26700/41412], Loss: 2.6246, Perplexity: 13.7995\n",
      "Epoch [4/25], Step [26800/41412], Loss: 2.5318, Perplexity: 12.5762\n",
      "Epoch [4/25], Step [26900/41412], Loss: 2.3037, Perplexity: 10.0108\n",
      "Epoch [4/25], Step [27000/41412], Loss: 2.4128, Perplexity: 11.1651\n",
      "Epoch [4/25], Step [27100/41412], Loss: 2.3798, Perplexity: 10.8030\n",
      "Epoch [4/25], Step [27200/41412], Loss: 2.6398, Perplexity: 14.0104\n",
      "Epoch [4/25], Step [27300/41412], Loss: 2.5648, Perplexity: 12.9986\n",
      "Epoch [4/25], Step [27400/41412], Loss: 2.5811, Perplexity: 13.2111\n",
      "Epoch [4/25], Step [27500/41412], Loss: 2.4981, Perplexity: 12.1591\n",
      "Epoch [4/25], Step [27600/41412], Loss: 2.4202, Perplexity: 11.2477\n",
      "Epoch [4/25], Step [27700/41412], Loss: 3.1157, Perplexity: 22.5481\n",
      "Epoch [4/25], Step [27800/41412], Loss: 2.8538, Perplexity: 17.3534\n",
      "Epoch [4/25], Step [27900/41412], Loss: 2.2079, Perplexity: 9.09687\n",
      "Epoch [4/25], Step [28000/41412], Loss: 2.5958, Perplexity: 13.4069\n",
      "Epoch [4/25], Step [28100/41412], Loss: 3.1651, Perplexity: 23.6914\n",
      "Epoch [4/25], Step [28200/41412], Loss: 2.4476, Perplexity: 11.56024\n",
      "Epoch [4/25], Step [28300/41412], Loss: 1.9313, Perplexity: 6.89865\n",
      "Epoch [4/25], Step [28400/41412], Loss: 2.4044, Perplexity: 11.0721\n",
      "Epoch [4/25], Step [28500/41412], Loss: 2.5241, Perplexity: 12.4793\n",
      "Epoch [4/25], Step [28600/41412], Loss: 1.5794, Perplexity: 4.85231\n",
      "Epoch [4/25], Step [28700/41412], Loss: 2.4339, Perplexity: 11.4028\n",
      "Epoch [4/25], Step [28800/41412], Loss: 2.3431, Perplexity: 10.4132\n",
      "Epoch [4/25], Step [28900/41412], Loss: 2.1146, Perplexity: 8.28630\n",
      "Epoch [4/25], Step [29000/41412], Loss: 2.8606, Perplexity: 17.4727\n",
      "Epoch [4/25], Step [29100/41412], Loss: 3.4299, Perplexity: 30.8722\n",
      "Epoch [4/25], Step [29200/41412], Loss: 2.1258, Perplexity: 8.37962\n",
      "Epoch [4/25], Step [29300/41412], Loss: 3.4937, Perplexity: 32.9084\n",
      "Epoch [4/25], Step [29400/41412], Loss: 2.2687, Perplexity: 9.66711\n",
      "Epoch [4/25], Step [29500/41412], Loss: 2.0603, Perplexity: 7.84801\n",
      "Epoch [4/25], Step [29600/41412], Loss: 2.6650, Perplexity: 14.3676\n",
      "Epoch [4/25], Step [29700/41412], Loss: 2.4543, Perplexity: 11.6377\n",
      "Epoch [4/25], Step [29800/41412], Loss: 2.2220, Perplexity: 9.22611\n",
      "Epoch [4/25], Step [29900/41412], Loss: 3.0193, Perplexity: 20.4761\n",
      "Epoch [4/25], Step [30000/41412], Loss: 2.0668, Perplexity: 7.89932\n",
      "Epoch [4/25], Step [30100/41412], Loss: 1.9059, Perplexity: 6.72514\n",
      "Epoch [4/25], Step [30200/41412], Loss: 3.4069, Perplexity: 30.1702\n",
      "Epoch [4/25], Step [30300/41412], Loss: 1.8790, Perplexity: 6.54703\n",
      "Epoch [4/25], Step [30400/41412], Loss: 2.2190, Perplexity: 9.19824\n",
      "Epoch [4/25], Step [30500/41412], Loss: 2.8905, Perplexity: 18.0026\n",
      "Epoch [4/25], Step [30600/41412], Loss: 2.8630, Perplexity: 17.5138\n",
      "Epoch [4/25], Step [30700/41412], Loss: 2.1448, Perplexity: 8.54052\n",
      "Epoch [4/25], Step [30800/41412], Loss: 2.7665, Perplexity: 15.9023\n",
      "Epoch [4/25], Step [30900/41412], Loss: 1.8774, Perplexity: 6.53643\n",
      "Epoch [4/25], Step [31000/41412], Loss: 2.1834, Perplexity: 8.87662\n",
      "Epoch [4/25], Step [31100/41412], Loss: 2.3306, Perplexity: 10.2845\n",
      "Epoch [4/25], Step [31200/41412], Loss: 2.8647, Perplexity: 17.5433\n",
      "Epoch [4/25], Step [31300/41412], Loss: 2.3168, Perplexity: 10.1437\n",
      "Epoch [4/25], Step [31400/41412], Loss: 2.4038, Perplexity: 11.0654\n",
      "Epoch [4/25], Step [31500/41412], Loss: 2.4387, Perplexity: 11.4586\n",
      "Epoch [4/25], Step [31600/41412], Loss: 2.8460, Perplexity: 17.2187\n",
      "Epoch [4/25], Step [31700/41412], Loss: 2.2717, Perplexity: 9.69549\n",
      "Epoch [4/25], Step [31800/41412], Loss: 1.8661, Perplexity: 6.46318\n",
      "Epoch [4/25], Step [31900/41412], Loss: 2.1205, Perplexity: 8.33503\n",
      "Epoch [4/25], Step [32000/41412], Loss: 2.3012, Perplexity: 9.98626\n",
      "Epoch [4/25], Step [32100/41412], Loss: 2.2904, Perplexity: 9.87845\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/25], Step [32200/41412], Loss: 2.6857, Perplexity: 14.6682\n",
      "Epoch [4/25], Step [32300/41412], Loss: 2.1404, Perplexity: 8.50291\n",
      "Epoch [4/25], Step [32400/41412], Loss: 3.3425, Perplexity: 28.2899\n",
      "Epoch [4/25], Step [32500/41412], Loss: 2.1670, Perplexity: 8.73194\n",
      "Epoch [4/25], Step [32600/41412], Loss: 2.6494, Perplexity: 14.1455\n",
      "Epoch [4/25], Step [32700/41412], Loss: 2.6463, Perplexity: 14.1022\n",
      "Epoch [4/25], Step [32800/41412], Loss: 2.1177, Perplexity: 8.31228\n",
      "Epoch [4/25], Step [32900/41412], Loss: 2.7479, Perplexity: 15.6092\n",
      "Epoch [4/25], Step [33000/41412], Loss: 2.5661, Perplexity: 13.0149\n",
      "Epoch [4/25], Step [33100/41412], Loss: 2.7575, Perplexity: 15.7604\n",
      "Epoch [4/25], Step [33200/41412], Loss: 2.7023, Perplexity: 14.9140\n",
      "Epoch [4/25], Step [33300/41412], Loss: 2.5327, Perplexity: 12.5876\n",
      "Epoch [4/25], Step [33400/41412], Loss: 2.2648, Perplexity: 9.628985\n",
      "Epoch [4/25], Step [33500/41412], Loss: 2.1775, Perplexity: 8.82467\n",
      "Epoch [4/25], Step [33600/41412], Loss: 2.6672, Perplexity: 14.3995\n",
      "Epoch [4/25], Step [33700/41412], Loss: 3.0175, Perplexity: 20.4411\n",
      "Epoch [4/25], Step [33800/41412], Loss: 2.1890, Perplexity: 8.92606\n",
      "Epoch [4/25], Step [33900/41412], Loss: 2.6453, Perplexity: 14.0875\n",
      "Epoch [4/25], Step [34000/41412], Loss: 1.9616, Perplexity: 7.11037\n",
      "Epoch [4/25], Step [34100/41412], Loss: 2.7095, Perplexity: 15.0215\n",
      "Epoch [4/25], Step [34200/41412], Loss: 2.2498, Perplexity: 9.48588\n",
      "Epoch [4/25], Step [34300/41412], Loss: 2.4343, Perplexity: 11.4079\n",
      "Epoch [4/25], Step [34400/41412], Loss: 3.1396, Perplexity: 23.0942\n",
      "Epoch [4/25], Step [34500/41412], Loss: 1.9895, Perplexity: 7.31207\n",
      "Epoch [4/25], Step [34600/41412], Loss: 3.3597, Perplexity: 28.7809\n",
      "Epoch [4/25], Step [34700/41412], Loss: 2.7431, Perplexity: 15.5354\n",
      "Epoch [4/25], Step [34800/41412], Loss: 2.1749, Perplexity: 8.80137\n",
      "Epoch [4/25], Step [34900/41412], Loss: 2.5368, Perplexity: 12.6388\n",
      "Epoch [4/25], Step [35000/41412], Loss: 2.8464, Perplexity: 17.2258\n",
      "Epoch [4/25], Step [35100/41412], Loss: 2.2735, Perplexity: 9.71347\n",
      "Epoch [4/25], Step [35200/41412], Loss: 1.8566, Perplexity: 6.40194\n",
      "Epoch [4/25], Step [35300/41412], Loss: 2.2465, Perplexity: 9.45462\n",
      "Epoch [4/25], Step [35400/41412], Loss: 2.4697, Perplexity: 11.8183\n",
      "Epoch [4/25], Step [35500/41412], Loss: 2.0851, Perplexity: 8.04589\n",
      "Epoch [4/25], Step [35600/41412], Loss: 2.2331, Perplexity: 9.32902\n",
      "Epoch [4/25], Step [35700/41412], Loss: 2.8054, Perplexity: 16.5342\n",
      "Epoch [4/25], Step [35800/41412], Loss: 2.4986, Perplexity: 12.1658\n",
      "Epoch [4/25], Step [35900/41412], Loss: 2.2104, Perplexity: 9.11928\n",
      "Epoch [4/25], Step [36000/41412], Loss: 2.1597, Perplexity: 8.66828\n",
      "Epoch [4/25], Step [36100/41412], Loss: 2.0547, Perplexity: 7.80483\n",
      "Epoch [4/25], Step [36200/41412], Loss: 2.3178, Perplexity: 10.1529\n",
      "Epoch [4/25], Step [36300/41412], Loss: 2.1440, Perplexity: 8.53357\n",
      "Epoch [4/25], Step [36400/41412], Loss: 2.5989, Perplexity: 13.4483\n",
      "Epoch [4/25], Step [36500/41412], Loss: 3.1860, Perplexity: 24.1925\n",
      "Epoch [4/25], Step [36600/41412], Loss: 2.4039, Perplexity: 11.0659\n",
      "Epoch [4/25], Step [36700/41412], Loss: 2.3596, Perplexity: 10.5865\n",
      "Epoch [4/25], Step [36800/41412], Loss: 2.4123, Perplexity: 11.1595\n",
      "Epoch [4/25], Step [36900/41412], Loss: 1.7207, Perplexity: 5.58846\n",
      "Epoch [4/25], Step [37000/41412], Loss: 2.1918, Perplexity: 8.95139\n",
      "Epoch [4/25], Step [37100/41412], Loss: 2.1937, Perplexity: 8.96873\n",
      "Epoch [4/25], Step [37200/41412], Loss: 2.9504, Perplexity: 19.1135\n",
      "Epoch [4/25], Step [37300/41412], Loss: 2.4925, Perplexity: 12.0918\n",
      "Epoch [4/25], Step [37400/41412], Loss: 2.4098, Perplexity: 11.1316\n",
      "Epoch [4/25], Step [37500/41412], Loss: 2.8807, Perplexity: 17.8261\n",
      "Epoch [4/25], Step [37600/41412], Loss: 1.8692, Perplexity: 6.48304\n",
      "Epoch [4/25], Step [37700/41412], Loss: 2.1810, Perplexity: 8.85479\n",
      "Epoch [4/25], Step [37800/41412], Loss: 2.3980, Perplexity: 11.0010\n",
      "Epoch [4/25], Step [37900/41412], Loss: 3.0896, Perplexity: 21.9693\n",
      "Epoch [4/25], Step [38000/41412], Loss: 2.2232, Perplexity: 9.23698\n",
      "Epoch [4/25], Step [38100/41412], Loss: 2.4422, Perplexity: 11.4984\n",
      "Epoch [4/25], Step [38200/41412], Loss: 2.4739, Perplexity: 11.8682\n",
      "Epoch [4/25], Step [38300/41412], Loss: 2.4894, Perplexity: 12.0545\n",
      "Epoch [4/25], Step [38400/41412], Loss: 2.1186, Perplexity: 8.31958\n",
      "Epoch [4/25], Step [38500/41412], Loss: 2.5223, Perplexity: 12.4578\n",
      "Epoch [4/25], Step [38600/41412], Loss: 2.3937, Perplexity: 10.9535\n",
      "Epoch [4/25], Step [38700/41412], Loss: 2.1804, Perplexity: 8.85002\n",
      "Epoch [4/25], Step [38800/41412], Loss: 2.5398, Perplexity: 12.6769\n",
      "Epoch [4/25], Step [38900/41412], Loss: 2.1616, Perplexity: 8.68501\n",
      "Epoch [4/25], Step [39000/41412], Loss: 2.1769, Perplexity: 8.81938\n",
      "Epoch [4/25], Step [39100/41412], Loss: 2.1740, Perplexity: 8.79335\n",
      "Epoch [4/25], Step [39200/41412], Loss: 2.0934, Perplexity: 8.11265\n",
      "Epoch [4/25], Step [39300/41412], Loss: 2.3188, Perplexity: 10.1634\n",
      "Epoch [4/25], Step [39400/41412], Loss: 2.5150, Perplexity: 12.3671\n",
      "Epoch [4/25], Step [39500/41412], Loss: 2.1242, Perplexity: 8.36592\n",
      "Epoch [4/25], Step [39600/41412], Loss: 2.3864, Perplexity: 10.8745\n",
      "Epoch [4/25], Step [39700/41412], Loss: 2.3396, Perplexity: 10.3773\n",
      "Epoch [4/25], Step [39800/41412], Loss: 2.4644, Perplexity: 11.7559\n",
      "Epoch [4/25], Step [39900/41412], Loss: 2.2991, Perplexity: 9.96555\n",
      "Epoch [4/25], Step [40000/41412], Loss: 2.3197, Perplexity: 10.1728\n",
      "Epoch [4/25], Step [40100/41412], Loss: 2.0277, Perplexity: 7.59651\n",
      "Epoch [4/25], Step [40200/41412], Loss: 2.3442, Perplexity: 10.4245\n",
      "Epoch [4/25], Step [40300/41412], Loss: 2.2534, Perplexity: 9.520046\n",
      "Epoch [4/25], Step [40400/41412], Loss: 2.3199, Perplexity: 10.1746\n",
      "Epoch [4/25], Step [40500/41412], Loss: 2.7192, Perplexity: 15.1678\n",
      "Epoch [4/25], Step [40600/41412], Loss: 2.3728, Perplexity: 10.7272\n",
      "Epoch [4/25], Step [40700/41412], Loss: 2.2566, Perplexity: 9.55052\n",
      "Epoch [4/25], Step [40800/41412], Loss: 2.1780, Perplexity: 8.82879\n",
      "Epoch [4/25], Step [40900/41412], Loss: 2.2953, Perplexity: 9.92744\n",
      "Epoch [4/25], Step [41000/41412], Loss: 2.8676, Perplexity: 17.5944\n",
      "Epoch [4/25], Step [41100/41412], Loss: 2.5762, Perplexity: 13.1470\n",
      "Epoch [4/25], Step [41200/41412], Loss: 2.7556, Perplexity: 15.7303\n",
      "Epoch [4/25], Step [41300/41412], Loss: 3.5845, Perplexity: 36.0364\n",
      "Epoch [4/25], Step [41400/41412], Loss: 2.8706, Perplexity: 17.6481\n",
      "Epoch [5/25], Step [100/41412], Loss: 2.2279, Perplexity: 9.2805222\n",
      "Epoch [5/25], Step [200/41412], Loss: 2.2893, Perplexity: 9.86769\n",
      "Epoch [5/25], Step [234/41412], Loss: 2.6215, Perplexity: 13.7569"
     ]
    }
   ],
   "source": [
    "import torch.utils.data as data\n",
    "import numpy as np\n",
    "import os\n",
    "import requests\n",
    "import time\n",
    "\n",
    "# temporary\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from model import EncoderCNN, DecoderRNN\n",
    "\n",
    "# Open the training log file.\n",
    "f = open(log_file, 'w')\n",
    "\n",
    "## running the training locally\n",
    "#old_time = time.time()\n",
    "#response = requests.request(\"GET\", \n",
    "#                            \"http://metadata.google.internal/computeMetadata/v1/instance/attributes/keep_alive_token\", \n",
    "#                            headers={\"Metadata-Flavor\":\"Google\"})\n",
    "\n",
    "\n",
    "for epoch in range(1, num_epochs+1):\n",
    "\n",
    "    for i_step in range(1, total_step+1):\n",
    "        \n",
    "        ## running the training locally\n",
    "        #if time.time() - old_time > 60:\n",
    "        #    old_time = time.time()\n",
    "        #    requests.request(\"POST\", \n",
    "        #                     \"https://nebula.udacity.com/api/v1/remote/keep-alive\", \n",
    "        #                     headers={'Authorization': \"STAR \" + response.text})\n",
    "\n",
    "        # Randomly sample a caption length, and sample indices with that length.\n",
    "        indices = data_loader.dataset.get_train_indices()\n",
    "        # Create and assign a batch sampler to retrieve a batch with the sampled indices.\n",
    "        new_sampler = data.sampler.SubsetRandomSampler(indices=indices)\n",
    "        data_loader.batch_sampler.sampler = new_sampler\n",
    "\n",
    "        # Obtain the batch.\n",
    "        images, captions = next(iter(data_loader))\n",
    "\n",
    "        # Move batch of images and captions to GPU if CUDA is available.\n",
    "        images = images.to(device)\n",
    "        captions = captions.to(device)\n",
    "\n",
    "        # Zero the gradients.\n",
    "        decoder.zero_grad()\n",
    "        encoder.zero_grad()\n",
    "\n",
    "        # Pass the inputs through the CNN-RNN model.\n",
    "        features = encoder(images)\n",
    "        outputs = decoder(features, captions)\n",
    "\n",
    "        # Calculate the batch loss.\n",
    "        loss = criterion(outputs.contiguous().view(-1, vocab_size), captions.contiguous().view(-1))\n",
    "\n",
    "        # Backward pass.\n",
    "        loss.backward(retain_graph=True)\n",
    "\n",
    "        # Update the parameters in the optimizer.\n",
    "        optimizer.step()\n",
    "\n",
    "        # Get training statistics.\n",
    "        stats = 'Epoch [%d/%d], Step [%d/%d], Loss: %.4f, Perplexity: %5.4f' % (epoch, num_epochs, i_step, total_step, loss.item(), np.exp(loss.item()))\n",
    "\n",
    "        # Print training statistics (on same line).\n",
    "        print('\\r' + stats, end=\"\")\n",
    "        sys.stdout.flush()\n",
    "\n",
    "        # Print training statistics to file.\n",
    "        f.write(stats + '\\n')\n",
    "        f.flush()\n",
    "\n",
    "        # Print training statistics (on different line).\n",
    "        if i_step % print_every == 0:\n",
    "            print('\\r' + stats)\n",
    "\n",
    "    # Save the weights.\n",
    "    if epoch % save_every == 0:\n",
    "        torch.save(decoder.state_dict(), os.path.join('./models', 'decoder-%d.pkl' % epoch))\n",
    "        torch.save(encoder.state_dict(), os.path.join('./models', 'encoder-%d.pkl' % epoch))\n",
    "\n",
    "# Close the training log file.\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step3'></a>\n",
    "## Step 3: (Optional) Validate your Model\n",
    "\n",
    "To assess potential overfitting, one approach is to assess performance on a validation set.  If you decide to do this **optional** task, you are required to first complete all of the steps in the next notebook in the sequence (**3_Inference.ipynb**); as part of that notebook, you will write and test code (specifically, the `sample` method in the `DecoderRNN` class) that uses your RNN decoder to generate captions.  That code will prove incredibly useful here. \n",
    "\n",
    "If you decide to validate your model, please do not edit the data loader in **data_loader.py**.  Instead, create a new file named **data_loader_val.py** containing the code for obtaining the data loader for the validation data.  You can access:\n",
    "- the validation images at filepath `'/opt/cocoapi/images/train2014/'`, and\n",
    "- the validation image caption annotation file at filepath `'/opt/cocoapi/annotations/captions_val2014.json'`.\n",
    "\n",
    "The suggested approach to validating your model involves creating a json file such as [this one](https://github.com/cocodataset/cocoapi/blob/master/results/captions_val2014_fakecap_results.json) containing your model's predicted captions for the validation images.  Then, you can write your own script or use one that you [find online](https://github.com/tylin/coco-caption) to calculate the BLEU score of your model.  You can read more about the BLEU score, along with other evaluation metrics (such as TEOR and Cider) in section 4.1 of [this paper](https://arxiv.org/pdf/1411.4555.pdf).  For more information about how to use the annotation file, check out the [website](http://cocodataset.org/#download) for the COCO dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Optional) TODO: Validate your model."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
