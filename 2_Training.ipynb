{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computer Vision Nanodegree\n",
    "\n",
    "## Project: Image Captioning\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, you will train your CNN-RNN model.  \n",
    "\n",
    "You are welcome and encouraged to try out many different architectures and hyperparameters when searching for a good model.\n",
    "\n",
    "This does have the potential to make the project quite messy!  Before submitting your project, make sure that you clean up:\n",
    "- the code you write in this notebook.  The notebook should describe how to train a single CNN-RNN architecture, corresponding to your final choice of hyperparameters.  You should structure the notebook so that the reviewer can replicate your results by running the code in this notebook.  \n",
    "- the output of the code cell in **Step 2**.  The output should show the output obtained when training the model from scratch.\n",
    "\n",
    "This notebook **will be graded**.  \n",
    "\n",
    "Feel free to use the links below to navigate the notebook:\n",
    "- [Step 1](#step1): Training Setup\n",
    "- [Step 2](#step2): Train your Model\n",
    "- [Step 3](#step3): (Optional) Validate your Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step1'></a>\n",
    "## Step 1: Training Setup\n",
    "\n",
    "In this step of the notebook, you will customize the training of your CNN-RNN model by specifying hyperparameters and setting other options that are important to the training procedure.  The values you set now will be used when training your model in **Step 2** below.\n",
    "\n",
    "You should only amend blocks of code that are preceded by a `TODO` statement.  **Any code blocks that are not preceded by a `TODO` statement should not be modified**.\n",
    "\n",
    "### Task #1\n",
    "\n",
    "Begin by setting the following variables:\n",
    "- `batch_size` - the batch size of each training batch.  It is the number of image-caption pairs used to amend the model weights in each training step. \n",
    "- `vocab_threshold` - the minimum word count threshold.  Note that a larger threshold will result in a smaller vocabulary, whereas a smaller threshold will include rarer words and result in a larger vocabulary.  \n",
    "- `vocab_from_file` - a Boolean that decides whether to load the vocabulary from file. \n",
    "- `embed_size` - the dimensionality of the image and word embeddings.  \n",
    "- `hidden_size` - the number of features in the hidden state of the RNN decoder.  \n",
    "- `num_epochs` - the number of epochs to train the model.  We recommend that you set `num_epochs=3`, but feel free to increase or decrease this number as you wish.  [This paper](https://arxiv.org/pdf/1502.03044.pdf) trained a captioning model on a single state-of-the-art GPU for 3 days, but you'll soon see that you can get reasonable results in a matter of a few hours!  (_But of course, if you want your model to compete with current research, you will have to train for much longer._)\n",
    "- `save_every` - determines how often to save the model weights.  We recommend that you set `save_every=1`, to save the model weights after each epoch.  This way, after the `i`th epoch, the encoder and decoder weights will be saved in the `models/` folder as `encoder-i.pkl` and `decoder-i.pkl`, respectively.\n",
    "- `print_every` - determines how often to print the batch loss to the Jupyter notebook while training.  Note that you **will not** observe a monotonic decrease in the loss function while training - this is perfectly fine and completely expected!  You are encouraged to keep this at its default value of `100` to avoid clogging the notebook, but feel free to change it.\n",
    "- `log_file` - the name of the text file containing - for every step - how the loss and perplexity evolved during training.\n",
    "\n",
    "If you're not sure where to begin to set some of the values above, you can peruse [this paper](https://arxiv.org/pdf/1502.03044.pdf) and [this paper](https://arxiv.org/pdf/1411.4555.pdf) for useful guidance!  **To avoid spending too long on this notebook**, you are encouraged to consult these suggested research papers to obtain a strong initial guess for which hyperparameters are likely to work best.  Then, train a single model, and proceed to the next notebook (**3_Inference.ipynb**).  If you are unhappy with your performance, you can return to this notebook to tweak the hyperparameters (and/or the architecture in **model.py**) and re-train your model.\n",
    "\n",
    "### Question 1\n",
    "\n",
    "**Question:** Describe your CNN-RNN architecture in detail.  With this architecture in mind, how did you select the values of the variables in Task 1?  If you consulted a research paper detailing a successful implementation of an image captioning model, please provide the reference.\n",
    "\n",
    "**Answer:** \n",
    "\n",
    "\n",
    "### (Optional) Task #2\n",
    "\n",
    "Note that we have provided a recommended image transform `transform_train` for pre-processing the training images, but you are welcome (and encouraged!) to modify it as you wish.  When modifying this transform, keep in mind that:\n",
    "- the images in the dataset have varying heights and widths, and \n",
    "- if using a pre-trained model, you must perform the corresponding appropriate normalization.\n",
    "\n",
    "### Question 2\n",
    "\n",
    "**Question:** How did you select the transform in `transform_train`?  If you left the transform at its provided value, why do you think that it is a good choice for your CNN architecture?\n",
    "\n",
    "**Answer:** The default looks reasonable, although I have some doubts as to whether it makes sense to discard the data around the edge in this case. \n",
    "\n",
    "### Task #3\n",
    "\n",
    "Next, you will specify a Python list containing the learnable parameters of the model.  For instance, if you decide to make all weights in the decoder trainable, but only want to train the weights in the embedding layer of the encoder, then you should set `params` to something like:\n",
    "```\n",
    "params = list(decoder.parameters()) + list(encoder.embed.parameters()) \n",
    "```\n",
    "\n",
    "### Question 3\n",
    "\n",
    "**Question:** How did you select the trainable parameters of your architecture?  Why do you think this is a good choice?\n",
    "\n",
    "**Answer:** \n",
    "\n",
    "### Task #4\n",
    "\n",
    "Finally, you will select an [optimizer](http://pytorch.org/docs/master/optim.html#torch.optim.Optimizer).\n",
    "\n",
    "### Question 4\n",
    "\n",
    "**Question:** How did you select the optimizer used to train your model?\n",
    "\n",
    "**Answer:** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/sthenc/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.52s)\n",
      "creating index...\n",
      "index created!\n",
      "[0/414113] Tokenizing captions...\n",
      "[100000/414113] Tokenizing captions...\n",
      "[200000/414113] Tokenizing captions...\n",
      "[300000/414113] Tokenizing captions...\n",
      "[400000/414113] Tokenizing captions...\n",
      "loading annotations into memory...\n",
      "Done (t=0.52s)\n",
      "creating index...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 817/414113 [00:00<00:50, 8165.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index created!\n",
      "Obtaining caption lengths...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 414113/414113 [00:42<00:00, 9735.74it/s] \n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "import sys\n",
    "sys.path.append('/opt/cocoapi/PythonAPI')\n",
    "from pycocotools.coco import COCO\n",
    "from data_loader import get_loader\n",
    "from model import EncoderCNN, DecoderRNN\n",
    "import math\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "## TODO #1: Select appropriate values for the Python variables below.\n",
    "batch_size=10\n",
    "#batch_size = 64          # batch size, we have 10GB of GPU memory, let's use it\n",
    "vocab_threshold = 5        # minimum word count threshold\n",
    "vocab_from_file = False    # if True, load existing vocab file\n",
    "embed_size = 512           # dimensionality of image and word embeddings\n",
    "hidden_size = 512          # number of features in hidden state of the RNN decoder\n",
    "num_epochs = 3             # number of training epochs\n",
    "save_every = 1             # determines frequency of saving model weights\n",
    "print_every = 100          # determines window for printing average loss\n",
    "log_file = 'training_log.txt'       # name of file with saved training loss and perplexity\n",
    "\n",
    "# (Optional) TODO #2: Amend the image transform below.\n",
    "transform_train = transforms.Compose([ \n",
    "    transforms.Resize(256),                          # smaller edge of image resized to 256\n",
    "    transforms.RandomCrop(224),                      # get 224x224 crop from random location\n",
    "    transforms.RandomHorizontalFlip(),               # horizontally flip image with probability=0.5\n",
    "    transforms.ToTensor(),                           # convert the PIL Image to a tensor\n",
    "    transforms.Normalize((0.485, 0.456, 0.406),      # normalize image for pre-trained model\n",
    "                         (0.229, 0.224, 0.225))])\n",
    "\n",
    "# Build data loader.\n",
    "data_loader = get_loader(transform=transform_train,\n",
    "                         mode='train',\n",
    "                         batch_size=batch_size,\n",
    "                         vocab_threshold=vocab_threshold,\n",
    "                         vocab_from_file=vocab_from_file)\n",
    "\n",
    "# The size of the vocabulary.\n",
    "vocab_size = len(data_loader.dataset.vocab)\n",
    "\n",
    "# Initialize the encoder and decoder. \n",
    "encoder = EncoderCNN(embed_size)\n",
    "decoder = DecoderRNN(embed_size, hidden_size, vocab_size, max_batch_size=batch_size)\n",
    "\n",
    "# Move models to GPU if CUDA is available. \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "encoder.to(device)\n",
    "decoder.to(device)\n",
    "\n",
    "# Define the loss function. \n",
    "criterion = nn.CrossEntropyLoss().cuda() if torch.cuda.is_available() else nn.CrossEntropyLoss()\n",
    "\n",
    "# TODO #3: Specify the learnable parameters of the model.\n",
    "params = list(decoder.parameters()) + list(encoder.embed.parameters()) + list(encoder.bn.parameters())\n",
    "\n",
    "# TODO #4: Define the optimizer.\n",
    "#optimizer = torch.optim.SGD(params, lr=0.01)\n",
    "\n",
    "# this data is probably pretty sparse, and defaults are probably ok\n",
    "#http://ruder.io/optimizing-gradient-descent/\n",
    "optimizer = torch.optim.Adam(params, lr=0.001)\n",
    "\n",
    "# Set the total number of training steps per epoch.\n",
    "total_step = math.ceil(len(data_loader.dataset.caption_lengths) / data_loader.batch_sampler.batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step2'></a>\n",
    "## Step 2: Train your Model\n",
    "\n",
    "Once you have executed the code cell in **Step 1**, the training procedure below should run without issue.  \n",
    "\n",
    "It is completely fine to leave the code cell below as-is without modifications to train your model.  However, if you would like to modify the code used to train the model below, you must ensure that your changes are easily parsed by your reviewer.  In other words, make sure to provide appropriate comments to describe how your code works!  \n",
    "\n",
    "You may find it useful to load saved weights to resume training.  In that case, note the names of the files containing the encoder and decoder weights that you'd like to load (`encoder_file` and `decoder_file`).  Then you can load the weights by using the lines below:\n",
    "\n",
    "```python\n",
    "# Load pre-trained weights before resuming training.\n",
    "encoder.load_state_dict(torch.load(os.path.join('./models', encoder_file)))\n",
    "decoder.load_state_dict(torch.load(os.path.join('./models', decoder_file)))\n",
    "```\n",
    "\n",
    "While trying out parameters, make sure to take extensive notes and record the settings that you used in your various training runs.  In particular, you don't want to encounter a situation where you've trained a model for several hours but can't remember what settings you used :).\n",
    "\n",
    "### A Note on Tuning Hyperparameters\n",
    "\n",
    "To figure out how well your model is doing, you can look at how the training loss and perplexity evolve during training - and for the purposes of this project, you are encouraged to amend the hyperparameters based on this information.  \n",
    "\n",
    "However, this will not tell you if your model is overfitting to the training data, and, unfortunately, overfitting is a problem that is commonly encountered when training image captioning models.  \n",
    "\n",
    "For this project, you need not worry about overfitting. **This project does not have strict requirements regarding the performance of your model**, and you just need to demonstrate that your model has learned **_something_** when you generate captions on the test data.  For now, we strongly encourage you to train your model for the suggested 3 epochs without worrying about performance; then, you should immediately transition to the next notebook in the sequence (**3_Inference.ipynb**) to see how your model performs on the test data.  If your model needs to be changed, you can come back to this notebook, amend hyperparameters (if necessary), and re-train the model.\n",
    "\n",
    "That said, if you would like to go above and beyond in this project, you can read about some approaches to minimizing overfitting in section 4.3.1 of [this paper](http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7505636).  In the next (optional) step of this notebook, we provide some guidance for assessing the performance on the validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/3], Step [100/41412], Loss: 4.2696, Perplexity: 71.4945\n",
      "Epoch [1/3], Step [200/41412], Loss: 4.0440, Perplexity: 57.05449\n",
      "Epoch [1/3], Step [300/41412], Loss: 3.3515, Perplexity: 28.54571\n",
      "Epoch [1/3], Step [400/41412], Loss: 2.6613, Perplexity: 14.31442\n",
      "Epoch [1/3], Step [500/41412], Loss: 3.8998, Perplexity: 49.3939\n",
      "Epoch [1/3], Step [600/41412], Loss: 3.2152, Perplexity: 24.90838\n",
      "Epoch [1/3], Step [700/41412], Loss: 3.5679, Perplexity: 35.4411\n",
      "Epoch [1/3], Step [800/41412], Loss: 3.5968, Perplexity: 36.48171\n",
      "Epoch [1/3], Step [900/41412], Loss: 3.0525, Perplexity: 21.1674\n",
      "Epoch [1/3], Step [1000/41412], Loss: 2.9991, Perplexity: 20.0665\n",
      "Epoch [1/3], Step [1100/41412], Loss: 3.1469, Perplexity: 23.26359\n",
      "Epoch [1/3], Step [1200/41412], Loss: 3.8761, Perplexity: 48.2379\n",
      "Epoch [1/3], Step [1300/41412], Loss: 3.1357, Perplexity: 23.0048\n",
      "Epoch [1/3], Step [1400/41412], Loss: 3.5219, Perplexity: 33.84868\n",
      "Epoch [1/3], Step [1500/41412], Loss: 3.1983, Perplexity: 24.4911\n",
      "Epoch [1/3], Step [1600/41412], Loss: 3.2269, Perplexity: 25.2008\n",
      "Epoch [1/3], Step [1700/41412], Loss: 3.4569, Perplexity: 31.7171\n",
      "Epoch [1/3], Step [1800/41412], Loss: 2.9679, Perplexity: 19.4516\n",
      "Epoch [1/3], Step [1900/41412], Loss: 3.0904, Perplexity: 21.9859\n",
      "Epoch [1/3], Step [2000/41412], Loss: 2.8375, Perplexity: 17.0727\n",
      "Epoch [1/3], Step [2100/41412], Loss: 3.1163, Perplexity: 22.5633\n",
      "Epoch [1/3], Step [2200/41412], Loss: 3.0448, Perplexity: 21.0059\n",
      "Epoch [1/3], Step [2300/41412], Loss: 3.0110, Perplexity: 20.3080\n",
      "Epoch [1/3], Step [2400/41412], Loss: 2.6477, Perplexity: 14.1211\n",
      "Epoch [1/3], Step [2500/41412], Loss: 2.3444, Perplexity: 10.4267\n",
      "Epoch [1/3], Step [2600/41412], Loss: 2.8828, Perplexity: 17.86389\n",
      "Epoch [1/3], Step [2700/41412], Loss: 2.5264, Perplexity: 12.5089\n",
      "Epoch [1/3], Step [2800/41412], Loss: 2.3193, Perplexity: 10.1686\n",
      "Epoch [1/3], Step [2900/41412], Loss: 2.6425, Perplexity: 14.0488\n",
      "Epoch [1/3], Step [3000/41412], Loss: 2.9793, Perplexity: 19.67403\n",
      "Epoch [1/3], Step [3100/41412], Loss: 2.8986, Perplexity: 18.1487\n",
      "Epoch [1/3], Step [3200/41412], Loss: 2.5833, Perplexity: 13.2408\n",
      "Epoch [1/3], Step [3300/41412], Loss: 2.2372, Perplexity: 9.36729\n",
      "Epoch [1/3], Step [3400/41412], Loss: 2.7296, Perplexity: 15.32685\n",
      "Epoch [1/3], Step [3500/41412], Loss: 2.5161, Perplexity: 12.3797\n",
      "Epoch [1/3], Step [3600/41412], Loss: 2.6873, Perplexity: 14.6922\n",
      "Epoch [1/3], Step [3700/41412], Loss: 2.7424, Perplexity: 15.5242\n",
      "Epoch [1/3], Step [3800/41412], Loss: 2.3429, Perplexity: 10.41178\n",
      "Epoch [1/3], Step [3900/41412], Loss: 2.8428, Perplexity: 17.1643\n",
      "Epoch [1/3], Step [4000/41412], Loss: 2.6890, Perplexity: 14.7171\n",
      "Epoch [1/3], Step [4100/41412], Loss: 2.8561, Perplexity: 17.3936\n",
      "Epoch [1/3], Step [4200/41412], Loss: 2.6116, Perplexity: 13.6207\n",
      "Epoch [1/3], Step [4300/41412], Loss: 2.4792, Perplexity: 11.9316\n",
      "Epoch [1/3], Step [4400/41412], Loss: 2.0249, Perplexity: 7.57578\n",
      "Epoch [1/3], Step [4500/41412], Loss: 2.5470, Perplexity: 12.7684\n",
      "Epoch [1/3], Step [4600/41412], Loss: 3.2423, Perplexity: 25.59241\n",
      "Epoch [1/3], Step [4700/41412], Loss: 2.9099, Perplexity: 18.3551\n",
      "Epoch [1/3], Step [4800/41412], Loss: 3.4328, Perplexity: 30.9645\n",
      "Epoch [1/3], Step [4900/41412], Loss: 2.4569, Perplexity: 11.6684\n",
      "Epoch [1/3], Step [5000/41412], Loss: 2.9460, Perplexity: 19.0294\n",
      "Epoch [1/3], Step [5100/41412], Loss: 2.2989, Perplexity: 9.96320\n",
      "Epoch [1/3], Step [5200/41412], Loss: 2.0729, Perplexity: 7.94757\n",
      "Epoch [1/3], Step [5300/41412], Loss: 3.2308, Perplexity: 25.3007\n",
      "Epoch [1/3], Step [5400/41412], Loss: 2.6672, Perplexity: 14.3994\n",
      "Epoch [1/3], Step [5500/41412], Loss: 2.3396, Perplexity: 10.3774\n",
      "Epoch [1/3], Step [5600/41412], Loss: 3.0540, Perplexity: 21.2000\n",
      "Epoch [1/3], Step [5700/41412], Loss: 2.8726, Perplexity: 17.6827\n",
      "Epoch [1/3], Step [5800/41412], Loss: 3.0001, Perplexity: 20.0869\n",
      "Epoch [1/3], Step [5900/41412], Loss: 2.4193, Perplexity: 11.2383\n",
      "Epoch [1/3], Step [6000/41412], Loss: 2.7176, Perplexity: 15.1433\n",
      "Epoch [1/3], Step [6100/41412], Loss: 2.7440, Perplexity: 15.5498\n",
      "Epoch [1/3], Step [6200/41412], Loss: 2.5503, Perplexity: 12.8114\n",
      "Epoch [1/3], Step [6300/41412], Loss: 2.7139, Perplexity: 15.0879\n",
      "Epoch [1/3], Step [6400/41412], Loss: 2.0239, Perplexity: 7.56750\n",
      "Epoch [1/3], Step [6500/41412], Loss: 2.2238, Perplexity: 9.24230\n",
      "Epoch [1/3], Step [6600/41412], Loss: 2.4028, Perplexity: 11.0537\n",
      "Epoch [1/3], Step [6700/41412], Loss: 2.7124, Perplexity: 15.0661\n",
      "Epoch [1/3], Step [6800/41412], Loss: 2.2581, Perplexity: 9.56466\n",
      "Epoch [1/3], Step [6900/41412], Loss: 2.4541, Perplexity: 11.6357\n",
      "Epoch [1/3], Step [7000/41412], Loss: 2.4671, Perplexity: 11.7884\n",
      "Epoch [1/3], Step [7100/41412], Loss: 2.5715, Perplexity: 13.08528\n",
      "Epoch [1/3], Step [7200/41412], Loss: 2.8069, Perplexity: 16.5587\n",
      "Epoch [1/3], Step [7300/41412], Loss: 2.3903, Perplexity: 10.91716\n",
      "Epoch [1/3], Step [7400/41412], Loss: 2.8033, Perplexity: 16.4987\n",
      "Epoch [1/3], Step [7500/41412], Loss: 2.7349, Perplexity: 15.4079\n",
      "Epoch [1/3], Step [7600/41412], Loss: 2.6388, Perplexity: 13.9965\n",
      "Epoch [1/3], Step [7700/41412], Loss: 3.2446, Perplexity: 25.6509\n",
      "Epoch [1/3], Step [7800/41412], Loss: 2.0501, Perplexity: 7.76905\n",
      "Epoch [1/3], Step [7900/41412], Loss: 2.5953, Perplexity: 13.4003\n",
      "Epoch [1/3], Step [8000/41412], Loss: 2.8575, Perplexity: 17.4175\n",
      "Epoch [1/3], Step [8100/41412], Loss: 2.1988, Perplexity: 9.01440\n",
      "Epoch [1/3], Step [8200/41412], Loss: 2.6764, Perplexity: 14.5327\n",
      "Epoch [1/3], Step [8300/41412], Loss: 2.7859, Perplexity: 16.2142\n",
      "Epoch [1/3], Step [8400/41412], Loss: 2.3990, Perplexity: 11.0122\n",
      "Epoch [1/3], Step [8500/41412], Loss: 2.6027, Perplexity: 13.5003\n",
      "Epoch [1/3], Step [8600/41412], Loss: 2.5900, Perplexity: 13.3303\n",
      "Epoch [1/3], Step [8700/41412], Loss: 2.2750, Perplexity: 9.72802\n",
      "Epoch [1/3], Step [8800/41412], Loss: 2.5209, Perplexity: 12.4400\n",
      "Epoch [1/3], Step [8900/41412], Loss: 2.5234, Perplexity: 12.4709\n",
      "Epoch [1/3], Step [9000/41412], Loss: 2.5599, Perplexity: 12.93403\n",
      "Epoch [1/3], Step [9100/41412], Loss: 2.3310, Perplexity: 10.2883\n",
      "Epoch [1/3], Step [9200/41412], Loss: 2.9066, Perplexity: 18.2950\n",
      "Epoch [1/3], Step [9300/41412], Loss: 2.4845, Perplexity: 11.9949\n",
      "Epoch [1/3], Step [9400/41412], Loss: 3.0881, Perplexity: 21.9344\n",
      "Epoch [1/3], Step [9500/41412], Loss: 2.0769, Perplexity: 7.98013\n",
      "Epoch [1/3], Step [9600/41412], Loss: 1.8482, Perplexity: 6.34874\n",
      "Epoch [1/3], Step [9700/41412], Loss: 2.7202, Perplexity: 15.1837\n",
      "Epoch [1/3], Step [9800/41412], Loss: 3.0420, Perplexity: 20.9475\n",
      "Epoch [1/3], Step [9900/41412], Loss: 2.1236, Perplexity: 8.36091\n",
      "Epoch [1/3], Step [10000/41412], Loss: 2.2803, Perplexity: 9.7796\n",
      "Epoch [1/3], Step [10100/41412], Loss: 3.1402, Perplexity: 23.1084\n",
      "Epoch [1/3], Step [10200/41412], Loss: 2.3897, Perplexity: 10.9103\n",
      "Epoch [1/3], Step [10300/41412], Loss: 2.1270, Perplexity: 8.39006\n",
      "Epoch [1/3], Step [10400/41412], Loss: 2.5068, Perplexity: 12.2651\n",
      "Epoch [1/3], Step [10500/41412], Loss: 2.7944, Perplexity: 16.3523\n",
      "Epoch [1/3], Step [10600/41412], Loss: 2.2779, Perplexity: 9.75664\n",
      "Epoch [1/3], Step [10700/41412], Loss: 2.1814, Perplexity: 8.85896\n",
      "Epoch [1/3], Step [10800/41412], Loss: 2.3634, Perplexity: 10.6268\n",
      "Epoch [1/3], Step [10900/41412], Loss: 2.7392, Perplexity: 15.4750\n",
      "Epoch [1/3], Step [11000/41412], Loss: 2.2042, Perplexity: 9.06299\n",
      "Epoch [1/3], Step [11100/41412], Loss: 2.5523, Perplexity: 12.8361\n",
      "Epoch [1/3], Step [11200/41412], Loss: 2.6314, Perplexity: 13.8939\n",
      "Epoch [1/3], Step [11300/41412], Loss: 1.9303, Perplexity: 6.89142\n",
      "Epoch [1/3], Step [11400/41412], Loss: 2.2987, Perplexity: 9.961553\n",
      "Epoch [1/3], Step [11500/41412], Loss: 3.0451, Perplexity: 21.0128\n",
      "Epoch [1/3], Step [11600/41412], Loss: 2.6023, Perplexity: 13.4952\n",
      "Epoch [1/3], Step [11700/41412], Loss: 2.5754, Perplexity: 13.1371\n",
      "Epoch [1/3], Step [11800/41412], Loss: 2.1857, Perplexity: 8.89723\n",
      "Epoch [1/3], Step [11900/41412], Loss: 2.2282, Perplexity: 9.28279\n",
      "Epoch [1/3], Step [12000/41412], Loss: 2.6208, Perplexity: 13.74646\n",
      "Epoch [1/3], Step [12100/41412], Loss: 2.7132, Perplexity: 15.0781\n",
      "Epoch [1/3], Step [12200/41412], Loss: 2.4261, Perplexity: 11.3143\n",
      "Epoch [1/3], Step [12300/41412], Loss: 2.7186, Perplexity: 15.1591\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/3], Step [12400/41412], Loss: 2.3371, Perplexity: 10.3516\n",
      "Epoch [1/3], Step [12500/41412], Loss: 2.5113, Perplexity: 12.3205\n",
      "Epoch [1/3], Step [12600/41412], Loss: 2.6599, Perplexity: 14.2945\n",
      "Epoch [1/3], Step [12700/41412], Loss: 1.9965, Perplexity: 7.36350\n",
      "Epoch [1/3], Step [12800/41412], Loss: 2.5177, Perplexity: 12.4006\n",
      "Epoch [1/3], Step [12900/41412], Loss: 2.4616, Perplexity: 11.7234\n",
      "Epoch [1/3], Step [13000/41412], Loss: 2.1033, Perplexity: 8.19294\n",
      "Epoch [1/3], Step [13100/41412], Loss: 2.8980, Perplexity: 18.1384\n",
      "Epoch [1/3], Step [13200/41412], Loss: 2.2467, Perplexity: 9.45692\n",
      "Epoch [1/3], Step [13300/41412], Loss: 2.1315, Perplexity: 8.42710\n",
      "Epoch [1/3], Step [13400/41412], Loss: 1.9843, Perplexity: 7.27402\n",
      "Epoch [1/3], Step [13500/41412], Loss: 2.3446, Perplexity: 10.4288\n",
      "Epoch [1/3], Step [13600/41412], Loss: 2.2258, Perplexity: 9.26064\n",
      "Epoch [1/3], Step [13700/41412], Loss: 2.4618, Perplexity: 11.7257\n",
      "Epoch [1/3], Step [13800/41412], Loss: 2.5169, Perplexity: 12.3901\n",
      "Epoch [1/3], Step [13900/41412], Loss: 1.9267, Perplexity: 6.86678\n",
      "Epoch [1/3], Step [14000/41412], Loss: 2.1015, Perplexity: 8.17867\n",
      "Epoch [1/3], Step [14100/41412], Loss: 3.6971, Perplexity: 40.3291\n",
      "Epoch [1/3], Step [14200/41412], Loss: 2.5220, Perplexity: 12.4537\n",
      "Epoch [1/3], Step [14300/41412], Loss: 2.2794, Perplexity: 9.77043\n",
      "Epoch [1/3], Step [14400/41412], Loss: 2.3055, Perplexity: 10.0289\n",
      "Epoch [1/3], Step [14500/41412], Loss: 2.1474, Perplexity: 8.56294\n",
      "Epoch [1/3], Step [14600/41412], Loss: 2.6356, Perplexity: 13.9517\n",
      "Epoch [1/3], Step [14700/41412], Loss: 2.3763, Perplexity: 10.7655\n",
      "Epoch [1/3], Step [14800/41412], Loss: 2.1577, Perplexity: 8.65116\n",
      "Epoch [1/3], Step [14900/41412], Loss: 2.2367, Perplexity: 9.36274\n",
      "Epoch [1/3], Step [15000/41412], Loss: 2.6094, Perplexity: 13.5911\n",
      "Epoch [1/3], Step [15100/41412], Loss: 2.2151, Perplexity: 9.16247\n",
      "Epoch [1/3], Step [15200/41412], Loss: 2.4701, Perplexity: 11.8239\n",
      "Epoch [1/3], Step [15300/41412], Loss: 2.9725, Perplexity: 19.5400\n",
      "Epoch [1/3], Step [15400/41412], Loss: 2.0172, Perplexity: 7.517297\n",
      "Epoch [1/3], Step [15500/41412], Loss: 2.7920, Perplexity: 16.3144\n",
      "Epoch [1/3], Step [15600/41412], Loss: 1.9067, Perplexity: 6.73114\n",
      "Epoch [1/3], Step [15700/41412], Loss: 2.2513, Perplexity: 9.49972\n",
      "Epoch [1/3], Step [15800/41412], Loss: 2.4219, Perplexity: 11.2675\n",
      "Epoch [1/3], Step [15900/41412], Loss: 2.5816, Perplexity: 13.2188\n",
      "Epoch [1/3], Step [16000/41412], Loss: 2.1221, Perplexity: 8.34906\n",
      "Epoch [1/3], Step [16100/41412], Loss: 2.5020, Perplexity: 12.2063\n",
      "Epoch [1/3], Step [16200/41412], Loss: 2.5763, Perplexity: 13.1490\n",
      "Epoch [1/3], Step [16300/41412], Loss: 2.0314, Perplexity: 7.624561\n",
      "Epoch [1/3], Step [16400/41412], Loss: 2.3665, Perplexity: 10.6602\n",
      "Epoch [1/3], Step [16500/41412], Loss: 2.0786, Perplexity: 7.99340\n",
      "Epoch [1/3], Step [16600/41412], Loss: 2.6368, Perplexity: 13.9687\n",
      "Epoch [1/3], Step [16700/41412], Loss: 2.9753, Perplexity: 19.5962\n",
      "Epoch [1/3], Step [16800/41412], Loss: 1.6320, Perplexity: 5.11384\n",
      "Epoch [1/3], Step [16900/41412], Loss: 2.4737, Perplexity: 11.8665\n",
      "Epoch [1/3], Step [17000/41412], Loss: 2.4688, Perplexity: 11.8082\n",
      "Epoch [1/3], Step [17100/41412], Loss: 2.8108, Perplexity: 16.6224\n",
      "Epoch [1/3], Step [17200/41412], Loss: 2.0051, Perplexity: 7.42653\n",
      "Epoch [1/3], Step [17300/41412], Loss: 2.2952, Perplexity: 9.92687\n",
      "Epoch [1/3], Step [17400/41412], Loss: 2.3580, Perplexity: 10.5697\n",
      "Epoch [1/3], Step [17500/41412], Loss: 2.4543, Perplexity: 11.6377\n",
      "Epoch [1/3], Step [17600/41412], Loss: 2.1693, Perplexity: 8.75225\n",
      "Epoch [1/3], Step [17700/41412], Loss: 3.3650, Perplexity: 28.9337\n",
      "Epoch [1/3], Step [17800/41412], Loss: 2.0943, Perplexity: 8.11993\n",
      "Epoch [1/3], Step [17900/41412], Loss: 2.4893, Perplexity: 12.0527\n",
      "Epoch [1/3], Step [18000/41412], Loss: 2.1663, Perplexity: 8.72563\n",
      "Epoch [1/3], Step [18100/41412], Loss: 3.1109, Perplexity: 22.4411\n",
      "Epoch [1/3], Step [18200/41412], Loss: 2.4358, Perplexity: 11.4248\n",
      "Epoch [1/3], Step [18300/41412], Loss: 2.3228, Perplexity: 10.2045\n",
      "Epoch [1/3], Step [18400/41412], Loss: 1.7781, Perplexity: 5.91874\n",
      "Epoch [1/3], Step [18500/41412], Loss: 2.7466, Perplexity: 15.5891\n",
      "Epoch [1/3], Step [18600/41412], Loss: 2.9243, Perplexity: 18.6212\n",
      "Epoch [1/3], Step [18700/41412], Loss: 2.5195, Perplexity: 12.4223\n",
      "Epoch [1/3], Step [18800/41412], Loss: 2.1109, Perplexity: 8.25560\n",
      "Epoch [1/3], Step [18900/41412], Loss: 1.9850, Perplexity: 7.27909\n",
      "Epoch [1/3], Step [19000/41412], Loss: 2.2007, Perplexity: 9.03160\n",
      "Epoch [1/3], Step [19100/41412], Loss: 2.1864, Perplexity: 8.90284\n",
      "Epoch [1/3], Step [19200/41412], Loss: 2.2296, Perplexity: 9.29599\n",
      "Epoch [1/3], Step [19300/41412], Loss: 2.5579, Perplexity: 12.9093\n",
      "Epoch [1/3], Step [19400/41412], Loss: 2.6200, Perplexity: 13.7355\n",
      "Epoch [1/3], Step [19500/41412], Loss: 2.1835, Perplexity: 8.87788\n",
      "Epoch [1/3], Step [19600/41412], Loss: 3.3731, Perplexity: 29.1680\n",
      "Epoch [1/3], Step [19700/41412], Loss: 2.5073, Perplexity: 12.2723\n",
      "Epoch [1/3], Step [19800/41412], Loss: 2.2825, Perplexity: 9.80107\n",
      "Epoch [1/3], Step [19900/41412], Loss: 2.7174, Perplexity: 15.1402\n",
      "Epoch [1/3], Step [20000/41412], Loss: 2.1136, Perplexity: 8.27815\n",
      "Epoch [1/3], Step [20100/41412], Loss: 3.0021, Perplexity: 20.1278\n",
      "Epoch [1/3], Step [20200/41412], Loss: 2.2087, Perplexity: 9.10388\n",
      "Epoch [1/3], Step [20300/41412], Loss: 2.0194, Perplexity: 7.53418\n",
      "Epoch [1/3], Step [20400/41412], Loss: 2.3923, Perplexity: 10.9387\n",
      "Epoch [1/3], Step [20500/41412], Loss: 2.0816, Perplexity: 8.01709\n",
      "Epoch [1/3], Step [20600/41412], Loss: 2.0099, Perplexity: 7.46237\n",
      "Epoch [1/3], Step [20700/41412], Loss: 2.0551, Perplexity: 7.80776\n",
      "Epoch [1/3], Step [20800/41412], Loss: 2.0772, Perplexity: 7.98181\n",
      "Epoch [1/3], Step [20900/41412], Loss: 2.1760, Perplexity: 8.81113\n",
      "Epoch [1/3], Step [21000/41412], Loss: 1.8077, Perplexity: 6.09647\n",
      "Epoch [1/3], Step [21100/41412], Loss: 1.9123, Perplexity: 6.76891\n",
      "Epoch [1/3], Step [21200/41412], Loss: 1.8549, Perplexity: 6.39136\n",
      "Epoch [1/3], Step [21300/41412], Loss: 2.5168, Perplexity: 12.3885\n",
      "Epoch [1/3], Step [21400/41412], Loss: 2.0715, Perplexity: 7.93701\n",
      "Epoch [1/3], Step [21500/41412], Loss: 1.9020, Perplexity: 6.69945\n",
      "Epoch [1/3], Step [21600/41412], Loss: 2.3841, Perplexity: 10.8492\n",
      "Epoch [1/3], Step [21700/41412], Loss: 1.9400, Perplexity: 6.95870\n",
      "Epoch [1/3], Step [21800/41412], Loss: 2.0733, Perplexity: 7.95081\n",
      "Epoch [1/3], Step [21900/41412], Loss: 2.3163, Perplexity: 10.1379\n",
      "Epoch [1/3], Step [22000/41412], Loss: 3.1942, Perplexity: 24.3919\n",
      "Epoch [1/3], Step [22100/41412], Loss: 2.0433, Perplexity: 7.71648\n",
      "Epoch [1/3], Step [22200/41412], Loss: 2.2911, Perplexity: 9.88573\n",
      "Epoch [1/3], Step [22300/41412], Loss: 1.8598, Perplexity: 6.42245\n",
      "Epoch [1/3], Step [22400/41412], Loss: 1.9178, Perplexity: 6.80634\n",
      "Epoch [1/3], Step [22500/41412], Loss: 2.5890, Perplexity: 13.3161\n",
      "Epoch [1/3], Step [22600/41412], Loss: 2.6925, Perplexity: 14.7687\n",
      "Epoch [1/3], Step [22700/41412], Loss: 2.0016, Perplexity: 7.40072\n",
      "Epoch [1/3], Step [22800/41412], Loss: 2.3047, Perplexity: 10.0211\n",
      "Epoch [1/3], Step [22900/41412], Loss: 1.7825, Perplexity: 5.94469\n",
      "Epoch [1/3], Step [23000/41412], Loss: 2.0585, Perplexity: 7.83427\n",
      "Epoch [1/3], Step [23100/41412], Loss: 2.1760, Perplexity: 8.81144\n",
      "Epoch [1/3], Step [23200/41412], Loss: 2.2186, Perplexity: 9.19440\n",
      "Epoch [1/3], Step [23300/41412], Loss: 2.4670, Perplexity: 11.7867\n",
      "Epoch [1/3], Step [23400/41412], Loss: 1.9564, Perplexity: 7.07421\n",
      "Epoch [1/3], Step [23500/41412], Loss: 2.7852, Perplexity: 16.2038\n",
      "Epoch [1/3], Step [23600/41412], Loss: 2.7023, Perplexity: 14.9133\n",
      "Epoch [1/3], Step [23700/41412], Loss: 2.3360, Perplexity: 10.3398\n",
      "Epoch [1/3], Step [23800/41412], Loss: 2.5214, Perplexity: 12.4458\n",
      "Epoch [1/3], Step [23900/41412], Loss: 2.4250, Perplexity: 11.3025\n",
      "Epoch [1/3], Step [24000/41412], Loss: 2.1359, Perplexity: 8.46505\n",
      "Epoch [1/3], Step [24100/41412], Loss: 2.4052, Perplexity: 11.0811\n",
      "Epoch [1/3], Step [24200/41412], Loss: 2.5657, Perplexity: 13.0101\n",
      "Epoch [1/3], Step [24300/41412], Loss: 1.8946, Perplexity: 6.65008\n",
      "Epoch [1/3], Step [24400/41412], Loss: 2.4308, Perplexity: 11.3675\n",
      "Epoch [1/3], Step [24500/41412], Loss: 2.2764, Perplexity: 9.74207\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/3], Step [24600/41412], Loss: 2.5629, Perplexity: 12.9736\n",
      "Epoch [1/3], Step [24700/41412], Loss: 2.7914, Perplexity: 16.3032\n",
      "Epoch [1/3], Step [24800/41412], Loss: 1.7684, Perplexity: 5.86133\n",
      "Epoch [1/3], Step [24900/41412], Loss: 2.1037, Perplexity: 8.19633\n",
      "Epoch [1/3], Step [25000/41412], Loss: 2.2479, Perplexity: 9.46774\n",
      "Epoch [1/3], Step [25100/41412], Loss: 2.7326, Perplexity: 15.3723\n",
      "Epoch [1/3], Step [25200/41412], Loss: 2.3651, Perplexity: 10.6450\n",
      "Epoch [1/3], Step [25300/41412], Loss: 2.3281, Perplexity: 10.2589\n",
      "Epoch [1/3], Step [25400/41412], Loss: 2.4536, Perplexity: 11.6301\n",
      "Epoch [1/3], Step [25500/41412], Loss: 2.4304, Perplexity: 11.3638\n",
      "Epoch [1/3], Step [25600/41412], Loss: 2.5535, Perplexity: 12.8518\n",
      "Epoch [1/3], Step [25700/41412], Loss: 1.6592, Perplexity: 5.25535\n",
      "Epoch [1/3], Step [25800/41412], Loss: 2.4214, Perplexity: 11.2614\n",
      "Epoch [1/3], Step [25900/41412], Loss: 2.0638, Perplexity: 7.87605\n",
      "Epoch [1/3], Step [26000/41412], Loss: 1.8483, Perplexity: 6.34897\n",
      "Epoch [1/3], Step [26100/41412], Loss: 2.2829, Perplexity: 9.80505\n",
      "Epoch [1/3], Step [26200/41412], Loss: 2.0076, Perplexity: 7.445756\n",
      "Epoch [1/3], Step [26300/41412], Loss: 1.9478, Perplexity: 7.01311\n",
      "Epoch [1/3], Step [26400/41412], Loss: 2.2187, Perplexity: 9.19572\n",
      "Epoch [1/3], Step [26500/41412], Loss: 2.9165, Perplexity: 18.4773\n",
      "Epoch [1/3], Step [26600/41412], Loss: 2.0651, Perplexity: 7.88593\n",
      "Epoch [1/3], Step [26700/41412], Loss: 2.6738, Perplexity: 14.4948\n",
      "Epoch [1/3], Step [26800/41412], Loss: 2.5691, Perplexity: 13.0546\n",
      "Epoch [1/3], Step [26900/41412], Loss: 2.1787, Perplexity: 8.83476\n",
      "Epoch [1/3], Step [27000/41412], Loss: 2.4407, Perplexity: 11.4812\n",
      "Epoch [1/3], Step [27100/41412], Loss: 2.6524, Perplexity: 14.1887\n",
      "Epoch [1/3], Step [27200/41412], Loss: 2.5662, Perplexity: 13.0165\n",
      "Epoch [1/3], Step [27300/41412], Loss: 2.4381, Perplexity: 11.4509\n",
      "Epoch [1/3], Step [27400/41412], Loss: 2.2954, Perplexity: 9.92897\n",
      "Epoch [1/3], Step [27500/41412], Loss: 1.8848, Perplexity: 6.58538\n",
      "Epoch [1/3], Step [27600/41412], Loss: 1.9112, Perplexity: 6.76109\n",
      "Epoch [1/3], Step [27700/41412], Loss: 1.9724, Perplexity: 7.18806\n",
      "Epoch [1/3], Step [27800/41412], Loss: 3.1020, Perplexity: 22.2415\n",
      "Epoch [1/3], Step [27900/41412], Loss: 2.7387, Perplexity: 15.4674\n",
      "Epoch [1/3], Step [28000/41412], Loss: 2.1711, Perplexity: 8.76774\n",
      "Epoch [1/3], Step [28100/41412], Loss: 1.9238, Perplexity: 6.84727\n",
      "Epoch [1/3], Step [28200/41412], Loss: 1.9224, Perplexity: 6.83766\n",
      "Epoch [1/3], Step [28300/41412], Loss: 2.7140, Perplexity: 15.0889\n",
      "Epoch [1/3], Step [28400/41412], Loss: 2.3816, Perplexity: 10.8221\n",
      "Epoch [1/3], Step [28500/41412], Loss: 1.8439, Perplexity: 6.32106\n",
      "Epoch [1/3], Step [28600/41412], Loss: 2.7349, Perplexity: 15.4084\n",
      "Epoch [1/3], Step [28700/41412], Loss: 1.9013, Perplexity: 6.69486\n",
      "Epoch [1/3], Step [28800/41412], Loss: 2.3935, Perplexity: 10.9520\n",
      "Epoch [1/3], Step [28900/41412], Loss: 2.3595, Perplexity: 10.5857\n",
      "Epoch [1/3], Step [29000/41412], Loss: 2.5747, Perplexity: 13.1279\n",
      "Epoch [1/3], Step [29100/41412], Loss: 2.1554, Perplexity: 8.63175\n",
      "Epoch [1/3], Step [29200/41412], Loss: 1.8491, Perplexity: 6.35437\n",
      "Epoch [1/3], Step [29300/41412], Loss: 2.5772, Perplexity: 13.1606\n",
      "Epoch [1/3], Step [29400/41412], Loss: 2.2070, Perplexity: 9.088759\n",
      "Epoch [1/3], Step [29500/41412], Loss: 2.0139, Perplexity: 7.49278\n",
      "Epoch [1/3], Step [29600/41412], Loss: 2.1341, Perplexity: 8.44949\n",
      "Epoch [1/3], Step [29700/41412], Loss: 2.3482, Perplexity: 10.4671\n",
      "Epoch [1/3], Step [29800/41412], Loss: 1.9533, Perplexity: 7.05173\n",
      "Epoch [1/3], Step [29900/41412], Loss: 2.0379, Perplexity: 7.67484\n",
      "Epoch [1/3], Step [30000/41412], Loss: 2.1985, Perplexity: 9.01111\n",
      "Epoch [1/3], Step [30100/41412], Loss: 2.6284, Perplexity: 13.8517\n",
      "Epoch [1/3], Step [30200/41412], Loss: 2.2118, Perplexity: 9.13239\n",
      "Epoch [1/3], Step [30300/41412], Loss: 2.3727, Perplexity: 10.7264\n",
      "Epoch [1/3], Step [30400/41412], Loss: 2.0476, Perplexity: 7.74946\n",
      "Epoch [1/3], Step [30500/41412], Loss: 2.1812, Perplexity: 8.85729\n",
      "Epoch [1/3], Step [30600/41412], Loss: 2.3576, Perplexity: 10.5660\n",
      "Epoch [1/3], Step [30700/41412], Loss: 2.3261, Perplexity: 10.2379\n",
      "Epoch [1/3], Step [30800/41412], Loss: 2.1981, Perplexity: 9.00794\n",
      "Epoch [1/3], Step [30900/41412], Loss: 2.3907, Perplexity: 10.9217\n",
      "Epoch [1/3], Step [31000/41412], Loss: 2.1633, Perplexity: 8.70004\n",
      "Epoch [1/3], Step [31100/41412], Loss: 2.5738, Perplexity: 13.1159\n",
      "Epoch [1/3], Step [31200/41412], Loss: 2.0836, Perplexity: 8.03351\n",
      "Epoch [1/3], Step [31300/41412], Loss: 1.6697, Perplexity: 5.31051\n",
      "Epoch [1/3], Step [31400/41412], Loss: 3.4915, Perplexity: 32.8356\n",
      "Epoch [1/3], Step [31500/41412], Loss: 1.8330, Perplexity: 6.25249\n",
      "Epoch [1/3], Step [31600/41412], Loss: 2.0162, Perplexity: 7.50943\n",
      "Epoch [1/3], Step [31700/41412], Loss: 2.4079, Perplexity: 11.1101\n",
      "Epoch [1/3], Step [31800/41412], Loss: 2.8003, Perplexity: 16.4504\n",
      "Epoch [1/3], Step [31900/41412], Loss: 2.0172, Perplexity: 7.51769\n",
      "Epoch [1/3], Step [32000/41412], Loss: 1.8505, Perplexity: 6.36325\n",
      "Epoch [1/3], Step [32100/41412], Loss: 2.1526, Perplexity: 8.60746\n",
      "Epoch [1/3], Step [32200/41412], Loss: 1.7998, Perplexity: 6.04841\n",
      "Epoch [1/3], Step [32300/41412], Loss: 2.2935, Perplexity: 9.90925\n",
      "Epoch [1/3], Step [32400/41412], Loss: 2.5187, Perplexity: 12.4127\n",
      "Epoch [1/3], Step [32500/41412], Loss: 2.1201, Perplexity: 8.33217\n",
      "Epoch [1/3], Step [32600/41412], Loss: 3.0958, Perplexity: 22.1039\n",
      "Epoch [1/3], Step [32700/41412], Loss: 2.1131, Perplexity: 8.27419\n",
      "Epoch [1/3], Step [32800/41412], Loss: 2.2183, Perplexity: 9.19157\n",
      "Epoch [1/3], Step [32900/41412], Loss: 1.6946, Perplexity: 5.44460\n",
      "Epoch [1/3], Step [33000/41412], Loss: 2.4823, Perplexity: 11.9683\n",
      "Epoch [1/3], Step [33100/41412], Loss: 2.5635, Perplexity: 12.9809\n",
      "Epoch [1/3], Step [33200/41412], Loss: 1.9548, Perplexity: 7.06234\n",
      "Epoch [1/3], Step [33300/41412], Loss: 2.3474, Perplexity: 10.4585\n",
      "Epoch [1/3], Step [33400/41412], Loss: 2.4690, Perplexity: 11.8103\n",
      "Epoch [1/3], Step [33500/41412], Loss: 2.2572, Perplexity: 9.55625\n",
      "Epoch [1/3], Step [33600/41412], Loss: 2.8845, Perplexity: 17.8944\n",
      "Epoch [1/3], Step [33700/41412], Loss: 2.1446, Perplexity: 8.53887\n",
      "Epoch [1/3], Step [33800/41412], Loss: 2.4126, Perplexity: 11.1628\n",
      "Epoch [1/3], Step [33900/41412], Loss: 1.9948, Perplexity: 7.35106\n",
      "Epoch [1/3], Step [34000/41412], Loss: 2.6652, Perplexity: 14.3708\n",
      "Epoch [1/3], Step [34100/41412], Loss: 2.1685, Perplexity: 8.74487\n",
      "Epoch [1/3], Step [34200/41412], Loss: 2.6531, Perplexity: 14.19789\n",
      "Epoch [1/3], Step [34300/41412], Loss: 2.1870, Perplexity: 8.90833\n",
      "Epoch [1/3], Step [34400/41412], Loss: 2.4796, Perplexity: 11.9368\n",
      "Epoch [1/3], Step [34500/41412], Loss: 1.4934, Perplexity: 4.45234\n",
      "Epoch [1/3], Step [34600/41412], Loss: 1.9173, Perplexity: 6.80276\n",
      "Epoch [1/3], Step [34700/41412], Loss: 2.2015, Perplexity: 9.03841\n",
      "Epoch [1/3], Step [34800/41412], Loss: 2.2329, Perplexity: 9.32657\n",
      "Epoch [1/3], Step [34900/41412], Loss: 1.9031, Perplexity: 6.70698\n",
      "Epoch [1/3], Step [35000/41412], Loss: 2.2487, Perplexity: 9.47586\n",
      "Epoch [1/3], Step [35100/41412], Loss: 2.5688, Perplexity: 13.0496\n",
      "Epoch [1/3], Step [35200/41412], Loss: 1.7717, Perplexity: 5.88106\n",
      "Epoch [1/3], Step [35300/41412], Loss: 2.5363, Perplexity: 12.6330\n",
      "Epoch [1/3], Step [35400/41412], Loss: 2.0621, Perplexity: 7.86253\n",
      "Epoch [1/3], Step [35500/41412], Loss: 1.9579, Perplexity: 7.08416\n",
      "Epoch [1/3], Step [35600/41412], Loss: 2.0735, Perplexity: 7.95260\n",
      "Epoch [1/3], Step [35700/41412], Loss: 2.0465, Perplexity: 7.74063\n",
      "Epoch [1/3], Step [35800/41412], Loss: 2.3901, Perplexity: 10.9146\n",
      "Epoch [1/3], Step [35900/41412], Loss: 2.0723, Perplexity: 7.94284\n",
      "Epoch [1/3], Step [36000/41412], Loss: 2.2940, Perplexity: 9.91507\n",
      "Epoch [1/3], Step [36100/41412], Loss: 2.4872, Perplexity: 12.0274\n",
      "Epoch [1/3], Step [36200/41412], Loss: 3.1386, Perplexity: 23.0709\n",
      "Epoch [1/3], Step [36300/41412], Loss: 2.2340, Perplexity: 9.33763\n",
      "Epoch [1/3], Step [36400/41412], Loss: 1.9107, Perplexity: 6.75796\n",
      "Epoch [1/3], Step [36500/41412], Loss: 1.8513, Perplexity: 6.36787\n",
      "Epoch [1/3], Step [36600/41412], Loss: 1.9098, Perplexity: 6.75191\n",
      "Epoch [1/3], Step [36700/41412], Loss: 1.8704, Perplexity: 6.49076\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/3], Step [36800/41412], Loss: 1.9753, Perplexity: 7.20906\n",
      "Epoch [1/3], Step [36900/41412], Loss: 2.7967, Perplexity: 16.3899\n",
      "Epoch [1/3], Step [37000/41412], Loss: 2.4969, Perplexity: 12.1443\n",
      "Epoch [1/3], Step [37100/41412], Loss: 2.7478, Perplexity: 15.6086\n",
      "Epoch [1/3], Step [37200/41412], Loss: 2.7636, Perplexity: 15.8574\n",
      "Epoch [1/3], Step [37300/41412], Loss: 1.8378, Perplexity: 6.28276\n",
      "Epoch [1/3], Step [37400/41412], Loss: 2.3406, Perplexity: 10.3876\n",
      "Epoch [1/3], Step [37500/41412], Loss: 2.3271, Perplexity: 10.2485\n",
      "Epoch [1/3], Step [37600/41412], Loss: 2.3625, Perplexity: 10.6178\n",
      "Epoch [1/3], Step [37700/41412], Loss: 2.1395, Perplexity: 8.49484\n",
      "Epoch [1/3], Step [37800/41412], Loss: 2.1816, Perplexity: 8.86036\n",
      "Epoch [1/3], Step [37900/41412], Loss: 1.7540, Perplexity: 5.77792\n",
      "Epoch [1/3], Step [38000/41412], Loss: 2.5743, Perplexity: 13.1222\n",
      "Epoch [1/3], Step [38100/41412], Loss: 2.2110, Perplexity: 9.12503\n",
      "Epoch [1/3], Step [38200/41412], Loss: 2.5969, Perplexity: 13.4215\n",
      "Epoch [1/3], Step [38300/41412], Loss: 2.3834, Perplexity: 10.8417\n",
      "Epoch [1/3], Step [38400/41412], Loss: 2.2968, Perplexity: 9.94272\n",
      "Epoch [1/3], Step [38500/41412], Loss: 2.5716, Perplexity: 13.0867\n",
      "Epoch [1/3], Step [38600/41412], Loss: 2.3178, Perplexity: 10.1537\n",
      "Epoch [1/3], Step [38700/41412], Loss: 1.7758, Perplexity: 5.905104\n",
      "Epoch [1/3], Step [38800/41412], Loss: 2.6346, Perplexity: 13.9374\n",
      "Epoch [1/3], Step [38900/41412], Loss: 2.7945, Perplexity: 16.3541\n",
      "Epoch [1/3], Step [39000/41412], Loss: 2.5347, Perplexity: 12.6129\n",
      "Epoch [1/3], Step [39100/41412], Loss: 1.8478, Perplexity: 6.34609\n",
      "Epoch [1/3], Step [39200/41412], Loss: 2.1872, Perplexity: 8.91069\n",
      "Epoch [1/3], Step [39300/41412], Loss: 2.2402, Perplexity: 9.39555\n",
      "Epoch [1/3], Step [39400/41412], Loss: 2.2991, Perplexity: 9.96549\n",
      "Epoch [1/3], Step [39500/41412], Loss: 1.9950, Perplexity: 7.35214\n",
      "Epoch [1/3], Step [39600/41412], Loss: 2.2722, Perplexity: 9.70049\n",
      "Epoch [1/3], Step [39700/41412], Loss: 2.1615, Perplexity: 8.68386\n",
      "Epoch [1/3], Step [39800/41412], Loss: 1.6948, Perplexity: 5.44569\n",
      "Epoch [1/3], Step [39900/41412], Loss: 1.9832, Perplexity: 7.26594\n",
      "Epoch [1/3], Step [40000/41412], Loss: 2.2175, Perplexity: 9.18487\n",
      "Epoch [1/3], Step [40100/41412], Loss: 2.9413, Perplexity: 18.9410\n",
      "Epoch [1/3], Step [40200/41412], Loss: 2.3004, Perplexity: 9.97869\n",
      "Epoch [1/3], Step [40300/41412], Loss: 1.8045, Perplexity: 6.07699\n",
      "Epoch [1/3], Step [40400/41412], Loss: 4.5709, Perplexity: 96.6314\n",
      "Epoch [1/3], Step [40500/41412], Loss: 2.0612, Perplexity: 7.85514\n",
      "Epoch [1/3], Step [40600/41412], Loss: 2.1750, Perplexity: 8.80252\n",
      "Epoch [1/3], Step [40700/41412], Loss: 2.0597, Perplexity: 7.84350\n",
      "Epoch [1/3], Step [40800/41412], Loss: 2.1325, Perplexity: 8.43620\n",
      "Epoch [1/3], Step [40900/41412], Loss: 2.4323, Perplexity: 11.3851\n",
      "Epoch [1/3], Step [41000/41412], Loss: 2.1714, Perplexity: 8.77078\n",
      "Epoch [1/3], Step [41100/41412], Loss: 2.5579, Perplexity: 12.9086\n",
      "Epoch [1/3], Step [41200/41412], Loss: 2.1467, Perplexity: 8.55698\n",
      "Epoch [1/3], Step [41300/41412], Loss: 2.1010, Perplexity: 8.17408\n",
      "Epoch [1/3], Step [41400/41412], Loss: 2.1537, Perplexity: 8.61700\n",
      "Epoch [2/3], Step [100/41412], Loss: 1.8279, Perplexity: 6.2211469\n",
      "Epoch [2/3], Step [200/41412], Loss: 1.9837, Perplexity: 7.26940\n",
      "Epoch [2/3], Step [300/41412], Loss: 1.9757, Perplexity: 7.21170\n",
      "Epoch [2/3], Step [400/41412], Loss: 2.0361, Perplexity: 7.66103\n",
      "Epoch [2/3], Step [500/41412], Loss: 1.8767, Perplexity: 6.53167\n",
      "Epoch [2/3], Step [600/41412], Loss: 1.8704, Perplexity: 6.49069\n",
      "Epoch [2/3], Step [700/41412], Loss: 2.0822, Perplexity: 8.02188\n",
      "Epoch [2/3], Step [800/41412], Loss: 1.6535, Perplexity: 5.22534\n",
      "Epoch [2/3], Step [900/41412], Loss: 2.2755, Perplexity: 9.73272\n",
      "Epoch [2/3], Step [1000/41412], Loss: 2.1034, Perplexity: 8.1939\n",
      "Epoch [2/3], Step [1100/41412], Loss: 1.7624, Perplexity: 5.82623\n",
      "Epoch [2/3], Step [1200/41412], Loss: 2.3339, Perplexity: 10.3181\n",
      "Epoch [2/3], Step [1300/41412], Loss: 1.7789, Perplexity: 5.92361\n",
      "Epoch [2/3], Step [1400/41412], Loss: 2.1033, Perplexity: 8.19338\n",
      "Epoch [2/3], Step [1500/41412], Loss: 2.6392, Perplexity: 14.0022\n",
      "Epoch [2/3], Step [1600/41412], Loss: 2.2650, Perplexity: 9.63091\n",
      "Epoch [2/3], Step [1700/41412], Loss: 2.0796, Perplexity: 8.00100\n",
      "Epoch [2/3], Step [1800/41412], Loss: 2.0911, Perplexity: 8.09346\n",
      "Epoch [2/3], Step [1900/41412], Loss: 1.8072, Perplexity: 6.09337\n",
      "Epoch [2/3], Step [2000/41412], Loss: 2.8353, Perplexity: 17.0362\n",
      "Epoch [2/3], Step [2100/41412], Loss: 2.0838, Perplexity: 8.03495\n",
      "Epoch [2/3], Step [2200/41412], Loss: 2.7389, Perplexity: 15.4697\n",
      "Epoch [2/3], Step [2300/41412], Loss: 1.9653, Perplexity: 7.13699\n",
      "Epoch [2/3], Step [2400/41412], Loss: 1.7927, Perplexity: 6.00568\n",
      "Epoch [2/3], Step [2500/41412], Loss: 2.1992, Perplexity: 9.01824\n",
      "Epoch [2/3], Step [2600/41412], Loss: 1.9083, Perplexity: 6.74150\n",
      "Epoch [2/3], Step [2700/41412], Loss: 2.3137, Perplexity: 10.1121\n",
      "Epoch [2/3], Step [2800/41412], Loss: 2.5725, Perplexity: 13.0989\n",
      "Epoch [2/3], Step [2900/41412], Loss: 2.5738, Perplexity: 13.1151\n",
      "Epoch [2/3], Step [3000/41412], Loss: 2.2895, Perplexity: 9.86997\n",
      "Epoch [2/3], Step [3100/41412], Loss: 2.1190, Perplexity: 8.32281\n",
      "Epoch [2/3], Step [3200/41412], Loss: 1.9131, Perplexity: 6.77421\n",
      "Epoch [2/3], Step [3300/41412], Loss: 2.0740, Perplexity: 7.95635\n",
      "Epoch [2/3], Step [3400/41412], Loss: 2.7749, Perplexity: 16.0367\n",
      "Epoch [2/3], Step [3500/41412], Loss: 2.3258, Perplexity: 10.2353\n",
      "Epoch [2/3], Step [3600/41412], Loss: 2.5458, Perplexity: 12.7534\n",
      "Epoch [2/3], Step [3700/41412], Loss: 2.2954, Perplexity: 9.92848\n",
      "Epoch [2/3], Step [3800/41412], Loss: 2.0364, Perplexity: 7.66282\n",
      "Epoch [2/3], Step [3900/41412], Loss: 2.5257, Perplexity: 12.4992\n",
      "Epoch [2/3], Step [4000/41412], Loss: 2.2390, Perplexity: 9.383992\n",
      "Epoch [2/3], Step [4100/41412], Loss: 2.4987, Perplexity: 12.1667\n",
      "Epoch [2/3], Step [4200/41412], Loss: 2.7370, Perplexity: 15.4401\n",
      "Epoch [2/3], Step [4300/41412], Loss: 2.8261, Perplexity: 16.8800\n",
      "Epoch [2/3], Step [4400/41412], Loss: 2.2137, Perplexity: 9.14987\n",
      "Epoch [2/3], Step [4500/41412], Loss: 2.1709, Perplexity: 8.76581\n",
      "Epoch [2/3], Step [4600/41412], Loss: 1.6735, Perplexity: 5.33075\n",
      "Epoch [2/3], Step [4700/41412], Loss: 2.1198, Perplexity: 8.32985\n",
      "Epoch [2/3], Step [4800/41412], Loss: 2.0361, Perplexity: 7.66053\n",
      "Epoch [2/3], Step [4900/41412], Loss: 2.1646, Perplexity: 8.71142\n",
      "Epoch [2/3], Step [5000/41412], Loss: 2.7441, Perplexity: 15.5508\n",
      "Epoch [2/3], Step [5100/41412], Loss: 1.9301, Perplexity: 6.89027\n",
      "Epoch [2/3], Step [5200/41412], Loss: 2.5872, Perplexity: 13.2926\n",
      "Epoch [2/3], Step [5300/41412], Loss: 2.6439, Perplexity: 14.0675\n",
      "Epoch [2/3], Step [5400/41412], Loss: 1.8660, Perplexity: 6.46267\n",
      "Epoch [2/3], Step [5500/41412], Loss: 2.4397, Perplexity: 11.4691\n",
      "Epoch [2/3], Step [5600/41412], Loss: 1.9538, Perplexity: 7.05556\n",
      "Epoch [2/3], Step [5700/41412], Loss: 2.5464, Perplexity: 12.7614\n",
      "Epoch [2/3], Step [5800/41412], Loss: 1.9969, Perplexity: 7.36627\n",
      "Epoch [2/3], Step [5900/41412], Loss: 2.0294, Perplexity: 7.60929\n",
      "Epoch [2/3], Step [6000/41412], Loss: 1.9516, Perplexity: 7.03982\n",
      "Epoch [2/3], Step [6100/41412], Loss: 2.0519, Perplexity: 7.78257\n",
      "Epoch [2/3], Step [6200/41412], Loss: 1.6040, Perplexity: 4.97266\n",
      "Epoch [2/3], Step [6300/41412], Loss: 2.2331, Perplexity: 9.32840\n",
      "Epoch [2/3], Step [6400/41412], Loss: 1.9350, Perplexity: 6.92397\n",
      "Epoch [2/3], Step [6500/41412], Loss: 2.2151, Perplexity: 9.16244\n",
      "Epoch [2/3], Step [6600/41412], Loss: 2.2436, Perplexity: 9.42750\n",
      "Epoch [2/3], Step [6700/41412], Loss: 2.0686, Perplexity: 7.91366\n",
      "Epoch [2/3], Step [6800/41412], Loss: 2.0909, Perplexity: 8.09202\n",
      "Epoch [2/3], Step [6900/41412], Loss: 2.5335, Perplexity: 12.5980\n",
      "Epoch [2/3], Step [7000/41412], Loss: 2.3486, Perplexity: 10.4714\n",
      "Epoch [2/3], Step [7100/41412], Loss: 2.1583, Perplexity: 8.65624\n",
      "Epoch [2/3], Step [7200/41412], Loss: 1.8328, Perplexity: 6.25125\n",
      "Epoch [2/3], Step [7300/41412], Loss: 2.5232, Perplexity: 12.4682\n",
      "Epoch [2/3], Step [7400/41412], Loss: 2.9292, Perplexity: 18.7123\n",
      "Epoch [2/3], Step [7500/41412], Loss: 1.6602, Perplexity: 5.26065\n",
      "Epoch [2/3], Step [7600/41412], Loss: 2.0741, Perplexity: 7.95769\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/3], Step [7700/41412], Loss: 2.1014, Perplexity: 8.17791\n",
      "Epoch [2/3], Step [7800/41412], Loss: 2.2806, Perplexity: 9.78246\n",
      "Epoch [2/3], Step [7900/41412], Loss: 2.5248, Perplexity: 12.4879\n",
      "Epoch [2/3], Step [8000/41412], Loss: 1.7994, Perplexity: 6.04626\n",
      "Epoch [2/3], Step [8100/41412], Loss: 2.1797, Perplexity: 8.84339\n",
      "Epoch [2/3], Step [8200/41412], Loss: 2.4439, Perplexity: 11.5174\n",
      "Epoch [2/3], Step [8300/41412], Loss: 2.3839, Perplexity: 10.8470\n",
      "Epoch [2/3], Step [8400/41412], Loss: 1.6283, Perplexity: 5.09541\n",
      "Epoch [2/3], Step [8500/41412], Loss: 1.9721, Perplexity: 7.18608\n",
      "Epoch [2/3], Step [8600/41412], Loss: 2.1364, Perplexity: 8.46873\n",
      "Epoch [2/3], Step [8700/41412], Loss: 2.1108, Perplexity: 8.25522\n",
      "Epoch [2/3], Step [8800/41412], Loss: 1.8769, Perplexity: 6.53331\n",
      "Epoch [2/3], Step [8900/41412], Loss: 1.8846, Perplexity: 6.58371\n",
      "Epoch [2/3], Step [9000/41412], Loss: 2.1827, Perplexity: 8.87052\n",
      "Epoch [2/3], Step [9100/41412], Loss: 2.4592, Perplexity: 11.6959\n",
      "Epoch [2/3], Step [9200/41412], Loss: 2.2877, Perplexity: 9.85213\n",
      "Epoch [2/3], Step [9300/41412], Loss: 1.9470, Perplexity: 7.00795\n",
      "Epoch [2/3], Step [9400/41412], Loss: 2.4090, Perplexity: 11.1225\n",
      "Epoch [2/3], Step [9500/41412], Loss: 2.1617, Perplexity: 8.68572\n",
      "Epoch [2/3], Step [9600/41412], Loss: 2.6137, Perplexity: 13.6496\n",
      "Epoch [2/3], Step [9700/41412], Loss: 2.0294, Perplexity: 7.60925\n",
      "Epoch [2/3], Step [9800/41412], Loss: 2.1245, Perplexity: 8.36842\n",
      "Epoch [2/3], Step [9900/41412], Loss: 1.9142, Perplexity: 6.78181\n",
      "Epoch [2/3], Step [10000/41412], Loss: 1.9150, Perplexity: 6.7869\n",
      "Epoch [2/3], Step [10100/41412], Loss: 2.0520, Perplexity: 7.78331\n",
      "Epoch [2/3], Step [10200/41412], Loss: 2.4616, Perplexity: 11.7238\n",
      "Epoch [2/3], Step [10300/41412], Loss: 1.9277, Perplexity: 6.87404\n",
      "Epoch [2/3], Step [10400/41412], Loss: 1.9621, Perplexity: 7.11415\n",
      "Epoch [2/3], Step [10500/41412], Loss: 2.5497, Perplexity: 12.8034\n",
      "Epoch [2/3], Step [10600/41412], Loss: 3.1820, Perplexity: 24.0947\n",
      "Epoch [2/3], Step [10700/41412], Loss: 1.8986, Perplexity: 6.67673\n",
      "Epoch [2/3], Step [10800/41412], Loss: 2.4926, Perplexity: 12.0926\n",
      "Epoch [2/3], Step [10900/41412], Loss: 1.9543, Perplexity: 7.05867\n",
      "Epoch [2/3], Step [11000/41412], Loss: 2.4886, Perplexity: 12.0440\n",
      "Epoch [2/3], Step [11100/41412], Loss: 2.4096, Perplexity: 11.1295\n",
      "Epoch [2/3], Step [11200/41412], Loss: 2.1109, Perplexity: 8.25572\n",
      "Epoch [2/3], Step [11300/41412], Loss: 2.1966, Perplexity: 8.99411\n",
      "Epoch [2/3], Step [11400/41412], Loss: 1.9413, Perplexity: 6.96815\n",
      "Epoch [2/3], Step [11500/41412], Loss: 2.5191, Perplexity: 12.4172\n",
      "Epoch [2/3], Step [11600/41412], Loss: 2.8282, Perplexity: 16.9156\n",
      "Epoch [2/3], Step [11700/41412], Loss: 2.0725, Perplexity: 7.94484\n",
      "Epoch [2/3], Step [11800/41412], Loss: 2.1875, Perplexity: 8.91326\n",
      "Epoch [2/3], Step [11900/41412], Loss: 1.6631, Perplexity: 5.27549\n",
      "Epoch [2/3], Step [12000/41412], Loss: 2.1664, Perplexity: 8.72652\n",
      "Epoch [2/3], Step [12100/41412], Loss: 2.8134, Perplexity: 16.6660\n",
      "Epoch [2/3], Step [12200/41412], Loss: 2.0757, Perplexity: 7.96998\n",
      "Epoch [2/3], Step [12300/41412], Loss: 2.2274, Perplexity: 9.27619\n",
      "Epoch [2/3], Step [12400/41412], Loss: 1.9041, Perplexity: 6.71356\n",
      "Epoch [2/3], Step [12500/41412], Loss: 2.4671, Perplexity: 11.7886\n",
      "Epoch [2/3], Step [12600/41412], Loss: 2.2383, Perplexity: 9.37788\n",
      "Epoch [2/3], Step [12700/41412], Loss: 2.5330, Perplexity: 12.5913\n",
      "Epoch [2/3], Step [12800/41412], Loss: 2.8198, Perplexity: 16.7742\n",
      "Epoch [2/3], Step [12900/41412], Loss: 2.8335, Perplexity: 17.0053\n",
      "Epoch [2/3], Step [13000/41412], Loss: 2.2987, Perplexity: 9.96108\n",
      "Epoch [2/3], Step [13100/41412], Loss: 1.4914, Perplexity: 4.44321\n",
      "Epoch [2/3], Step [13200/41412], Loss: 2.5416, Perplexity: 12.7000\n",
      "Epoch [2/3], Step [13300/41412], Loss: 2.1047, Perplexity: 8.20469\n",
      "Epoch [2/3], Step [13400/41412], Loss: 3.0594, Perplexity: 21.3142\n",
      "Epoch [2/3], Step [13500/41412], Loss: 2.4456, Perplexity: 11.5370\n",
      "Epoch [2/3], Step [13600/41412], Loss: 1.7165, Perplexity: 5.56523\n",
      "Epoch [2/3], Step [13700/41412], Loss: 2.0118, Perplexity: 7.47711\n",
      "Epoch [2/3], Step [13800/41412], Loss: 1.5199, Perplexity: 4.57170\n",
      "Epoch [2/3], Step [13900/41412], Loss: 2.1031, Perplexity: 8.19131\n",
      "Epoch [2/3], Step [14000/41412], Loss: 2.6376, Perplexity: 13.9796\n",
      "Epoch [2/3], Step [14100/41412], Loss: 2.3514, Perplexity: 10.5003\n",
      "Epoch [2/3], Step [14200/41412], Loss: 2.3303, Perplexity: 10.2813\n",
      "Epoch [2/3], Step [14300/41412], Loss: 2.3416, Perplexity: 10.3976\n",
      "Epoch [2/3], Step [14400/41412], Loss: 2.0957, Perplexity: 8.13135\n",
      "Epoch [2/3], Step [14500/41412], Loss: 2.0919, Perplexity: 8.10058\n",
      "Epoch [2/3], Step [14600/41412], Loss: 2.0862, Perplexity: 8.05469\n",
      "Epoch [2/3], Step [14700/41412], Loss: 2.3876, Perplexity: 10.8873\n",
      "Epoch [2/3], Step [14800/41412], Loss: 2.4348, Perplexity: 11.4132\n",
      "Epoch [2/3], Step [14900/41412], Loss: 1.8087, Perplexity: 6.10277\n",
      "Epoch [2/3], Step [15000/41412], Loss: 1.8093, Perplexity: 6.10608\n",
      "Epoch [2/3], Step [15100/41412], Loss: 2.4804, Perplexity: 11.9461\n",
      "Epoch [2/3], Step [15200/41412], Loss: 2.2062, Perplexity: 9.08133\n",
      "Epoch [2/3], Step [15300/41412], Loss: 2.0954, Perplexity: 8.12908\n",
      "Epoch [2/3], Step [15400/41412], Loss: 1.9791, Perplexity: 7.23631\n",
      "Epoch [2/3], Step [15500/41412], Loss: 2.6560, Perplexity: 14.2388\n",
      "Epoch [2/3], Step [15600/41412], Loss: 2.0905, Perplexity: 8.08887\n",
      "Epoch [2/3], Step [15700/41412], Loss: 1.9689, Perplexity: 7.16275\n",
      "Epoch [2/3], Step [15800/41412], Loss: 2.5010, Perplexity: 12.1949\n",
      "Epoch [2/3], Step [15900/41412], Loss: 2.7393, Perplexity: 15.4759\n",
      "Epoch [2/3], Step [16000/41412], Loss: 1.6482, Perplexity: 5.19753\n",
      "Epoch [2/3], Step [16100/41412], Loss: 2.0665, Perplexity: 7.89685\n",
      "Epoch [2/3], Step [16200/41412], Loss: 2.1140, Perplexity: 8.28175\n",
      "Epoch [2/3], Step [16300/41412], Loss: 1.8286, Perplexity: 6.22513\n",
      "Epoch [2/3], Step [16400/41412], Loss: 2.3049, Perplexity: 10.0230\n",
      "Epoch [2/3], Step [16500/41412], Loss: 1.9431, Perplexity: 6.98070\n",
      "Epoch [2/3], Step [16600/41412], Loss: 2.7299, Perplexity: 15.3312\n",
      "Epoch [2/3], Step [16700/41412], Loss: 1.7413, Perplexity: 5.70481\n",
      "Epoch [2/3], Step [16800/41412], Loss: 2.2097, Perplexity: 9.11264\n",
      "Epoch [2/3], Step [16900/41412], Loss: 2.1265, Perplexity: 8.38513\n",
      "Epoch [2/3], Step [17000/41412], Loss: 2.2586, Perplexity: 9.56946\n",
      "Epoch [2/3], Step [17100/41412], Loss: 2.0558, Perplexity: 7.81290\n",
      "Epoch [2/3], Step [17200/41412], Loss: 2.4091, Perplexity: 11.1236\n",
      "Epoch [2/3], Step [17300/41412], Loss: 2.0661, Perplexity: 7.89431\n",
      "Epoch [2/3], Step [17400/41412], Loss: 1.9250, Perplexity: 6.85489\n",
      "Epoch [2/3], Step [17500/41412], Loss: 1.8647, Perplexity: 6.45397\n",
      "Epoch [2/3], Step [17600/41412], Loss: 2.1676, Perplexity: 8.73695\n",
      "Epoch [2/3], Step [17700/41412], Loss: 2.0348, Perplexity: 7.65114\n",
      "Epoch [2/3], Step [17800/41412], Loss: 2.4423, Perplexity: 11.4992\n",
      "Epoch [2/3], Step [17900/41412], Loss: 2.3652, Perplexity: 10.6465\n",
      "Epoch [2/3], Step [18000/41412], Loss: 2.0104, Perplexity: 7.46665\n",
      "Epoch [2/3], Step [18100/41412], Loss: 1.8994, Perplexity: 6.68176\n",
      "Epoch [2/3], Step [18200/41412], Loss: 1.9781, Perplexity: 7.22898\n",
      "Epoch [2/3], Step [18300/41412], Loss: 1.8179, Perplexity: 6.15884\n",
      "Epoch [2/3], Step [18400/41412], Loss: 2.0927, Perplexity: 8.10682\n",
      "Epoch [2/3], Step [18500/41412], Loss: 2.0085, Perplexity: 7.45195\n",
      "Epoch [2/3], Step [18600/41412], Loss: 2.1673, Perplexity: 8.73505\n",
      "Epoch [2/3], Step [18700/41412], Loss: 1.6256, Perplexity: 5.08148\n",
      "Epoch [2/3], Step [18800/41412], Loss: 1.6952, Perplexity: 5.44758\n",
      "Epoch [2/3], Step [18900/41412], Loss: 1.9939, Perplexity: 7.34394\n",
      "Epoch [2/3], Step [19000/41412], Loss: 1.9110, Perplexity: 6.75965\n",
      "Epoch [2/3], Step [19100/41412], Loss: 1.8497, Perplexity: 6.35770\n",
      "Epoch [2/3], Step [19200/41412], Loss: 2.0466, Perplexity: 7.74186\n",
      "Epoch [2/3], Step [19300/41412], Loss: 2.6486, Perplexity: 14.1347\n",
      "Epoch [2/3], Step [19400/41412], Loss: 1.9812, Perplexity: 7.25125\n",
      "Epoch [2/3], Step [19500/41412], Loss: 2.1827, Perplexity: 8.86997\n",
      "Epoch [2/3], Step [19600/41412], Loss: 1.4917, Perplexity: 4.44478\n",
      "Epoch [2/3], Step [19700/41412], Loss: 1.5183, Perplexity: 4.56476\n",
      "Epoch [2/3], Step [19800/41412], Loss: 1.7759, Perplexity: 5.90533\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/3], Step [19900/41412], Loss: 2.2591, Perplexity: 9.57407\n",
      "Epoch [2/3], Step [20000/41412], Loss: 1.9491, Perplexity: 7.02249\n",
      "Epoch [2/3], Step [20100/41412], Loss: 1.4661, Perplexity: 4.33245\n",
      "Epoch [2/3], Step [20200/41412], Loss: 1.7104, Perplexity: 5.53133\n",
      "Epoch [2/3], Step [20300/41412], Loss: 2.1214, Perplexity: 8.34293\n",
      "Epoch [2/3], Step [20400/41412], Loss: 2.2747, Perplexity: 9.72489\n",
      "Epoch [2/3], Step [20500/41412], Loss: 1.8337, Perplexity: 6.25722\n",
      "Epoch [2/3], Step [20600/41412], Loss: 1.6976, Perplexity: 5.46115\n",
      "Epoch [2/3], Step [20700/41412], Loss: 1.8797, Perplexity: 6.55186\n",
      "Epoch [2/3], Step [20800/41412], Loss: 1.9777, Perplexity: 7.22602\n",
      "Epoch [2/3], Step [20900/41412], Loss: 1.4021, Perplexity: 4.06396\n",
      "Epoch [2/3], Step [21000/41412], Loss: 1.6836, Perplexity: 5.38513\n",
      "Epoch [2/3], Step [21100/41412], Loss: 2.1330, Perplexity: 8.44016\n",
      "Epoch [2/3], Step [21200/41412], Loss: 2.0849, Perplexity: 8.04391\n",
      "Epoch [2/3], Step [21300/41412], Loss: 2.3492, Perplexity: 10.4767\n",
      "Epoch [2/3], Step [21400/41412], Loss: 2.1274, Perplexity: 8.39295\n",
      "Epoch [2/3], Step [21500/41412], Loss: 2.0139, Perplexity: 7.49269\n",
      "Epoch [2/3], Step [21600/41412], Loss: 1.9695, Perplexity: 7.16720\n",
      "Epoch [2/3], Step [21700/41412], Loss: 2.2745, Perplexity: 9.72284\n",
      "Epoch [2/3], Step [21800/41412], Loss: 2.0767, Perplexity: 7.97836\n",
      "Epoch [2/3], Step [21900/41412], Loss: 2.3625, Perplexity: 10.6170\n",
      "Epoch [2/3], Step [22000/41412], Loss: 1.8629, Perplexity: 6.44210\n",
      "Epoch [2/3], Step [22100/41412], Loss: 2.0921, Perplexity: 8.10178\n",
      "Epoch [2/3], Step [22200/41412], Loss: 1.8494, Perplexity: 6.355768\n",
      "Epoch [2/3], Step [22300/41412], Loss: 2.4590, Perplexity: 11.6930\n",
      "Epoch [2/3], Step [22400/41412], Loss: 2.3934, Perplexity: 10.9509\n",
      "Epoch [2/3], Step [22500/41412], Loss: 2.0596, Perplexity: 7.84250\n",
      "Epoch [2/3], Step [22600/41412], Loss: 2.0558, Perplexity: 7.813175\n",
      "Epoch [2/3], Step [22700/41412], Loss: 1.6056, Perplexity: 4.98078\n",
      "Epoch [2/3], Step [22800/41412], Loss: 2.8606, Perplexity: 17.4718\n",
      "Epoch [2/3], Step [22900/41412], Loss: 2.6563, Perplexity: 14.2428\n",
      "Epoch [2/3], Step [23000/41412], Loss: 2.0773, Perplexity: 7.98254\n",
      "Epoch [2/3], Step [23100/41412], Loss: 1.8487, Perplexity: 6.35166\n",
      "Epoch [2/3], Step [23200/41412], Loss: 1.9414, Perplexity: 6.96851\n",
      "Epoch [2/3], Step [23300/41412], Loss: 1.5258, Perplexity: 4.59863\n",
      "Epoch [2/3], Step [23400/41412], Loss: 2.1902, Perplexity: 8.93725\n",
      "Epoch [2/3], Step [23500/41412], Loss: 2.6756, Perplexity: 14.5207\n",
      "Epoch [2/3], Step [23600/41412], Loss: 2.7921, Perplexity: 16.3159\n",
      "Epoch [2/3], Step [23700/41412], Loss: 2.2843, Perplexity: 9.81867\n",
      "Epoch [2/3], Step [23800/41412], Loss: 2.1677, Perplexity: 8.73840\n",
      "Epoch [2/3], Step [23900/41412], Loss: 1.9413, Perplexity: 6.96773\n",
      "Epoch [2/3], Step [24000/41412], Loss: 1.7226, Perplexity: 5.59891\n",
      "Epoch [2/3], Step [24100/41412], Loss: 2.3220, Perplexity: 10.1955\n",
      "Epoch [2/3], Step [24200/41412], Loss: 1.8787, Perplexity: 6.54512\n",
      "Epoch [2/3], Step [24300/41412], Loss: 2.4523, Perplexity: 11.6152\n",
      "Epoch [2/3], Step [24400/41412], Loss: 3.0284, Perplexity: 20.6634\n",
      "Epoch [2/3], Step [24500/41412], Loss: 2.2593, Perplexity: 9.57667\n",
      "Epoch [2/3], Step [24600/41412], Loss: 1.8220, Perplexity: 6.18407\n",
      "Epoch [2/3], Step [24700/41412], Loss: 2.2839, Perplexity: 9.81470\n",
      "Epoch [2/3], Step [24800/41412], Loss: 2.2138, Perplexity: 9.15075\n",
      "Epoch [2/3], Step [24900/41412], Loss: 2.3858, Perplexity: 10.8677\n",
      "Epoch [2/3], Step [25000/41412], Loss: 2.1156, Perplexity: 8.29460\n",
      "Epoch [2/3], Step [25100/41412], Loss: 2.3167, Perplexity: 10.1423\n",
      "Epoch [2/3], Step [25200/41412], Loss: 2.1574, Perplexity: 8.64839\n",
      "Epoch [2/3], Step [25300/41412], Loss: 2.1460, Perplexity: 8.55077\n",
      "Epoch [2/3], Step [25400/41412], Loss: 1.7919, Perplexity: 6.00068\n",
      "Epoch [2/3], Step [25500/41412], Loss: 1.6674, Perplexity: 5.29852\n",
      "Epoch [2/3], Step [25600/41412], Loss: 1.8091, Perplexity: 6.10524\n",
      "Epoch [2/3], Step [25700/41412], Loss: 2.2396, Perplexity: 9.38949\n",
      "Epoch [2/3], Step [25800/41412], Loss: 1.7784, Perplexity: 5.92017\n",
      "Epoch [2/3], Step [25900/41412], Loss: 2.0208, Perplexity: 7.54455\n",
      "Epoch [2/3], Step [26000/41412], Loss: 1.7099, Perplexity: 5.52835\n",
      "Epoch [2/3], Step [26100/41412], Loss: 2.1817, Perplexity: 8.86178\n",
      "Epoch [2/3], Step [26200/41412], Loss: 2.0622, Perplexity: 7.86309\n",
      "Epoch [2/3], Step [26300/41412], Loss: 2.1005, Perplexity: 8.17058\n",
      "Epoch [2/3], Step [26400/41412], Loss: 2.5935, Perplexity: 13.3763\n",
      "Epoch [2/3], Step [26500/41412], Loss: 1.8871, Perplexity: 6.60008\n",
      "Epoch [2/3], Step [26600/41412], Loss: 1.7253, Perplexity: 5.61412\n",
      "Epoch [2/3], Step [26700/41412], Loss: 2.2315, Perplexity: 9.31387\n",
      "Epoch [2/3], Step [26800/41412], Loss: 2.1540, Perplexity: 8.61972\n",
      "Epoch [2/3], Step [26900/41412], Loss: 2.1181, Perplexity: 8.31533\n",
      "Epoch [2/3], Step [27000/41412], Loss: 2.0177, Perplexity: 7.52129\n",
      "Epoch [2/3], Step [27100/41412], Loss: 1.9535, Perplexity: 7.05351\n",
      "Epoch [2/3], Step [27200/41412], Loss: 2.1638, Perplexity: 8.70407\n",
      "Epoch [2/3], Step [27300/41412], Loss: 2.4234, Perplexity: 11.2837\n",
      "Epoch [2/3], Step [27400/41412], Loss: 2.0688, Perplexity: 7.91508\n",
      "Epoch [2/3], Step [27500/41412], Loss: 1.8371, Perplexity: 6.27848\n",
      "Epoch [2/3], Step [27600/41412], Loss: 2.1720, Perplexity: 8.77618\n",
      "Epoch [2/3], Step [27700/41412], Loss: 1.8015, Perplexity: 6.05868\n",
      "Epoch [2/3], Step [27800/41412], Loss: 2.1451, Perplexity: 8.54286\n",
      "Epoch [2/3], Step [27900/41412], Loss: 2.5301, Perplexity: 12.5547\n",
      "Epoch [2/3], Step [28000/41412], Loss: 1.7411, Perplexity: 5.70383\n",
      "Epoch [2/3], Step [28100/41412], Loss: 2.2631, Perplexity: 9.61270\n",
      "Epoch [2/3], Step [28200/41412], Loss: 1.8079, Perplexity: 6.09793\n",
      "Epoch [2/3], Step [28300/41412], Loss: 2.1108, Perplexity: 8.25486\n",
      "Epoch [2/3], Step [28400/41412], Loss: 2.1830, Perplexity: 8.87289\n",
      "Epoch [2/3], Step [28500/41412], Loss: 2.2038, Perplexity: 9.05903\n",
      "Epoch [2/3], Step [28600/41412], Loss: 2.1437, Perplexity: 8.53073\n",
      "Epoch [2/3], Step [28700/41412], Loss: 1.6635, Perplexity: 5.27789\n",
      "Epoch [2/3], Step [28800/41412], Loss: 2.0181, Perplexity: 7.52367\n",
      "Epoch [2/3], Step [28900/41412], Loss: 2.1435, Perplexity: 8.52932\n",
      "Epoch [2/3], Step [29000/41412], Loss: 2.2424, Perplexity: 9.41626\n",
      "Epoch [2/3], Step [29100/41412], Loss: 1.7256, Perplexity: 5.615822\n",
      "Epoch [2/3], Step [29200/41412], Loss: 2.1044, Perplexity: 8.20259\n",
      "Epoch [2/3], Step [29300/41412], Loss: 3.0512, Perplexity: 21.1398\n",
      "Epoch [2/3], Step [29400/41412], Loss: 2.5804, Perplexity: 13.2020\n",
      "Epoch [2/3], Step [29500/41412], Loss: 1.9614, Perplexity: 7.10921\n",
      "Epoch [2/3], Step [29600/41412], Loss: 4.8471, Perplexity: 127.3754\n",
      "Epoch [2/3], Step [29700/41412], Loss: 2.1130, Perplexity: 8.27300\n",
      "Epoch [2/3], Step [29800/41412], Loss: 1.8991, Perplexity: 6.68018\n",
      "Epoch [2/3], Step [29900/41412], Loss: 1.8702, Perplexity: 6.48936\n",
      "Epoch [2/3], Step [30000/41412], Loss: 1.8341, Perplexity: 6.25928\n",
      "Epoch [2/3], Step [30100/41412], Loss: 3.1752, Perplexity: 23.9314\n",
      "Epoch [2/3], Step [30200/41412], Loss: 2.2810, Perplexity: 9.78658\n",
      "Epoch [2/3], Step [30300/41412], Loss: 1.8113, Perplexity: 6.11845\n",
      "Epoch [2/3], Step [30400/41412], Loss: 1.9221, Perplexity: 6.83501\n",
      "Epoch [2/3], Step [30500/41412], Loss: 2.2397, Perplexity: 9.39060\n",
      "Epoch [2/3], Step [30600/41412], Loss: 1.9123, Perplexity: 6.76900\n",
      "Epoch [2/3], Step [30700/41412], Loss: 2.2482, Perplexity: 9.47115\n",
      "Epoch [2/3], Step [30800/41412], Loss: 1.7657, Perplexity: 5.84589\n",
      "Epoch [2/3], Step [30900/41412], Loss: 2.4464, Perplexity: 11.5462\n",
      "Epoch [2/3], Step [31000/41412], Loss: 1.6764, Perplexity: 5.34602\n",
      "Epoch [2/3], Step [31100/41412], Loss: 1.9475, Perplexity: 7.01105\n",
      "Epoch [2/3], Step [31200/41412], Loss: 2.2640, Perplexity: 9.62118\n",
      "Epoch [2/3], Step [31300/41412], Loss: 1.7922, Perplexity: 6.00266\n",
      "Epoch [2/3], Step [31400/41412], Loss: 1.6981, Perplexity: 5.46384\n",
      "Epoch [2/3], Step [31500/41412], Loss: 2.0005, Perplexity: 7.39284\n",
      "Epoch [2/3], Step [31600/41412], Loss: 2.2489, Perplexity: 9.47766\n",
      "Epoch [2/3], Step [31700/41412], Loss: 1.7503, Perplexity: 5.75625\n",
      "Epoch [2/3], Step [31800/41412], Loss: 2.2883, Perplexity: 9.85813\n",
      "Epoch [2/3], Step [31900/41412], Loss: 1.8470, Perplexity: 6.34111\n",
      "Epoch [2/3], Step [32000/41412], Loss: 1.9342, Perplexity: 6.91840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/3], Step [32100/41412], Loss: 2.1392, Perplexity: 8.49290\n",
      "Epoch [2/3], Step [32200/41412], Loss: 2.3417, Perplexity: 10.3984\n",
      "Epoch [2/3], Step [32300/41412], Loss: 1.7130, Perplexity: 5.54554\n",
      "Epoch [2/3], Step [32400/41412], Loss: 2.1831, Perplexity: 8.87413\n",
      "Epoch [2/3], Step [32500/41412], Loss: 2.3657, Perplexity: 10.6514\n",
      "Epoch [2/3], Step [32600/41412], Loss: 1.9720, Perplexity: 7.18490\n",
      "Epoch [2/3], Step [32700/41412], Loss: 1.6328, Perplexity: 5.11823\n",
      "Epoch [2/3], Step [32800/41412], Loss: 1.8592, Perplexity: 6.41887\n",
      "Epoch [2/3], Step [32900/41412], Loss: 2.1007, Perplexity: 8.17223\n",
      "Epoch [2/3], Step [33000/41412], Loss: 2.4924, Perplexity: 12.0906\n",
      "Epoch [2/3], Step [33100/41412], Loss: 1.8690, Perplexity: 6.48151\n",
      "Epoch [2/3], Step [33200/41412], Loss: 1.9130, Perplexity: 6.77310\n",
      "Epoch [2/3], Step [33300/41412], Loss: 2.5205, Perplexity: 12.4345\n",
      "Epoch [2/3], Step [33400/41412], Loss: 1.5852, Perplexity: 4.88020\n",
      "Epoch [2/3], Step [33500/41412], Loss: 2.0922, Perplexity: 8.10254\n",
      "Epoch [2/3], Step [33600/41412], Loss: 2.2955, Perplexity: 9.92996\n",
      "Epoch [2/3], Step [33700/41412], Loss: 1.8727, Perplexity: 6.50603\n",
      "Epoch [2/3], Step [33800/41412], Loss: 1.4510, Perplexity: 4.26741\n",
      "Epoch [2/3], Step [33900/41412], Loss: 2.8169, Perplexity: 16.7249\n",
      "Epoch [2/3], Step [34000/41412], Loss: 2.2877, Perplexity: 9.85200\n",
      "Epoch [2/3], Step [34100/41412], Loss: 2.0553, Perplexity: 7.80917\n",
      "Epoch [2/3], Step [34200/41412], Loss: 1.7177, Perplexity: 5.57153\n",
      "Epoch [2/3], Step [34300/41412], Loss: 1.9238, Perplexity: 6.84670\n",
      "Epoch [2/3], Step [34400/41412], Loss: 1.9394, Perplexity: 6.95432\n",
      "Epoch [2/3], Step [34500/41412], Loss: 2.1893, Perplexity: 8.92860\n",
      "Epoch [2/3], Step [34600/41412], Loss: 2.3539, Perplexity: 10.5268\n",
      "Epoch [2/3], Step [34700/41412], Loss: 2.0707, Perplexity: 7.93014\n",
      "Epoch [2/3], Step [34800/41412], Loss: 1.4255, Perplexity: 4.15974\n",
      "Epoch [2/3], Step [34900/41412], Loss: 2.3293, Perplexity: 10.2708\n",
      "Epoch [2/3], Step [35000/41412], Loss: 2.7498, Perplexity: 15.6394\n",
      "Epoch [2/3], Step [35100/41412], Loss: 2.3526, Perplexity: 10.5126\n",
      "Epoch [2/3], Step [35200/41412], Loss: 2.5279, Perplexity: 12.5277\n",
      "Epoch [2/3], Step [35300/41412], Loss: 2.2316, Perplexity: 9.31444\n",
      "Epoch [2/3], Step [35400/41412], Loss: 1.9923, Perplexity: 7.33230\n",
      "Epoch [2/3], Step [35500/41412], Loss: 2.3640, Perplexity: 10.6335\n",
      "Epoch [2/3], Step [35600/41412], Loss: 2.2223, Perplexity: 9.22880\n",
      "Epoch [2/3], Step [35700/41412], Loss: 2.7692, Perplexity: 15.9463\n",
      "Epoch [2/3], Step [35800/41412], Loss: 2.4967, Perplexity: 12.1425\n",
      "Epoch [2/3], Step [35900/41412], Loss: 2.1945, Perplexity: 8.97527\n",
      "Epoch [2/3], Step [36000/41412], Loss: 2.4751, Perplexity: 11.8835\n",
      "Epoch [2/3], Step [36100/41412], Loss: 2.4665, Perplexity: 11.7811\n",
      "Epoch [2/3], Step [36200/41412], Loss: 2.5512, Perplexity: 12.8230\n",
      "Epoch [2/3], Step [36300/41412], Loss: 2.0694, Perplexity: 7.92036\n",
      "Epoch [2/3], Step [36400/41412], Loss: 2.3684, Perplexity: 10.6798\n",
      "Epoch [2/3], Step [36500/41412], Loss: 1.8959, Perplexity: 6.65888\n",
      "Epoch [2/3], Step [36600/41412], Loss: 2.1051, Perplexity: 8.20812\n",
      "Epoch [2/3], Step [36700/41412], Loss: 2.1621, Perplexity: 8.68916\n",
      "Epoch [2/3], Step [36800/41412], Loss: 1.7283, Perplexity: 5.63097\n",
      "Epoch [2/3], Step [36900/41412], Loss: 1.8100, Perplexity: 6.11025\n",
      "Epoch [2/3], Step [37000/41412], Loss: 2.0907, Perplexity: 8.09026\n",
      "Epoch [2/3], Step [37100/41412], Loss: 1.8684, Perplexity: 6.47787\n",
      "Epoch [2/3], Step [37200/41412], Loss: 2.4353, Perplexity: 11.4193\n",
      "Epoch [2/3], Step [37300/41412], Loss: 2.0009, Perplexity: 7.39616\n",
      "Epoch [2/3], Step [37400/41412], Loss: 2.7941, Perplexity: 16.3472\n",
      "Epoch [2/3], Step [37500/41412], Loss: 1.7982, Perplexity: 6.03880\n",
      "Epoch [2/3], Step [37600/41412], Loss: 2.4615, Perplexity: 11.7224\n",
      "Epoch [2/3], Step [37700/41412], Loss: 1.8514, Perplexity: 6.36871\n",
      "Epoch [2/3], Step [37800/41412], Loss: 2.2361, Perplexity: 9.35722\n",
      "Epoch [2/3], Step [37900/41412], Loss: 2.2554, Perplexity: 9.53893\n",
      "Epoch [2/3], Step [38000/41412], Loss: 2.0739, Perplexity: 7.95564\n",
      "Epoch [2/3], Step [38100/41412], Loss: 2.3132, Perplexity: 10.1064\n",
      "Epoch [2/3], Step [38200/41412], Loss: 1.9232, Perplexity: 6.84270\n",
      "Epoch [2/3], Step [38300/41412], Loss: 2.4152, Perplexity: 11.1920\n",
      "Epoch [2/3], Step [38400/41412], Loss: 2.1934, Perplexity: 8.96562\n",
      "Epoch [2/3], Step [38500/41412], Loss: 2.1204, Perplexity: 8.33430\n",
      "Epoch [2/3], Step [38600/41412], Loss: 2.1342, Perplexity: 8.44993\n",
      "Epoch [2/3], Step [38700/41412], Loss: 2.2025, Perplexity: 9.04759\n",
      "Epoch [2/3], Step [38800/41412], Loss: 2.4900, Perplexity: 12.0613\n",
      "Epoch [2/3], Step [38900/41412], Loss: 2.3965, Perplexity: 10.9844\n",
      "Epoch [2/3], Step [39000/41412], Loss: 2.3518, Perplexity: 10.5047\n",
      "Epoch [2/3], Step [39100/41412], Loss: 2.2598, Perplexity: 9.58169\n",
      "Epoch [2/3], Step [39200/41412], Loss: 2.1624, Perplexity: 8.69162\n",
      "Epoch [2/3], Step [39300/41412], Loss: 2.1795, Perplexity: 8.84238\n",
      "Epoch [2/3], Step [39400/41412], Loss: 2.1989, Perplexity: 9.01510\n",
      "Epoch [2/3], Step [39500/41412], Loss: 1.6910, Perplexity: 5.42475\n",
      "Epoch [2/3], Step [39600/41412], Loss: 2.1083, Perplexity: 8.23406\n",
      "Epoch [2/3], Step [39700/41412], Loss: 2.0476, Perplexity: 7.74961\n",
      "Epoch [2/3], Step [39800/41412], Loss: 1.9362, Perplexity: 6.93233\n",
      "Epoch [2/3], Step [39900/41412], Loss: 1.8008, Perplexity: 6.05422\n",
      "Epoch [2/3], Step [40000/41412], Loss: 1.9565, Perplexity: 7.07447\n",
      "Epoch [2/3], Step [40100/41412], Loss: 2.0875, Perplexity: 8.06449\n",
      "Epoch [2/3], Step [40200/41412], Loss: 2.1176, Perplexity: 8.31112\n",
      "Epoch [2/3], Step [40300/41412], Loss: 1.9054, Perplexity: 6.72229\n",
      "Epoch [2/3], Step [40400/41412], Loss: 1.7747, Perplexity: 5.89880\n",
      "Epoch [2/3], Step [40500/41412], Loss: 2.0467, Perplexity: 7.74232\n",
      "Epoch [2/3], Step [40600/41412], Loss: 2.1761, Perplexity: 8.81202\n",
      "Epoch [2/3], Step [40700/41412], Loss: 1.9029, Perplexity: 6.70554\n",
      "Epoch [2/3], Step [40800/41412], Loss: 1.6983, Perplexity: 5.46472\n",
      "Epoch [2/3], Step [40900/41412], Loss: 2.0582, Perplexity: 7.83229\n",
      "Epoch [2/3], Step [41000/41412], Loss: 2.0115, Perplexity: 7.47480\n",
      "Epoch [2/3], Step [41100/41412], Loss: 1.9201, Perplexity: 6.82131\n",
      "Epoch [2/3], Step [41200/41412], Loss: 1.6288, Perplexity: 5.09808\n",
      "Epoch [2/3], Step [41300/41412], Loss: 2.1657, Perplexity: 8.72077\n",
      "Epoch [2/3], Step [41400/41412], Loss: 2.0080, Perplexity: 7.44840\n",
      "Epoch [3/3], Step [100/41412], Loss: 1.9063, Perplexity: 6.7280990\n",
      "Epoch [3/3], Step [200/41412], Loss: 2.1524, Perplexity: 8.60583\n",
      "Epoch [3/3], Step [300/41412], Loss: 1.5038, Perplexity: 4.49873\n",
      "Epoch [3/3], Step [400/41412], Loss: 1.7446, Perplexity: 5.72391\n",
      "Epoch [3/3], Step [500/41412], Loss: 2.1271, Perplexity: 8.39060\n",
      "Epoch [3/3], Step [600/41412], Loss: 2.1895, Perplexity: 8.93048\n",
      "Epoch [3/3], Step [700/41412], Loss: 2.6656, Perplexity: 14.3764\n",
      "Epoch [3/3], Step [800/41412], Loss: 2.5457, Perplexity: 12.7523\n",
      "Epoch [3/3], Step [900/41412], Loss: 2.1341, Perplexity: 8.44926\n",
      "Epoch [3/3], Step [1000/41412], Loss: 1.8984, Perplexity: 6.6755\n",
      "Epoch [3/3], Step [1100/41412], Loss: 2.0662, Perplexity: 7.89444\n",
      "Epoch [3/3], Step [1200/41412], Loss: 1.8465, Perplexity: 6.33786\n",
      "Epoch [3/3], Step [1300/41412], Loss: 2.9124, Perplexity: 18.4007\n",
      "Epoch [3/3], Step [1400/41412], Loss: 1.7904, Perplexity: 5.99181\n",
      "Epoch [3/3], Step [1500/41412], Loss: 1.8546, Perplexity: 6.38928\n",
      "Epoch [3/3], Step [1600/41412], Loss: 2.3278, Perplexity: 10.2552\n",
      "Epoch [3/3], Step [1700/41412], Loss: 2.1981, Perplexity: 9.00780\n",
      "Epoch [3/3], Step [1800/41412], Loss: 2.3143, Perplexity: 10.1176\n",
      "Epoch [3/3], Step [1900/41412], Loss: 2.4151, Perplexity: 11.1906\n",
      "Epoch [3/3], Step [2000/41412], Loss: 2.3391, Perplexity: 10.3723\n",
      "Epoch [3/3], Step [2100/41412], Loss: 1.6885, Perplexity: 5.41163\n",
      "Epoch [3/3], Step [2200/41412], Loss: 1.6209, Perplexity: 5.05781\n",
      "Epoch [3/3], Step [2300/41412], Loss: 1.5758, Perplexity: 4.83484\n",
      "Epoch [3/3], Step [2400/41412], Loss: 2.0805, Perplexity: 8.00872\n",
      "Epoch [3/3], Step [2500/41412], Loss: 1.9360, Perplexity: 6.93089\n",
      "Epoch [3/3], Step [2600/41412], Loss: 1.9001, Perplexity: 6.68679\n",
      "Epoch [3/3], Step [2700/41412], Loss: 2.4408, Perplexity: 11.4828\n",
      "Epoch [3/3], Step [2800/41412], Loss: 2.0938, Perplexity: 8.11557\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/3], Step [2900/41412], Loss: 2.0211, Perplexity: 7.54666\n",
      "Epoch [3/3], Step [3000/41412], Loss: 2.1555, Perplexity: 8.63239\n",
      "Epoch [3/3], Step [3100/41412], Loss: 2.1748, Perplexity: 8.80026\n",
      "Epoch [3/3], Step [3200/41412], Loss: 1.9004, Perplexity: 6.68841\n",
      "Epoch [3/3], Step [3300/41412], Loss: 2.5239, Perplexity: 12.4769\n",
      "Epoch [3/3], Step [3400/41412], Loss: 1.7351, Perplexity: 5.66925\n",
      "Epoch [3/3], Step [3500/41412], Loss: 2.2424, Perplexity: 9.41573\n",
      "Epoch [3/3], Step [3600/41412], Loss: 2.6455, Perplexity: 14.0904\n",
      "Epoch [3/3], Step [3700/41412], Loss: 2.1382, Perplexity: 8.48417\n",
      "Epoch [3/3], Step [3800/41412], Loss: 2.2261, Perplexity: 9.26401\n",
      "Epoch [3/3], Step [3900/41412], Loss: 1.9332, Perplexity: 6.91138\n",
      "Epoch [3/3], Step [4000/41412], Loss: 1.7474, Perplexity: 5.73992\n",
      "Epoch [3/3], Step [4100/41412], Loss: 2.3000, Perplexity: 9.97446\n",
      "Epoch [3/3], Step [4200/41412], Loss: 2.2915, Perplexity: 9.88949\n",
      "Epoch [3/3], Step [4300/41412], Loss: 1.9118, Perplexity: 6.76525\n",
      "Epoch [3/3], Step [4400/41412], Loss: 2.2675, Perplexity: 9.65514\n",
      "Epoch [3/3], Step [4500/41412], Loss: 2.2303, Perplexity: 9.30294\n",
      "Epoch [3/3], Step [4600/41412], Loss: 2.2922, Perplexity: 9.89671\n",
      "Epoch [3/3], Step [4700/41412], Loss: 1.9088, Perplexity: 6.74529\n",
      "Epoch [3/3], Step [4800/41412], Loss: 2.4079, Perplexity: 11.1103\n",
      "Epoch [3/3], Step [4900/41412], Loss: 1.6484, Perplexity: 5.19883\n",
      "Epoch [3/3], Step [5000/41412], Loss: 1.8546, Perplexity: 6.38946\n",
      "Epoch [3/3], Step [5100/41412], Loss: 2.0835, Perplexity: 8.03254\n",
      "Epoch [3/3], Step [5200/41412], Loss: 2.0537, Perplexity: 7.79669\n",
      "Epoch [3/3], Step [5300/41412], Loss: 2.1966, Perplexity: 8.99407\n",
      "Epoch [3/3], Step [5400/41412], Loss: 1.8582, Perplexity: 6.41242\n",
      "Epoch [3/3], Step [5500/41412], Loss: 1.5420, Perplexity: 4.67408\n",
      "Epoch [3/3], Step [5600/41412], Loss: 2.0385, Perplexity: 7.67931\n",
      "Epoch [3/3], Step [5700/41412], Loss: 2.0786, Perplexity: 7.99344\n",
      "Epoch [3/3], Step [5800/41412], Loss: 1.8976, Perplexity: 6.66995\n",
      "Epoch [3/3], Step [5900/41412], Loss: 2.3473, Perplexity: 10.4572\n",
      "Epoch [3/3], Step [6000/41412], Loss: 2.3065, Perplexity: 10.0395\n",
      "Epoch [3/3], Step [6100/41412], Loss: 2.1628, Perplexity: 8.69511\n",
      "Epoch [3/3], Step [6200/41412], Loss: 1.7390, Perplexity: 5.69161\n",
      "Epoch [3/3], Step [6300/41412], Loss: 2.1297, Perplexity: 8.41241\n",
      "Epoch [3/3], Step [6400/41412], Loss: 1.8073, Perplexity: 6.09376\n",
      "Epoch [3/3], Step [6500/41412], Loss: 2.5561, Perplexity: 12.8850\n",
      "Epoch [3/3], Step [6600/41412], Loss: 2.0295, Perplexity: 7.61057\n",
      "Epoch [3/3], Step [6700/41412], Loss: 1.6877, Perplexity: 5.40720\n",
      "Epoch [3/3], Step [6800/41412], Loss: 1.9296, Perplexity: 6.88705\n",
      "Epoch [3/3], Step [6900/41412], Loss: 2.4201, Perplexity: 11.2466\n",
      "Epoch [3/3], Step [7000/41412], Loss: 2.0375, Perplexity: 7.67130\n",
      "Epoch [3/3], Step [7100/41412], Loss: 1.6981, Perplexity: 5.46349\n",
      "Epoch [3/3], Step [7200/41412], Loss: 1.8496, Perplexity: 6.35701\n",
      "Epoch [3/3], Step [7300/41412], Loss: 2.6755, Perplexity: 14.5202\n",
      "Epoch [3/3], Step [7400/41412], Loss: 1.6847, Perplexity: 5.39072\n",
      "Epoch [3/3], Step [7500/41412], Loss: 1.6476, Perplexity: 5.19460\n",
      "Epoch [3/3], Step [7600/41412], Loss: 2.4352, Perplexity: 11.4181\n",
      "Epoch [3/3], Step [7700/41412], Loss: 2.3207, Perplexity: 10.1830\n",
      "Epoch [3/3], Step [7800/41412], Loss: 1.8152, Perplexity: 6.14241\n",
      "Epoch [3/3], Step [7900/41412], Loss: 2.0608, Perplexity: 7.85260\n",
      "Epoch [3/3], Step [8000/41412], Loss: 1.2742, Perplexity: 3.57604\n",
      "Epoch [3/3], Step [8100/41412], Loss: 2.0662, Perplexity: 7.89485\n",
      "Epoch [3/3], Step [8200/41412], Loss: 2.3134, Perplexity: 10.1088\n",
      "Epoch [3/3], Step [8300/41412], Loss: 1.5889, Perplexity: 4.89836\n",
      "Epoch [3/3], Step [8400/41412], Loss: 2.0206, Perplexity: 7.54286\n",
      "Epoch [3/3], Step [8500/41412], Loss: 2.0612, Perplexity: 7.85564\n",
      "Epoch [3/3], Step [8600/41412], Loss: 1.7364, Perplexity: 5.67672\n",
      "Epoch [3/3], Step [8700/41412], Loss: 2.0525, Perplexity: 7.78743\n",
      "Epoch [3/3], Step [8800/41412], Loss: 2.3768, Perplexity: 10.7705\n",
      "Epoch [3/3], Step [8900/41412], Loss: 1.9613, Perplexity: 7.10864\n",
      "Epoch [3/3], Step [9000/41412], Loss: 1.6873, Perplexity: 5.40491\n",
      "Epoch [3/3], Step [9100/41412], Loss: 2.2967, Perplexity: 9.94167\n",
      "Epoch [3/3], Step [9200/41412], Loss: 1.8876, Perplexity: 6.60354\n",
      "Epoch [3/3], Step [9300/41412], Loss: 2.5261, Perplexity: 12.5051\n",
      "Epoch [3/3], Step [9400/41412], Loss: 2.2603, Perplexity: 9.58620\n",
      "Epoch [3/3], Step [9500/41412], Loss: 1.8270, Perplexity: 6.21529\n",
      "Epoch [3/3], Step [9600/41412], Loss: 1.7316, Perplexity: 5.64953\n",
      "Epoch [3/3], Step [9700/41412], Loss: 2.5089, Perplexity: 12.2911\n",
      "Epoch [3/3], Step [9800/41412], Loss: 2.2182, Perplexity: 9.19088\n",
      "Epoch [3/3], Step [9900/41412], Loss: 1.8511, Perplexity: 6.36681\n",
      "Epoch [3/3], Step [10000/41412], Loss: 2.5401, Perplexity: 12.6813\n",
      "Epoch [3/3], Step [10100/41412], Loss: 2.1818, Perplexity: 8.86232\n",
      "Epoch [3/3], Step [10200/41412], Loss: 1.8281, Perplexity: 6.22194\n",
      "Epoch [3/3], Step [10300/41412], Loss: 2.6738, Perplexity: 14.4955\n",
      "Epoch [3/3], Step [10400/41412], Loss: 1.7542, Perplexity: 5.77897\n",
      "Epoch [3/3], Step [10500/41412], Loss: 2.0109, Perplexity: 7.46997\n",
      "Epoch [3/3], Step [10600/41412], Loss: 2.1962, Perplexity: 8.99047\n",
      "Epoch [3/3], Step [10700/41412], Loss: 1.7688, Perplexity: 5.86363\n",
      "Epoch [3/3], Step [10800/41412], Loss: 1.9709, Perplexity: 7.17749\n",
      "Epoch [3/3], Step [10900/41412], Loss: 2.1835, Perplexity: 8.87752\n",
      "Epoch [3/3], Step [11000/41412], Loss: 2.5328, Perplexity: 12.5893\n",
      "Epoch [3/3], Step [11100/41412], Loss: 1.9948, Perplexity: 7.35068\n",
      "Epoch [3/3], Step [11200/41412], Loss: 2.5142, Perplexity: 12.3566\n",
      "Epoch [3/3], Step [11300/41412], Loss: 2.1309, Perplexity: 8.42213\n",
      "Epoch [3/3], Step [11400/41412], Loss: 1.9806, Perplexity: 7.24739\n",
      "Epoch [3/3], Step [11500/41412], Loss: 1.6182, Perplexity: 5.04421\n",
      "Epoch [3/3], Step [11600/41412], Loss: 2.4501, Perplexity: 11.5893\n",
      "Epoch [3/3], Step [11700/41412], Loss: 2.0871, Perplexity: 8.06170\n",
      "Epoch [3/3], Step [11800/41412], Loss: 1.9527, Perplexity: 7.04776\n",
      "Epoch [3/3], Step [11900/41412], Loss: 2.2383, Perplexity: 9.37729\n",
      "Epoch [3/3], Step [12000/41412], Loss: 1.9672, Perplexity: 7.15104\n",
      "Epoch [3/3], Step [12100/41412], Loss: 2.1453, Perplexity: 8.54494\n",
      "Epoch [3/3], Step [12200/41412], Loss: 1.6053, Perplexity: 4.97920\n",
      "Epoch [3/3], Step [12300/41412], Loss: 2.1186, Perplexity: 8.31915\n",
      "Epoch [3/3], Step [12400/41412], Loss: 1.9163, Perplexity: 6.79602\n",
      "Epoch [3/3], Step [12500/41412], Loss: 2.1269, Perplexity: 8.38914\n",
      "Epoch [3/3], Step [12600/41412], Loss: 1.7497, Perplexity: 5.75321\n",
      "Epoch [3/3], Step [12700/41412], Loss: 2.3101, Perplexity: 10.0759\n",
      "Epoch [3/3], Step [12800/41412], Loss: 2.0468, Perplexity: 7.74291\n",
      "Epoch [3/3], Step [12900/41412], Loss: 2.3345, Perplexity: 10.3240\n",
      "Epoch [3/3], Step [13000/41412], Loss: 1.8777, Perplexity: 6.53834\n",
      "Epoch [3/3], Step [13100/41412], Loss: 1.8933, Perplexity: 6.64104\n",
      "Epoch [3/3], Step [13200/41412], Loss: 1.8637, Perplexity: 6.44720\n",
      "Epoch [3/3], Step [13300/41412], Loss: 2.1013, Perplexity: 8.17670\n",
      "Epoch [3/3], Step [13400/41412], Loss: 1.6848, Perplexity: 5.39139\n",
      "Epoch [3/3], Step [13500/41412], Loss: 2.4141, Perplexity: 11.1797\n",
      "Epoch [3/3], Step [13600/41412], Loss: 1.9359, Perplexity: 6.93022\n",
      "Epoch [3/3], Step [13700/41412], Loss: 2.0438, Perplexity: 7.72037\n",
      "Epoch [3/3], Step [13800/41412], Loss: 2.6029, Perplexity: 13.5030\n",
      "Epoch [3/3], Step [13900/41412], Loss: 2.2390, Perplexity: 9.38404\n",
      "Epoch [3/3], Step [14000/41412], Loss: 2.1259, Perplexity: 8.38034\n",
      "Epoch [3/3], Step [14100/41412], Loss: 2.0043, Perplexity: 7.42071\n",
      "Epoch [3/3], Step [14200/41412], Loss: 1.5644, Perplexity: 4.77975\n",
      "Epoch [3/3], Step [14300/41412], Loss: 2.1997, Perplexity: 9.02222\n",
      "Epoch [3/3], Step [14400/41412], Loss: 1.9166, Perplexity: 6.79791\n",
      "Epoch [3/3], Step [14500/41412], Loss: 1.8284, Perplexity: 6.22413\n",
      "Epoch [3/3], Step [14600/41412], Loss: 2.1407, Perplexity: 8.50558\n",
      "Epoch [3/3], Step [14700/41412], Loss: 1.9560, Perplexity: 7.07130\n",
      "Epoch [3/3], Step [14800/41412], Loss: 1.9220, Perplexity: 6.83499\n",
      "Epoch [3/3], Step [14900/41412], Loss: 1.8156, Perplexity: 6.14471\n",
      "Epoch [3/3], Step [15000/41412], Loss: 2.3106, Perplexity: 10.0804\n",
      "Epoch [3/3], Step [15100/41412], Loss: 1.8066, Perplexity: 6.08982\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/3], Step [15200/41412], Loss: 2.3313, Perplexity: 10.2911\n",
      "Epoch [3/3], Step [15300/41412], Loss: 2.2315, Perplexity: 9.31354\n",
      "Epoch [3/3], Step [15400/41412], Loss: 1.4803, Perplexity: 4.39412\n",
      "Epoch [3/3], Step [15500/41412], Loss: 2.1931, Perplexity: 8.96253\n",
      "Epoch [3/3], Step [15600/41412], Loss: 2.5301, Perplexity: 12.5541\n",
      "Epoch [3/3], Step [15700/41412], Loss: 2.0332, Perplexity: 7.63883\n",
      "Epoch [3/3], Step [15800/41412], Loss: 2.2818, Perplexity: 9.79418\n",
      "Epoch [3/3], Step [15900/41412], Loss: 1.8545, Perplexity: 6.38853\n",
      "Epoch [3/3], Step [16000/41412], Loss: 1.5414, Perplexity: 4.67136\n",
      "Epoch [3/3], Step [16100/41412], Loss: 2.0676, Perplexity: 7.90589\n",
      "Epoch [3/3], Step [16200/41412], Loss: 2.0283, Perplexity: 7.60118\n",
      "Epoch [3/3], Step [16300/41412], Loss: 1.8137, Perplexity: 6.13299\n",
      "Epoch [3/3], Step [16400/41412], Loss: 2.2516, Perplexity: 9.50300\n",
      "Epoch [3/3], Step [16500/41412], Loss: 1.9125, Perplexity: 6.77036\n",
      "Epoch [3/3], Step [16600/41412], Loss: 2.0162, Perplexity: 7.50944\n",
      "Epoch [3/3], Step [16700/41412], Loss: 2.7342, Perplexity: 15.3971\n",
      "Epoch [3/3], Step [16800/41412], Loss: 2.2426, Perplexity: 9.41787\n",
      "Epoch [3/3], Step [16900/41412], Loss: 2.1955, Perplexity: 8.98485\n",
      "Epoch [3/3], Step [17000/41412], Loss: 2.1375, Perplexity: 8.47810\n",
      "Epoch [3/3], Step [17100/41412], Loss: 1.7595, Perplexity: 5.80989\n",
      "Epoch [3/3], Step [17200/41412], Loss: 2.3529, Perplexity: 10.5155\n",
      "Epoch [3/3], Step [17300/41412], Loss: 1.4644, Perplexity: 4.32517\n",
      "Epoch [3/3], Step [17400/41412], Loss: 2.3990, Perplexity: 11.0117\n",
      "Epoch [3/3], Step [17500/41412], Loss: 2.1532, Perplexity: 8.61231\n",
      "Epoch [3/3], Step [17600/41412], Loss: 1.6658, Perplexity: 5.29011\n",
      "Epoch [3/3], Step [17700/41412], Loss: 1.7606, Perplexity: 5.81596\n",
      "Epoch [3/3], Step [17800/41412], Loss: 1.5068, Perplexity: 4.51232\n",
      "Epoch [3/3], Step [17900/41412], Loss: 1.8553, Perplexity: 6.39365\n",
      "Epoch [3/3], Step [18000/41412], Loss: 2.2535, Perplexity: 9.52087\n",
      "Epoch [3/3], Step [18100/41412], Loss: 1.8527, Perplexity: 6.37677\n",
      "Epoch [3/3], Step [18200/41412], Loss: 1.7779, Perplexity: 5.91722\n",
      "Epoch [3/3], Step [18300/41412], Loss: 2.1873, Perplexity: 8.91115\n",
      "Epoch [3/3], Step [18400/41412], Loss: 2.0798, Perplexity: 8.00301\n",
      "Epoch [3/3], Step [18500/41412], Loss: 2.1361, Perplexity: 8.46677\n",
      "Epoch [3/3], Step [18600/41412], Loss: 1.8893, Perplexity: 6.61507\n",
      "Epoch [3/3], Step [18700/41412], Loss: 3.5433, Perplexity: 34.5819\n",
      "Epoch [3/3], Step [18800/41412], Loss: 1.9887, Perplexity: 7.30638\n",
      "Epoch [3/3], Step [18900/41412], Loss: 1.9537, Perplexity: 7.05483\n",
      "Epoch [3/3], Step [19000/41412], Loss: 1.6462, Perplexity: 5.18700\n",
      "Epoch [3/3], Step [19100/41412], Loss: 1.6991, Perplexity: 5.46917\n",
      "Epoch [3/3], Step [19200/41412], Loss: 2.1550, Perplexity: 8.62825\n",
      "Epoch [3/3], Step [19300/41412], Loss: 2.2069, Perplexity: 9.08782\n",
      "Epoch [3/3], Step [19400/41412], Loss: 1.6761, Perplexity: 5.34487\n",
      "Epoch [3/3], Step [19500/41412], Loss: 2.1798, Perplexity: 8.84433\n",
      "Epoch [3/3], Step [19600/41412], Loss: 1.8976, Perplexity: 6.67002\n",
      "Epoch [3/3], Step [19700/41412], Loss: 1.8413, Perplexity: 6.30509\n",
      "Epoch [3/3], Step [19800/41412], Loss: 2.0085, Perplexity: 7.45220\n",
      "Epoch [3/3], Step [19900/41412], Loss: 1.8212, Perplexity: 6.17969\n",
      "Epoch [3/3], Step [20000/41412], Loss: 2.3360, Perplexity: 10.3403\n",
      "Epoch [3/3], Step [20100/41412], Loss: 1.7456, Perplexity: 5.72915\n",
      "Epoch [3/3], Step [20200/41412], Loss: 2.3899, Perplexity: 10.9125\n",
      "Epoch [3/3], Step [20300/41412], Loss: 1.6360, Perplexity: 5.13450\n",
      "Epoch [3/3], Step [20400/41412], Loss: 1.7010, Perplexity: 5.47944\n",
      "Epoch [3/3], Step [20500/41412], Loss: 2.1338, Perplexity: 8.44720\n",
      "Epoch [3/3], Step [20600/41412], Loss: 1.9940, Perplexity: 7.34500\n",
      "Epoch [3/3], Step [20700/41412], Loss: 2.1398, Perplexity: 8.49791\n",
      "Epoch [3/3], Step [20800/41412], Loss: 1.8767, Perplexity: 6.53161\n",
      "Epoch [3/3], Step [20900/41412], Loss: 1.7757, Perplexity: 5.90431\n",
      "Epoch [3/3], Step [21000/41412], Loss: 1.8439, Perplexity: 6.32097\n",
      "Epoch [3/3], Step [21100/41412], Loss: 1.9542, Perplexity: 7.05844\n",
      "Epoch [3/3], Step [21200/41412], Loss: 2.0849, Perplexity: 8.04404\n",
      "Epoch [3/3], Step [21300/41412], Loss: 2.0124, Perplexity: 7.48128\n",
      "Epoch [3/3], Step [21400/41412], Loss: 2.1777, Perplexity: 8.82567\n",
      "Epoch [3/3], Step [21500/41412], Loss: 2.0486, Perplexity: 7.75707\n",
      "Epoch [3/3], Step [21600/41412], Loss: 1.7876, Perplexity: 5.97492\n",
      "Epoch [3/3], Step [21700/41412], Loss: 2.0661, Perplexity: 7.89402\n",
      "Epoch [3/3], Step [21800/41412], Loss: 2.0601, Perplexity: 7.84675\n",
      "Epoch [3/3], Step [21900/41412], Loss: 1.9458, Perplexity: 6.99897\n",
      "Epoch [3/3], Step [22000/41412], Loss: 2.0932, Perplexity: 8.11093\n",
      "Epoch [3/3], Step [22100/41412], Loss: 1.9643, Perplexity: 7.12983\n",
      "Epoch [3/3], Step [22200/41412], Loss: 1.7172, Perplexity: 5.56882\n",
      "Epoch [3/3], Step [22300/41412], Loss: 2.4379, Perplexity: 11.4493\n",
      "Epoch [3/3], Step [22400/41412], Loss: 1.6890, Perplexity: 5.41391\n",
      "Epoch [3/3], Step [22500/41412], Loss: 1.9277, Perplexity: 6.87395\n",
      "Epoch [3/3], Step [22600/41412], Loss: 2.2595, Perplexity: 9.57887\n",
      "Epoch [3/3], Step [22700/41412], Loss: 2.2756, Perplexity: 9.73373\n",
      "Epoch [3/3], Step [22800/41412], Loss: 2.3272, Perplexity: 10.2494\n",
      "Epoch [3/3], Step [22900/41412], Loss: 2.4193, Perplexity: 11.2377\n",
      "Epoch [3/3], Step [23000/41412], Loss: 1.9327, Perplexity: 6.90823\n",
      "Epoch [3/3], Step [23100/41412], Loss: 2.3895, Perplexity: 10.9083\n",
      "Epoch [3/3], Step [23200/41412], Loss: 1.8676, Perplexity: 6.47269\n",
      "Epoch [3/3], Step [23300/41412], Loss: 2.6342, Perplexity: 13.9323\n",
      "Epoch [3/3], Step [23400/41412], Loss: 2.0199, Perplexity: 7.53747\n",
      "Epoch [3/3], Step [23500/41412], Loss: 2.1808, Perplexity: 8.85326\n",
      "Epoch [3/3], Step [23600/41412], Loss: 2.2014, Perplexity: 9.03809\n",
      "Epoch [3/3], Step [23700/41412], Loss: 2.7647, Perplexity: 15.8739\n",
      "Epoch [3/3], Step [23800/41412], Loss: 1.9875, Perplexity: 7.296937\n",
      "Epoch [3/3], Step [23900/41412], Loss: 2.3151, Perplexity: 10.1262\n",
      "Epoch [3/3], Step [24000/41412], Loss: 1.6573, Perplexity: 5.24513\n",
      "Epoch [3/3], Step [24100/41412], Loss: 1.7221, Perplexity: 5.59607\n",
      "Epoch [3/3], Step [24200/41412], Loss: 2.0407, Perplexity: 7.69581\n",
      "Epoch [3/3], Step [24300/41412], Loss: 2.4467, Perplexity: 11.5498\n",
      "Epoch [3/3], Step [24400/41412], Loss: 2.1284, Perplexity: 8.40170\n",
      "Epoch [3/3], Step [24500/41412], Loss: 2.5466, Perplexity: 12.7640\n",
      "Epoch [3/3], Step [24600/41412], Loss: 2.1177, Perplexity: 8.31242\n",
      "Epoch [3/3], Step [24700/41412], Loss: 2.0035, Perplexity: 7.41505\n",
      "Epoch [3/3], Step [24800/41412], Loss: 1.8076, Perplexity: 6.09610\n",
      "Epoch [3/3], Step [24900/41412], Loss: 2.5003, Perplexity: 12.1866\n",
      "Epoch [3/3], Step [25000/41412], Loss: 1.6146, Perplexity: 5.02618\n",
      "Epoch [3/3], Step [25100/41412], Loss: 2.2247, Perplexity: 9.25110\n",
      "Epoch [3/3], Step [25200/41412], Loss: 2.1143, Perplexity: 8.28369\n",
      "Epoch [3/3], Step [25300/41412], Loss: 2.0618, Perplexity: 7.86020\n",
      "Epoch [3/3], Step [25400/41412], Loss: 1.7730, Perplexity: 5.88840\n",
      "Epoch [3/3], Step [25500/41412], Loss: 2.7180, Perplexity: 15.1507\n",
      "Epoch [3/3], Step [25600/41412], Loss: 2.1184, Perplexity: 8.31811\n",
      "Epoch [3/3], Step [25700/41412], Loss: 1.7388, Perplexity: 5.69076\n",
      "Epoch [3/3], Step [25800/41412], Loss: 2.1150, Perplexity: 8.28925\n",
      "Epoch [3/3], Step [25900/41412], Loss: 1.7542, Perplexity: 5.77903\n",
      "Epoch [3/3], Step [26000/41412], Loss: 1.6343, Perplexity: 5.12609\n",
      "Epoch [3/3], Step [26100/41412], Loss: 1.7990, Perplexity: 6.04384\n",
      "Epoch [3/3], Step [26200/41412], Loss: 2.1023, Perplexity: 8.18524\n",
      "Epoch [3/3], Step [26300/41412], Loss: 1.9955, Perplexity: 7.35569\n",
      "Epoch [3/3], Step [26400/41412], Loss: 1.9682, Perplexity: 7.15794\n",
      "Epoch [3/3], Step [26500/41412], Loss: 1.6858, Perplexity: 5.39691\n",
      "Epoch [3/3], Step [26600/41412], Loss: 1.6524, Perplexity: 5.21978\n",
      "Epoch [3/3], Step [26700/41412], Loss: 1.6845, Perplexity: 5.38986\n",
      "Epoch [3/3], Step [26800/41412], Loss: 2.2467, Perplexity: 9.45622\n",
      "Epoch [3/3], Step [26900/41412], Loss: 1.9297, Perplexity: 6.88729\n",
      "Epoch [3/3], Step [27000/41412], Loss: 1.9258, Perplexity: 6.86059\n",
      "Epoch [3/3], Step [27100/41412], Loss: 2.1177, Perplexity: 8.31162\n",
      "Epoch [3/3], Step [27200/41412], Loss: 1.8077, Perplexity: 6.09675\n",
      "Epoch [3/3], Step [27300/41412], Loss: 2.0806, Perplexity: 8.00966\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/3], Step [27400/41412], Loss: 2.1061, Perplexity: 8.21588\n",
      "Epoch [3/3], Step [27500/41412], Loss: 2.5147, Perplexity: 12.36262\n",
      "Epoch [3/3], Step [27600/41412], Loss: 2.2110, Perplexity: 9.12526\n",
      "Epoch [3/3], Step [27700/41412], Loss: 1.9271, Perplexity: 6.86953\n",
      "Epoch [3/3], Step [27800/41412], Loss: 1.7101, Perplexity: 5.52975\n",
      "Epoch [3/3], Step [27900/41412], Loss: 2.1111, Perplexity: 8.25731\n",
      "Epoch [3/3], Step [28000/41412], Loss: 2.0511, Perplexity: 7.77672\n",
      "Epoch [3/3], Step [28100/41412], Loss: 2.4810, Perplexity: 11.9531\n",
      "Epoch [3/3], Step [28200/41412], Loss: 1.7948, Perplexity: 6.01800\n",
      "Epoch [3/3], Step [28300/41412], Loss: 1.9310, Perplexity: 6.89616\n",
      "Epoch [3/3], Step [28400/41412], Loss: 2.6673, Perplexity: 14.4013\n",
      "Epoch [3/3], Step [28500/41412], Loss: 1.7339, Perplexity: 5.66287\n",
      "Epoch [3/3], Step [28600/41412], Loss: 1.7867, Perplexity: 5.97004\n",
      "Epoch [3/3], Step [28700/41412], Loss: 1.8895, Perplexity: 6.61588\n",
      "Epoch [3/3], Step [28800/41412], Loss: 1.6389, Perplexity: 5.14975\n",
      "Epoch [3/3], Step [28900/41412], Loss: 2.0354, Perplexity: 7.65521\n",
      "Epoch [3/3], Step [29000/41412], Loss: 1.7165, Perplexity: 5.56501\n",
      "Epoch [3/3], Step [29100/41412], Loss: 1.8961, Perplexity: 6.65959\n",
      "Epoch [3/3], Step [29200/41412], Loss: 2.4669, Perplexity: 11.7863\n",
      "Epoch [3/3], Step [29300/41412], Loss: 2.1152, Perplexity: 8.29098\n",
      "Epoch [3/3], Step [29400/41412], Loss: 2.0697, Perplexity: 7.92284\n",
      "Epoch [3/3], Step [29500/41412], Loss: 1.7861, Perplexity: 5.96647\n",
      "Epoch [3/3], Step [29600/41412], Loss: 1.8953, Perplexity: 6.65471\n",
      "Epoch [3/3], Step [29700/41412], Loss: 1.8537, Perplexity: 6.38337\n",
      "Epoch [3/3], Step [29800/41412], Loss: 1.3863, Perplexity: 4.00025\n",
      "Epoch [3/3], Step [29900/41412], Loss: 1.3856, Perplexity: 3.99723\n",
      "Epoch [3/3], Step [30000/41412], Loss: 1.9720, Perplexity: 7.18508\n",
      "Epoch [3/3], Step [30100/41412], Loss: 1.3556, Perplexity: 3.87917\n",
      "Epoch [3/3], Step [30200/41412], Loss: 2.1012, Perplexity: 8.17563\n",
      "Epoch [3/3], Step [30300/41412], Loss: 2.1724, Perplexity: 8.77948\n",
      "Epoch [3/3], Step [30400/41412], Loss: 1.7184, Perplexity: 5.57585\n",
      "Epoch [3/3], Step [30500/41412], Loss: 2.0331, Perplexity: 7.63770\n",
      "Epoch [3/3], Step [30600/41412], Loss: 2.5168, Perplexity: 12.3889\n",
      "Epoch [3/3], Step [30700/41412], Loss: 2.1078, Perplexity: 8.23049\n",
      "Epoch [3/3], Step [30800/41412], Loss: 2.0157, Perplexity: 7.50563\n",
      "Epoch [3/3], Step [30900/41412], Loss: 2.0778, Perplexity: 7.98665\n",
      "Epoch [3/3], Step [31000/41412], Loss: 2.0297, Perplexity: 7.61166\n",
      "Epoch [3/3], Step [31100/41412], Loss: 2.0074, Perplexity: 7.44385\n",
      "Epoch [3/3], Step [31200/41412], Loss: 1.8289, Perplexity: 6.22718\n",
      "Epoch [3/3], Step [31300/41412], Loss: 2.3320, Perplexity: 10.2988\n",
      "Epoch [3/3], Step [31400/41412], Loss: 2.0615, Perplexity: 7.858033\n",
      "Epoch [3/3], Step [31500/41412], Loss: 2.2918, Perplexity: 9.89318\n",
      "Epoch [3/3], Step [31600/41412], Loss: 2.4072, Perplexity: 11.1031\n",
      "Epoch [3/3], Step [31700/41412], Loss: 2.1263, Perplexity: 8.38354\n",
      "Epoch [3/3], Step [31800/41412], Loss: 2.2962, Perplexity: 9.93594\n",
      "Epoch [3/3], Step [31900/41412], Loss: 1.8455, Perplexity: 6.33135\n",
      "Epoch [3/3], Step [32000/41412], Loss: 2.0072, Perplexity: 7.442534\n",
      "Epoch [3/3], Step [32100/41412], Loss: 1.5539, Perplexity: 4.72984\n",
      "Epoch [3/3], Step [32200/41412], Loss: 1.9544, Perplexity: 7.05971\n",
      "Epoch [3/3], Step [32300/41412], Loss: 2.0305, Perplexity: 7.61791\n",
      "Epoch [3/3], Step [32400/41412], Loss: 2.2259, Perplexity: 9.26181\n",
      "Epoch [3/3], Step [32500/41412], Loss: 1.8555, Perplexity: 6.39510\n",
      "Epoch [3/3], Step [32600/41412], Loss: 2.1007, Perplexity: 8.17228\n",
      "Epoch [3/3], Step [32700/41412], Loss: 1.9318, Perplexity: 6.90219\n",
      "Epoch [3/3], Step [32800/41412], Loss: 1.8307, Perplexity: 6.23805\n",
      "Epoch [3/3], Step [32900/41412], Loss: 1.6658, Perplexity: 5.28971\n",
      "Epoch [3/3], Step [33000/41412], Loss: 2.0745, Perplexity: 7.96082\n",
      "Epoch [3/3], Step [33100/41412], Loss: 2.0140, Perplexity: 7.49332\n",
      "Epoch [3/3], Step [33200/41412], Loss: 2.2500, Perplexity: 9.48740\n",
      "Epoch [3/3], Step [33300/41412], Loss: 2.4402, Perplexity: 11.4753\n",
      "Epoch [3/3], Step [33400/41412], Loss: 2.3995, Perplexity: 11.0174\n",
      "Epoch [3/3], Step [33500/41412], Loss: 2.1372, Perplexity: 8.47595\n",
      "Epoch [3/3], Step [33600/41412], Loss: 1.9777, Perplexity: 7.22631\n",
      "Epoch [3/3], Step [33700/41412], Loss: 2.1807, Perplexity: 8.85280\n",
      "Epoch [3/3], Step [33800/41412], Loss: 1.9845, Perplexity: 7.27515\n",
      "Epoch [3/3], Step [33900/41412], Loss: 2.1474, Perplexity: 8.56271\n",
      "Epoch [3/3], Step [34000/41412], Loss: 2.8383, Perplexity: 17.0864\n",
      "Epoch [3/3], Step [34100/41412], Loss: 1.7017, Perplexity: 5.48318\n",
      "Epoch [3/3], Step [34200/41412], Loss: 2.6335, Perplexity: 13.9223\n",
      "Epoch [3/3], Step [34300/41412], Loss: 1.9310, Perplexity: 6.89655\n",
      "Epoch [3/3], Step [34400/41412], Loss: 1.7990, Perplexity: 6.04361\n",
      "Epoch [3/3], Step [34500/41412], Loss: 1.6387, Perplexity: 5.14844\n",
      "Epoch [3/3], Step [34600/41412], Loss: 2.0903, Perplexity: 8.08730\n",
      "Epoch [3/3], Step [34700/41412], Loss: 1.6078, Perplexity: 4.99187\n",
      "Epoch [3/3], Step [34800/41412], Loss: 2.2955, Perplexity: 9.92931\n",
      "Epoch [3/3], Step [34900/41412], Loss: 1.9699, Perplexity: 7.17038\n",
      "Epoch [3/3], Step [35000/41412], Loss: 1.9697, Perplexity: 7.16873\n",
      "Epoch [3/3], Step [35100/41412], Loss: 1.9950, Perplexity: 7.35237\n",
      "Epoch [3/3], Step [35200/41412], Loss: 2.7572, Perplexity: 15.7553\n",
      "Epoch [3/3], Step [35300/41412], Loss: 2.4492, Perplexity: 11.5796\n",
      "Epoch [3/3], Step [35400/41412], Loss: 1.9212, Perplexity: 6.82935\n",
      "Epoch [3/3], Step [35500/41412], Loss: 1.6006, Perplexity: 4.95592\n",
      "Epoch [3/3], Step [35600/41412], Loss: 2.0469, Perplexity: 7.74353\n",
      "Epoch [3/3], Step [35700/41412], Loss: 2.2122, Perplexity: 9.13625\n",
      "Epoch [3/3], Step [35800/41412], Loss: 3.3168, Perplexity: 27.5729\n",
      "Epoch [3/3], Step [35900/41412], Loss: 2.0407, Perplexity: 7.69616\n",
      "Epoch [3/3], Step [36000/41412], Loss: 1.7689, Perplexity: 5.86432\n",
      "Epoch [3/3], Step [36100/41412], Loss: 2.2562, Perplexity: 9.54662\n",
      "Epoch [3/3], Step [36200/41412], Loss: 2.5637, Perplexity: 12.9836\n",
      "Epoch [3/3], Step [36300/41412], Loss: 2.2734, Perplexity: 9.71246\n",
      "Epoch [3/3], Step [36400/41412], Loss: 2.4078, Perplexity: 11.1099\n",
      "Epoch [3/3], Step [36500/41412], Loss: 1.8629, Perplexity: 6.44255\n",
      "Epoch [3/3], Step [36600/41412], Loss: 1.7825, Perplexity: 5.94495\n",
      "Epoch [3/3], Step [36700/41412], Loss: 1.8328, Perplexity: 6.25151\n",
      "Epoch [3/3], Step [36800/41412], Loss: 2.2006, Perplexity: 9.03056\n",
      "Epoch [3/3], Step [36900/41412], Loss: 2.2444, Perplexity: 9.43513\n",
      "Epoch [3/3], Step [37000/41412], Loss: 2.2566, Perplexity: 9.55059\n",
      "Epoch [3/3], Step [37100/41412], Loss: 2.2418, Perplexity: 9.41065\n",
      "Epoch [3/3], Step [37200/41412], Loss: 2.1834, Perplexity: 8.87686\n",
      "Epoch [3/3], Step [37300/41412], Loss: 1.8835, Perplexity: 6.57638\n",
      "Epoch [3/3], Step [37400/41412], Loss: 1.6570, Perplexity: 5.24357\n",
      "Epoch [3/3], Step [37500/41412], Loss: 2.4288, Perplexity: 11.3450\n",
      "Epoch [3/3], Step [37600/41412], Loss: 2.5226, Perplexity: 12.4604\n",
      "Epoch [3/3], Step [37700/41412], Loss: 1.8797, Perplexity: 6.55123\n",
      "Epoch [3/3], Step [37800/41412], Loss: 1.9623, Perplexity: 7.11582\n",
      "Epoch [3/3], Step [37900/41412], Loss: 2.6355, Perplexity: 13.9499\n",
      "Epoch [3/3], Step [38000/41412], Loss: 1.8157, Perplexity: 6.14516\n",
      "Epoch [3/3], Step [38100/41412], Loss: 2.0158, Perplexity: 7.50669\n",
      "Epoch [3/3], Step [38200/41412], Loss: 1.9194, Perplexity: 6.81680\n",
      "Epoch [3/3], Step [38300/41412], Loss: 2.0419, Perplexity: 7.70499\n",
      "Epoch [3/3], Step [38400/41412], Loss: 2.0709, Perplexity: 7.93160\n",
      "Epoch [3/3], Step [38500/41412], Loss: 1.8381, Perplexity: 6.28450\n",
      "Epoch [3/3], Step [38600/41412], Loss: 2.6378, Perplexity: 13.9820\n",
      "Epoch [3/3], Step [38700/41412], Loss: 2.5178, Perplexity: 12.4008\n",
      "Epoch [3/3], Step [38800/41412], Loss: 1.8470, Perplexity: 6.34070\n",
      "Epoch [3/3], Step [38900/41412], Loss: 1.8673, Perplexity: 6.47055\n",
      "Epoch [3/3], Step [39000/41412], Loss: 2.3983, Perplexity: 11.0046\n",
      "Epoch [3/3], Step [39100/41412], Loss: 2.0826, Perplexity: 8.02544\n",
      "Epoch [3/3], Step [39200/41412], Loss: 1.8157, Perplexity: 6.14550\n",
      "Epoch [3/3], Step [39300/41412], Loss: 1.5763, Perplexity: 4.83690\n",
      "Epoch [3/3], Step [39400/41412], Loss: 1.8967, Perplexity: 6.66406\n",
      "Epoch [3/3], Step [39500/41412], Loss: 2.2700, Perplexity: 9.67970\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/3], Step [39600/41412], Loss: 2.0859, Perplexity: 8.05151\n",
      "Epoch [3/3], Step [39700/41412], Loss: 2.4824, Perplexity: 11.9695\n",
      "Epoch [3/3], Step [39800/41412], Loss: 2.1294, Perplexity: 8.40952\n",
      "Epoch [3/3], Step [39900/41412], Loss: 2.5433, Perplexity: 12.7221\n",
      "Epoch [3/3], Step [40000/41412], Loss: 2.2637, Perplexity: 9.61874\n",
      "Epoch [3/3], Step [40100/41412], Loss: 2.1867, Perplexity: 8.90568\n",
      "Epoch [3/3], Step [40200/41412], Loss: 2.0229, Perplexity: 7.56040\n",
      "Epoch [3/3], Step [40300/41412], Loss: 1.9445, Perplexity: 6.99012\n",
      "Epoch [3/3], Step [40400/41412], Loss: 2.4210, Perplexity: 11.2576\n",
      "Epoch [3/3], Step [40500/41412], Loss: 2.1203, Perplexity: 8.33396\n",
      "Epoch [3/3], Step [40600/41412], Loss: 1.8267, Perplexity: 6.21358\n",
      "Epoch [3/3], Step [40700/41412], Loss: 2.6225, Perplexity: 13.7695\n",
      "Epoch [3/3], Step [40800/41412], Loss: 2.1856, Perplexity: 8.89560\n",
      "Epoch [3/3], Step [40900/41412], Loss: 1.7097, Perplexity: 5.52741\n",
      "Epoch [3/3], Step [41000/41412], Loss: 1.9982, Perplexity: 7.37583\n",
      "Epoch [3/3], Step [41100/41412], Loss: 2.1666, Perplexity: 8.72875\n",
      "Epoch [3/3], Step [41200/41412], Loss: 1.7692, Perplexity: 5.86613\n",
      "Epoch [3/3], Step [41300/41412], Loss: 1.6325, Perplexity: 5.11680\n",
      "Epoch [3/3], Step [41400/41412], Loss: 2.3832, Perplexity: 10.8399\n",
      "Epoch [3/3], Step [41412/41412], Loss: 2.2182, Perplexity: 9.19061"
     ]
    }
   ],
   "source": [
    "import torch.utils.data as data\n",
    "import numpy as np\n",
    "import os\n",
    "import requests\n",
    "import time\n",
    "\n",
    "# temporary\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from model import EncoderCNN, DecoderRNN\n",
    "\n",
    "# Open the training log file.\n",
    "f = open(log_file, 'w')\n",
    "\n",
    "## running the training locally\n",
    "#old_time = time.time()\n",
    "#response = requests.request(\"GET\", \n",
    "#                            \"http://metadata.google.internal/computeMetadata/v1/instance/attributes/keep_alive_token\", \n",
    "#                            headers={\"Metadata-Flavor\":\"Google\"})\n",
    "\n",
    "\n",
    "for epoch in range(1, num_epochs+1):\n",
    "\n",
    "    for i_step in range(1, total_step+1):\n",
    "        \n",
    "        ## running the training locally\n",
    "        #if time.time() - old_time > 60:\n",
    "        #    old_time = time.time()\n",
    "        #    requests.request(\"POST\", \n",
    "        #                     \"https://nebula.udacity.com/api/v1/remote/keep-alive\", \n",
    "        #                     headers={'Authorization': \"STAR \" + response.text})\n",
    "\n",
    "        # Randomly sample a caption length, and sample indices with that length.\n",
    "        indices = data_loader.dataset.get_train_indices()\n",
    "        # Create and assign a batch sampler to retrieve a batch with the sampled indices.\n",
    "        new_sampler = data.sampler.SubsetRandomSampler(indices=indices)\n",
    "        data_loader.batch_sampler.sampler = new_sampler\n",
    "\n",
    "        # Obtain the batch.\n",
    "        images, captions = next(iter(data_loader))\n",
    "\n",
    "        # Move batch of images and captions to GPU if CUDA is available.\n",
    "        images = images.to(device)\n",
    "        captions = captions.to(device)\n",
    "\n",
    "        # Zero the gradients.\n",
    "        decoder.zero_grad()\n",
    "        encoder.zero_grad()\n",
    "\n",
    "        # Pass the inputs through the CNN-RNN model.\n",
    "        features = encoder(images)\n",
    "        outputs = decoder(features, captions)\n",
    "\n",
    "        # Calculate the batch loss.\n",
    "        loss = criterion(outputs.contiguous().view(-1, vocab_size), captions.contiguous().view(-1))\n",
    "\n",
    "        # Backward pass.\n",
    "        loss.backward(retain_graph=True)\n",
    "\n",
    "        # Update the parameters in the optimizer.\n",
    "        optimizer.step()\n",
    "\n",
    "        # Get training statistics.\n",
    "        stats = 'Epoch [%d/%d], Step [%d/%d], Loss: %.4f, Perplexity: %5.4f' % (epoch, num_epochs, i_step, total_step, loss.item(), np.exp(loss.item()))\n",
    "\n",
    "        # Print training statistics (on same line).\n",
    "        print('\\r' + stats, end=\"\")\n",
    "        sys.stdout.flush()\n",
    "\n",
    "        # Print training statistics to file.\n",
    "        f.write(stats + '\\n')\n",
    "        f.flush()\n",
    "\n",
    "        # Print training statistics (on different line).\n",
    "        if i_step % print_every == 0:\n",
    "            print('\\r' + stats)\n",
    "\n",
    "    # Save the weights.\n",
    "    if epoch % save_every == 0:\n",
    "        torch.save(decoder.state_dict(), os.path.join('./models', 'decoder-%d.pkl' % epoch))\n",
    "        torch.save(encoder.state_dict(), os.path.join('./models', 'encoder-%d.pkl' % epoch))\n",
    "\n",
    "# Close the training log file.\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step3'></a>\n",
    "## Step 3: (Optional) Validate your Model\n",
    "\n",
    "To assess potential overfitting, one approach is to assess performance on a validation set.  If you decide to do this **optional** task, you are required to first complete all of the steps in the next notebook in the sequence (**3_Inference.ipynb**); as part of that notebook, you will write and test code (specifically, the `sample` method in the `DecoderRNN` class) that uses your RNN decoder to generate captions.  That code will prove incredibly useful here. \n",
    "\n",
    "If you decide to validate your model, please do not edit the data loader in **data_loader.py**.  Instead, create a new file named **data_loader_val.py** containing the code for obtaining the data loader for the validation data.  You can access:\n",
    "- the validation images at filepath `'/opt/cocoapi/images/train2014/'`, and\n",
    "- the validation image caption annotation file at filepath `'/opt/cocoapi/annotations/captions_val2014.json'`.\n",
    "\n",
    "The suggested approach to validating your model involves creating a json file such as [this one](https://github.com/cocodataset/cocoapi/blob/master/results/captions_val2014_fakecap_results.json) containing your model's predicted captions for the validation images.  Then, you can write your own script or use one that you [find online](https://github.com/tylin/coco-caption) to calculate the BLEU score of your model.  You can read more about the BLEU score, along with other evaluation metrics (such as TEOR and Cider) in section 4.1 of [this paper](https://arxiv.org/pdf/1411.4555.pdf).  For more information about how to use the annotation file, check out the [website](http://cocodataset.org/#download) for the COCO dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Optional) TODO: Validate your model."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
