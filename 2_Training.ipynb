{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computer Vision Nanodegree\n",
    "\n",
    "## Project: Image Captioning\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, you will train your CNN-RNN model.  \n",
    "\n",
    "You are welcome and encouraged to try out many different architectures and hyperparameters when searching for a good model.\n",
    "\n",
    "This does have the potential to make the project quite messy!  Before submitting your project, make sure that you clean up:\n",
    "- the code you write in this notebook.  The notebook should describe how to train a single CNN-RNN architecture, corresponding to your final choice of hyperparameters.  You should structure the notebook so that the reviewer can replicate your results by running the code in this notebook.  \n",
    "- the output of the code cell in **Step 2**.  The output should show the output obtained when training the model from scratch.\n",
    "\n",
    "This notebook **will be graded**.  \n",
    "\n",
    "Feel free to use the links below to navigate the notebook:\n",
    "- [Step 1](#step1): Training Setup\n",
    "- [Step 2](#step2): Train your Model\n",
    "- [Step 3](#step3): (Optional) Validate your Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step1'></a>\n",
    "## Step 1: Training Setup\n",
    "\n",
    "In this step of the notebook, you will customize the training of your CNN-RNN model by specifying hyperparameters and setting other options that are important to the training procedure.  The values you set now will be used when training your model in **Step 2** below.\n",
    "\n",
    "You should only amend blocks of code that are preceded by a `TODO` statement.  **Any code blocks that are not preceded by a `TODO` statement should not be modified**.\n",
    "\n",
    "### Task #1\n",
    "\n",
    "Begin by setting the following variables:\n",
    "- `batch_size` - the batch size of each training batch.  It is the number of image-caption pairs used to amend the model weights in each training step. \n",
    "- `vocab_threshold` - the minimum word count threshold.  Note that a larger threshold will result in a smaller vocabulary, whereas a smaller threshold will include rarer words and result in a larger vocabulary.  \n",
    "- `vocab_from_file` - a Boolean that decides whether to load the vocabulary from file. \n",
    "- `embed_size` - the dimensionality of the image and word embeddings.  \n",
    "- `hidden_size` - the number of features in the hidden state of the RNN decoder.  \n",
    "- `num_epochs` - the number of epochs to train the model.  We recommend that you set `num_epochs=3`, but feel free to increase or decrease this number as you wish.  [This paper](https://arxiv.org/pdf/1502.03044.pdf) trained a captioning model on a single state-of-the-art GPU for 3 days, but you'll soon see that you can get reasonable results in a matter of a few hours!  (_But of course, if you want your model to compete with current research, you will have to train for much longer._)\n",
    "- `save_every` - determines how often to save the model weights.  We recommend that you set `save_every=1`, to save the model weights after each epoch.  This way, after the `i`th epoch, the encoder and decoder weights will be saved in the `models/` folder as `encoder-i.pkl` and `decoder-i.pkl`, respectively.\n",
    "- `print_every` - determines how often to print the batch loss to the Jupyter notebook while training.  Note that you **will not** observe a monotonic decrease in the loss function while training - this is perfectly fine and completely expected!  You are encouraged to keep this at its default value of `100` to avoid clogging the notebook, but feel free to change it.\n",
    "- `log_file` - the name of the text file containing - for every step - how the loss and perplexity evolved during training.\n",
    "\n",
    "If you're not sure where to begin to set some of the values above, you can peruse [this paper](https://arxiv.org/pdf/1502.03044.pdf) and [this paper](https://arxiv.org/pdf/1411.4555.pdf) for useful guidance!  **To avoid spending too long on this notebook**, you are encouraged to consult these suggested research papers to obtain a strong initial guess for which hyperparameters are likely to work best.  Then, train a single model, and proceed to the next notebook (**3_Inference.ipynb**).  If you are unhappy with your performance, you can return to this notebook to tweak the hyperparameters (and/or the architecture in **model.py**) and re-train your model.\n",
    "\n",
    "### Question 1\n",
    "\n",
    "**Question:** Describe your CNN-RNN architecture in detail.  With this architecture in mind, how did you select the values of the variables in Task 1?  If you consulted a research paper detailing a successful implementation of an image captioning model, please provide the reference.\n",
    "\n",
    "**Answer:** \n",
    "\n",
    "\n",
    "### (Optional) Task #2\n",
    "\n",
    "Note that we have provided a recommended image transform `transform_train` for pre-processing the training images, but you are welcome (and encouraged!) to modify it as you wish.  When modifying this transform, keep in mind that:\n",
    "- the images in the dataset have varying heights and widths, and \n",
    "- if using a pre-trained model, you must perform the corresponding appropriate normalization.\n",
    "\n",
    "### Question 2\n",
    "\n",
    "**Question:** How did you select the transform in `transform_train`?  If you left the transform at its provided value, why do you think that it is a good choice for your CNN architecture?\n",
    "\n",
    "**Answer:** The default looks reasonable, although I have some doubts as to whether it makes sense to discard the data around the edge in this case. \n",
    "\n",
    "### Task #3\n",
    "\n",
    "Next, you will specify a Python list containing the learnable parameters of the model.  For instance, if you decide to make all weights in the decoder trainable, but only want to train the weights in the embedding layer of the encoder, then you should set `params` to something like:\n",
    "```\n",
    "params = list(decoder.parameters()) + list(encoder.embed.parameters()) \n",
    "```\n",
    "\n",
    "### Question 3\n",
    "\n",
    "**Question:** How did you select the trainable parameters of your architecture?  Why do you think this is a good choice?\n",
    "\n",
    "**Answer:** \n",
    "\n",
    "### Task #4\n",
    "\n",
    "Finally, you will select an [optimizer](http://pytorch.org/docs/master/optim.html#torch.optim.Optimizer).\n",
    "\n",
    "### Question 4\n",
    "\n",
    "**Question:** How did you select the optimizer used to train your model?\n",
    "\n",
    "**Answer:** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/sthenc/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary successfully loaded from vocab.pkl file!\n",
      "loading annotations into memory...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1686/414113 [00:00<00:49, 8344.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done (t=0.55s)\n",
      "creating index...\n",
      "index created!\n",
      "Obtaining caption lengths...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 414113/414113 [00:43<00:00, 9522.13it/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "import sys\n",
    "sys.path.append('/opt/cocoapi/PythonAPI')\n",
    "from pycocotools.coco import COCO\n",
    "from data_loader import get_loader\n",
    "from model import EncoderCNN, DecoderRNN\n",
    "import math\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "## TODO #1: Select appropriate values for the Python variables below.\n",
    "batch_size=10\n",
    "#batch_size = 64          # batch size, we have 10GB of GPU memory, let's use it\n",
    "vocab_threshold = 7        # minimum word count threshold\n",
    "vocab_from_file = True    # if True, load existing vocab file\n",
    "embed_size = 512           # dimensionality of image and word embeddings\n",
    "hidden_size = 512          # number of features in hidden state of the RNN decoder\n",
    "num_epochs = 3             # number of training epochs\n",
    "save_every = 1             # determines frequency of saving model weights\n",
    "print_every = 100          # determines window for printing average loss\n",
    "log_file = 'training_log.txt'       # name of file with saved training loss and perplexity\n",
    "\n",
    "# (Optional) TODO #2: Amend the image transform below.\n",
    "transform_train = transforms.Compose([ \n",
    "    transforms.Resize(256),                          # smaller edge of image resized to 256\n",
    "    transforms.RandomCrop(224),                      # get 224x224 crop from random location\n",
    "    transforms.RandomHorizontalFlip(),               # horizontally flip image with probability=0.5\n",
    "    transforms.ToTensor(),                           # convert the PIL Image to a tensor\n",
    "    transforms.Normalize((0.485, 0.456, 0.406),      # normalize image for pre-trained model\n",
    "                         (0.229, 0.224, 0.225))])\n",
    "\n",
    "# Build data loader.\n",
    "data_loader = get_loader(transform=transform_train,\n",
    "                         mode='train',\n",
    "                         batch_size=batch_size,\n",
    "                         vocab_threshold=vocab_threshold,\n",
    "                         vocab_from_file=vocab_from_file)\n",
    "\n",
    "# The size of the vocabulary.\n",
    "vocab_size = len(data_loader.dataset.vocab)\n",
    "\n",
    "# Initialize the encoder and decoder. \n",
    "encoder = EncoderCNN(embed_size)\n",
    "decoder = DecoderRNN(embed_size, hidden_size, vocab_size, max_batch_size=batch_size)\n",
    "\n",
    "# Move models to GPU if CUDA is available. \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "encoder.to(device)\n",
    "decoder.to(device)\n",
    "\n",
    "# Define the loss function. \n",
    "criterion = nn.CrossEntropyLoss().cuda() if torch.cuda.is_available() else nn.CrossEntropyLoss()\n",
    "\n",
    "# TODO #3: Specify the learnable parameters of the model.\n",
    "params = list(decoder.parameters()) + list(encoder.embed.parameters()) + list(encoder.bn.parameters())\n",
    "\n",
    "# TODO #4: Define the optimizer.\n",
    "#optimizer = torch.optim.SGD(params, lr=0.01)\n",
    "\n",
    "# this data is probably pretty sparse, and defaults are probably ok\n",
    "#http://ruder.io/optimizing-gradient-descent/\n",
    "optimizer = torch.optim.Adam(params, lr=0.001)\n",
    "\n",
    "# Set the total number of training steps per epoch.\n",
    "total_step = math.ceil(len(data_loader.dataset.caption_lengths) / data_loader.batch_sampler.batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step2'></a>\n",
    "## Step 2: Train your Model\n",
    "\n",
    "Once you have executed the code cell in **Step 1**, the training procedure below should run without issue.  \n",
    "\n",
    "It is completely fine to leave the code cell below as-is without modifications to train your model.  However, if you would like to modify the code used to train the model below, you must ensure that your changes are easily parsed by your reviewer.  In other words, make sure to provide appropriate comments to describe how your code works!  \n",
    "\n",
    "You may find it useful to load saved weights to resume training.  In that case, note the names of the files containing the encoder and decoder weights that you'd like to load (`encoder_file` and `decoder_file`).  Then you can load the weights by using the lines below:\n",
    "\n",
    "```python\n",
    "# Load pre-trained weights before resuming training.\n",
    "encoder.load_state_dict(torch.load(os.path.join('./models', encoder_file)))\n",
    "decoder.load_state_dict(torch.load(os.path.join('./models', decoder_file)))\n",
    "```\n",
    "\n",
    "While trying out parameters, make sure to take extensive notes and record the settings that you used in your various training runs.  In particular, you don't want to encounter a situation where you've trained a model for several hours but can't remember what settings you used :).\n",
    "\n",
    "### A Note on Tuning Hyperparameters\n",
    "\n",
    "To figure out how well your model is doing, you can look at how the training loss and perplexity evolve during training - and for the purposes of this project, you are encouraged to amend the hyperparameters based on this information.  \n",
    "\n",
    "However, this will not tell you if your model is overfitting to the training data, and, unfortunately, overfitting is a problem that is commonly encountered when training image captioning models.  \n",
    "\n",
    "For this project, you need not worry about overfitting. **This project does not have strict requirements regarding the performance of your model**, and you just need to demonstrate that your model has learned **_something_** when you generate captions on the test data.  For now, we strongly encourage you to train your model for the suggested 3 epochs without worrying about performance; then, you should immediately transition to the next notebook in the sequence (**3_Inference.ipynb**) to see how your model performs on the test data.  If your model needs to be changed, you can come back to this notebook, amend hyperparameters (if necessary), and re-train the model.\n",
    "\n",
    "That said, if you would like to go above and beyond in this project, you can read about some approaches to minimizing overfitting in section 4.3.1 of [this paper](http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7505636).  In the next (optional) step of this notebook, we provide some guidance for assessing the performance on the validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/3], Step [100/41412], Loss: 4.5572, Perplexity: 95.3167\n",
      "Epoch [1/3], Step [200/41412], Loss: 4.3654, Perplexity: 78.68137\n",
      "Epoch [1/3], Step [300/41412], Loss: 3.7437, Perplexity: 42.25210\n",
      "Epoch [1/3], Step [400/41412], Loss: 3.1819, Perplexity: 24.09146\n",
      "Epoch [1/3], Step [500/41412], Loss: 3.6090, Perplexity: 36.92961\n",
      "Epoch [1/3], Step [600/41412], Loss: 3.5878, Perplexity: 36.1558\n",
      "Epoch [1/3], Step [700/41412], Loss: 2.8022, Perplexity: 16.4810\n",
      "Epoch [1/3], Step [800/41412], Loss: 3.2283, Perplexity: 25.2379\n",
      "Epoch [1/3], Step [900/41412], Loss: 3.0680, Perplexity: 21.4995\n",
      "Epoch [1/3], Step [1000/41412], Loss: 3.9100, Perplexity: 49.8969\n",
      "Epoch [1/3], Step [1100/41412], Loss: 3.9502, Perplexity: 51.9479\n",
      "Epoch [1/3], Step [1200/41412], Loss: 3.1403, Perplexity: 23.1115\n",
      "Epoch [1/3], Step [1300/41412], Loss: 2.7303, Perplexity: 15.3375\n",
      "Epoch [1/3], Step [1400/41412], Loss: 2.8546, Perplexity: 17.36786\n",
      "Epoch [1/3], Step [1500/41412], Loss: 3.5363, Perplexity: 34.3406\n",
      "Epoch [1/3], Step [1600/41412], Loss: 2.3952, Perplexity: 10.9708\n",
      "Epoch [1/3], Step [1700/41412], Loss: 2.6677, Perplexity: 14.4074\n",
      "Epoch [1/3], Step [1800/41412], Loss: 2.0789, Perplexity: 7.99548\n",
      "Epoch [1/3], Step [1900/41412], Loss: 3.4319, Perplexity: 30.93510\n",
      "Epoch [1/3], Step [2000/41412], Loss: 2.7709, Perplexity: 15.9735\n",
      "Epoch [1/3], Step [2100/41412], Loss: 3.0187, Perplexity: 20.4646\n",
      "Epoch [1/3], Step [2200/41412], Loss: 3.3694, Perplexity: 29.06057\n",
      "Epoch [1/3], Step [2300/41412], Loss: 3.0875, Perplexity: 21.9219\n",
      "Epoch [1/3], Step [2400/41412], Loss: 3.0359, Perplexity: 20.8189\n",
      "Epoch [1/3], Step [2500/41412], Loss: 3.0861, Perplexity: 21.8916\n",
      "Epoch [1/3], Step [2600/41412], Loss: 3.1068, Perplexity: 22.3488\n",
      "Epoch [1/3], Step [2700/41412], Loss: 2.6762, Perplexity: 14.5301\n",
      "Epoch [1/3], Step [2800/41412], Loss: 3.1937, Perplexity: 24.3796\n",
      "Epoch [1/3], Step [2900/41412], Loss: 3.3246, Perplexity: 27.7892\n",
      "Epoch [1/3], Step [3000/41412], Loss: 2.5325, Perplexity: 12.5843\n",
      "Epoch [1/3], Step [3100/41412], Loss: 3.4491, Perplexity: 31.4722\n",
      "Epoch [1/3], Step [3200/41412], Loss: 2.6799, Perplexity: 14.5838\n",
      "Epoch [1/3], Step [3300/41412], Loss: 2.1939, Perplexity: 8.97019\n",
      "Epoch [1/3], Step [3400/41412], Loss: 2.6335, Perplexity: 13.9230\n",
      "Epoch [1/3], Step [3500/41412], Loss: 2.2733, Perplexity: 9.71117\n",
      "Epoch [1/3], Step [3600/41412], Loss: 2.2764, Perplexity: 9.741552\n",
      "Epoch [1/3], Step [3700/41412], Loss: 3.3641, Perplexity: 28.9065\n",
      "Epoch [1/3], Step [3800/41412], Loss: 2.3763, Perplexity: 10.7647\n",
      "Epoch [1/3], Step [3900/41412], Loss: 2.7309, Perplexity: 15.3468\n",
      "Epoch [1/3], Step [4000/41412], Loss: 2.5013, Perplexity: 12.19799\n",
      "Epoch [1/3], Step [4100/41412], Loss: 2.4023, Perplexity: 11.0489\n",
      "Epoch [1/3], Step [4200/41412], Loss: 3.0757, Perplexity: 21.6659\n",
      "Epoch [1/3], Step [4300/41412], Loss: 2.8712, Perplexity: 17.6587\n",
      "Epoch [1/3], Step [4400/41412], Loss: 2.6533, Perplexity: 14.2010\n",
      "Epoch [1/3], Step [4500/41412], Loss: 2.6880, Perplexity: 14.7024\n",
      "Epoch [1/3], Step [4600/41412], Loss: 3.1102, Perplexity: 22.4262\n",
      "Epoch [1/3], Step [4700/41412], Loss: 2.1720, Perplexity: 8.77568\n",
      "Epoch [1/3], Step [4800/41412], Loss: 3.1432, Perplexity: 23.1783\n",
      "Epoch [1/3], Step [4900/41412], Loss: 2.7441, Perplexity: 15.5509\n",
      "Epoch [1/3], Step [5000/41412], Loss: 1.9466, Perplexity: 7.00516\n",
      "Epoch [1/3], Step [5100/41412], Loss: 2.3984, Perplexity: 11.0053\n",
      "Epoch [1/3], Step [5200/41412], Loss: 2.3701, Perplexity: 10.6983\n",
      "Epoch [1/3], Step [5300/41412], Loss: 2.5851, Perplexity: 13.2649\n",
      "Epoch [1/3], Step [5400/41412], Loss: 2.6305, Perplexity: 13.8813\n",
      "Epoch [1/3], Step [5500/41412], Loss: 2.5037, Perplexity: 12.2273\n",
      "Epoch [1/3], Step [5600/41412], Loss: 2.2044, Perplexity: 9.06513\n",
      "Epoch [1/3], Step [5700/41412], Loss: 2.7075, Perplexity: 14.9916\n",
      "Epoch [1/3], Step [5800/41412], Loss: 2.3821, Perplexity: 10.8278\n",
      "Epoch [1/3], Step [5900/41412], Loss: 2.3902, Perplexity: 10.9160\n",
      "Epoch [1/3], Step [6000/41412], Loss: 2.4435, Perplexity: 11.5133\n",
      "Epoch [1/3], Step [6100/41412], Loss: 2.1420, Perplexity: 8.51677\n",
      "Epoch [1/3], Step [6200/41412], Loss: 2.3998, Perplexity: 11.0205\n",
      "Epoch [1/3], Step [6300/41412], Loss: 2.4342, Perplexity: 11.40708\n",
      "Epoch [1/3], Step [6400/41412], Loss: 3.0138, Perplexity: 20.3643\n",
      "Epoch [1/3], Step [6500/41412], Loss: 2.2416, Perplexity: 9.40832\n",
      "Epoch [1/3], Step [6600/41412], Loss: 2.5573, Perplexity: 12.9014\n",
      "Epoch [1/3], Step [6700/41412], Loss: 2.6700, Perplexity: 14.4403\n",
      "Epoch [1/3], Step [6800/41412], Loss: 2.7585, Perplexity: 15.77689\n",
      "Epoch [1/3], Step [6900/41412], Loss: 2.4956, Perplexity: 12.1290\n",
      "Epoch [1/3], Step [7000/41412], Loss: 2.3385, Perplexity: 10.3657\n",
      "Epoch [1/3], Step [7100/41412], Loss: 2.6441, Perplexity: 14.0708\n",
      "Epoch [1/3], Step [7200/41412], Loss: 2.3536, Perplexity: 10.5236\n",
      "Epoch [1/3], Step [7300/41412], Loss: 2.6625, Perplexity: 14.3315\n",
      "Epoch [1/3], Step [7400/41412], Loss: 2.2774, Perplexity: 9.75114\n",
      "Epoch [1/3], Step [7500/41412], Loss: 1.9543, Perplexity: 7.05872\n",
      "Epoch [1/3], Step [7600/41412], Loss: 2.6585, Perplexity: 14.2748\n",
      "Epoch [1/3], Step [7700/41412], Loss: 2.0480, Perplexity: 7.75254\n",
      "Epoch [1/3], Step [7800/41412], Loss: 2.5852, Perplexity: 13.2657\n",
      "Epoch [1/3], Step [7900/41412], Loss: 2.7253, Perplexity: 15.2605\n",
      "Epoch [1/3], Step [8000/41412], Loss: 2.6999, Perplexity: 14.87835\n",
      "Epoch [1/3], Step [8100/41412], Loss: 2.7119, Perplexity: 15.05776\n",
      "Epoch [1/3], Step [8200/41412], Loss: 2.8912, Perplexity: 18.0154\n",
      "Epoch [1/3], Step [8300/41412], Loss: 2.3517, Perplexity: 10.5032\n",
      "Epoch [1/3], Step [8400/41412], Loss: 2.5753, Perplexity: 13.1347\n",
      "Epoch [1/3], Step [8500/41412], Loss: 2.4552, Perplexity: 11.6482\n",
      "Epoch [1/3], Step [8600/41412], Loss: 2.5981, Perplexity: 13.4382\n",
      "Epoch [1/3], Step [8700/41412], Loss: 2.6342, Perplexity: 13.9328\n",
      "Epoch [1/3], Step [8800/41412], Loss: 2.4201, Perplexity: 11.2467\n",
      "Epoch [1/3], Step [8900/41412], Loss: 2.4552, Perplexity: 11.6487\n",
      "Epoch [1/3], Step [9000/41412], Loss: 2.4166, Perplexity: 11.2081\n",
      "Epoch [1/3], Step [9100/41412], Loss: 2.7354, Perplexity: 15.4159\n",
      "Epoch [1/3], Step [9200/41412], Loss: 2.4636, Perplexity: 11.7465\n",
      "Epoch [1/3], Step [9300/41412], Loss: 2.0186, Perplexity: 7.52792\n",
      "Epoch [1/3], Step [9400/41412], Loss: 2.5950, Perplexity: 13.3963\n",
      "Epoch [1/3], Step [9500/41412], Loss: 2.2010, Perplexity: 9.03371\n",
      "Epoch [1/3], Step [9600/41412], Loss: 3.6519, Perplexity: 38.5490\n",
      "Epoch [1/3], Step [9700/41412], Loss: 2.8779, Perplexity: 17.7772\n",
      "Epoch [1/3], Step [9800/41412], Loss: 1.9072, Perplexity: 6.73425\n",
      "Epoch [1/3], Step [9900/41412], Loss: 2.8092, Perplexity: 16.5970\n",
      "Epoch [1/3], Step [10000/41412], Loss: 4.5118, Perplexity: 91.0857\n",
      "Epoch [1/3], Step [10100/41412], Loss: 2.9676, Perplexity: 19.4444\n",
      "Epoch [1/3], Step [10200/41412], Loss: 2.0656, Perplexity: 7.89021\n",
      "Epoch [1/3], Step [10300/41412], Loss: 2.9937, Perplexity: 19.9588\n",
      "Epoch [1/3], Step [10400/41412], Loss: 2.3583, Perplexity: 10.5730\n",
      "Epoch [1/3], Step [10500/41412], Loss: 3.1091, Perplexity: 22.4010\n",
      "Epoch [1/3], Step [10600/41412], Loss: 2.4451, Perplexity: 11.5321\n",
      "Epoch [1/3], Step [10700/41412], Loss: 2.2731, Perplexity: 9.70983\n",
      "Epoch [1/3], Step [10800/41412], Loss: 2.5183, Perplexity: 12.4074\n",
      "Epoch [1/3], Step [10900/41412], Loss: 2.3446, Perplexity: 10.4289\n",
      "Epoch [1/3], Step [11000/41412], Loss: 2.4688, Perplexity: 11.8077\n",
      "Epoch [1/3], Step [11100/41412], Loss: 2.6973, Perplexity: 14.8394\n",
      "Epoch [1/3], Step [11200/41412], Loss: 2.5706, Perplexity: 13.0740\n",
      "Epoch [1/3], Step [11300/41412], Loss: 2.2732, Perplexity: 9.71002\n",
      "Epoch [1/3], Step [11400/41412], Loss: 2.1995, Perplexity: 9.02053\n",
      "Epoch [1/3], Step [11500/41412], Loss: 2.1594, Perplexity: 8.66565\n",
      "Epoch [1/3], Step [11600/41412], Loss: 2.4696, Perplexity: 11.8172\n",
      "Epoch [1/3], Step [11700/41412], Loss: 2.5686, Perplexity: 13.0476\n",
      "Epoch [1/3], Step [11800/41412], Loss: 2.2200, Perplexity: 9.20769\n",
      "Epoch [1/3], Step [11900/41412], Loss: 1.9913, Perplexity: 7.32507\n",
      "Epoch [1/3], Step [12000/41412], Loss: 2.3217, Perplexity: 10.1935\n",
      "Epoch [1/3], Step [12100/41412], Loss: 2.4510, Perplexity: 11.6002\n",
      "Epoch [1/3], Step [12200/41412], Loss: 2.5865, Perplexity: 13.2836\n",
      "Epoch [1/3], Step [12300/41412], Loss: 2.4469, Perplexity: 11.5520\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/3], Step [12400/41412], Loss: 2.5429, Perplexity: 12.7165\n",
      "Epoch [1/3], Step [12500/41412], Loss: 1.9587, Perplexity: 7.09004\n",
      "Epoch [1/3], Step [12600/41412], Loss: 3.4181, Perplexity: 30.5114\n",
      "Epoch [1/3], Step [12700/41412], Loss: 2.3022, Perplexity: 9.99615\n",
      "Epoch [1/3], Step [12800/41412], Loss: 2.6740, Perplexity: 14.4978\n",
      "Epoch [1/3], Step [12900/41412], Loss: 2.7376, Perplexity: 15.4495\n",
      "Epoch [1/3], Step [13000/41412], Loss: 2.3848, Perplexity: 10.8574\n",
      "Epoch [1/3], Step [13100/41412], Loss: 2.1173, Perplexity: 8.30905\n",
      "Epoch [1/3], Step [13200/41412], Loss: 2.4443, Perplexity: 11.5225\n",
      "Epoch [1/3], Step [13300/41412], Loss: 2.9367, Perplexity: 18.8528\n",
      "Epoch [1/3], Step [13400/41412], Loss: 2.0150, Perplexity: 7.50103\n",
      "Epoch [1/3], Step [13500/41412], Loss: 2.5569, Perplexity: 12.8961\n",
      "Epoch [1/3], Step [13600/41412], Loss: 2.0946, Perplexity: 8.12216\n",
      "Epoch [1/3], Step [13700/41412], Loss: 2.3521, Perplexity: 10.5077\n",
      "Epoch [1/3], Step [13800/41412], Loss: 2.2810, Perplexity: 9.78623\n",
      "Epoch [1/3], Step [13900/41412], Loss: 2.0476, Perplexity: 7.74924\n",
      "Epoch [1/3], Step [14000/41412], Loss: 2.1696, Perplexity: 8.75443\n",
      "Epoch [1/3], Step [14100/41412], Loss: 2.0778, Perplexity: 7.98666\n",
      "Epoch [1/3], Step [14200/41412], Loss: 2.2341, Perplexity: 9.33772\n",
      "Epoch [1/3], Step [14300/41412], Loss: 2.3103, Perplexity: 10.0775\n",
      "Epoch [1/3], Step [14400/41412], Loss: 2.3305, Perplexity: 10.2828\n",
      "Epoch [1/3], Step [14500/41412], Loss: 2.7163, Perplexity: 15.1250\n",
      "Epoch [1/3], Step [14600/41412], Loss: 2.4473, Perplexity: 11.5568\n",
      "Epoch [1/3], Step [14700/41412], Loss: 2.5425, Perplexity: 12.7111\n",
      "Epoch [1/3], Step [14800/41412], Loss: 2.2684, Perplexity: 9.66435\n",
      "Epoch [1/3], Step [14900/41412], Loss: 2.4415, Perplexity: 11.4906\n",
      "Epoch [1/3], Step [15000/41412], Loss: 2.2566, Perplexity: 9.55060\n",
      "Epoch [1/3], Step [15100/41412], Loss: 2.7068, Perplexity: 14.9809\n",
      "Epoch [1/3], Step [15200/41412], Loss: 2.4746, Perplexity: 11.8768\n",
      "Epoch [1/3], Step [15300/41412], Loss: 2.0104, Perplexity: 7.46621\n",
      "Epoch [1/3], Step [15400/41412], Loss: 2.9156, Perplexity: 18.4594\n",
      "Epoch [1/3], Step [15500/41412], Loss: 2.3816, Perplexity: 10.8227\n",
      "Epoch [1/3], Step [15600/41412], Loss: 2.2676, Perplexity: 9.65632\n",
      "Epoch [1/3], Step [15700/41412], Loss: 2.7891, Perplexity: 16.2669\n",
      "Epoch [1/3], Step [15800/41412], Loss: 2.5811, Perplexity: 13.2120\n",
      "Epoch [1/3], Step [15900/41412], Loss: 2.4118, Perplexity: 11.1535\n",
      "Epoch [1/3], Step [16000/41412], Loss: 2.4389, Perplexity: 11.4602\n",
      "Epoch [1/3], Step [16100/41412], Loss: 2.0169, Perplexity: 7.51517\n",
      "Epoch [1/3], Step [16200/41412], Loss: 2.0155, Perplexity: 7.50429\n",
      "Epoch [1/3], Step [16300/41412], Loss: 2.6206, Perplexity: 13.7435\n",
      "Epoch [1/3], Step [16400/41412], Loss: 2.0387, Perplexity: 7.68034\n",
      "Epoch [1/3], Step [16500/41412], Loss: 2.1493, Perplexity: 8.57912\n",
      "Epoch [1/3], Step [16600/41412], Loss: 3.0639, Perplexity: 21.4112\n",
      "Epoch [1/3], Step [16700/41412], Loss: 1.7927, Perplexity: 6.00570\n",
      "Epoch [1/3], Step [16800/41412], Loss: 2.6867, Perplexity: 14.6835\n",
      "Epoch [1/3], Step [16900/41412], Loss: 2.3238, Perplexity: 10.2140\n",
      "Epoch [1/3], Step [17000/41412], Loss: 2.4241, Perplexity: 11.2915\n",
      "Epoch [1/3], Step [17100/41412], Loss: 2.0119, Perplexity: 7.47770\n",
      "Epoch [1/3], Step [17200/41412], Loss: 2.6874, Perplexity: 14.6936\n",
      "Epoch [1/3], Step [17300/41412], Loss: 2.2060, Perplexity: 9.07909\n",
      "Epoch [1/3], Step [17400/41412], Loss: 2.6641, Perplexity: 14.3544\n",
      "Epoch [1/3], Step [17500/41412], Loss: 3.1011, Perplexity: 22.2221\n",
      "Epoch [1/3], Step [17600/41412], Loss: 2.5639, Perplexity: 12.9868\n",
      "Epoch [1/3], Step [17700/41412], Loss: 2.7269, Perplexity: 15.2854\n",
      "Epoch [1/3], Step [17800/41412], Loss: 2.0165, Perplexity: 7.51212\n",
      "Epoch [1/3], Step [17900/41412], Loss: 2.3537, Perplexity: 10.5245\n",
      "Epoch [1/3], Step [18000/41412], Loss: 2.4311, Perplexity: 11.3708\n",
      "Epoch [1/3], Step [18100/41412], Loss: 2.5641, Perplexity: 12.9892\n",
      "Epoch [1/3], Step [18200/41412], Loss: 1.9861, Perplexity: 7.28727\n",
      "Epoch [1/3], Step [18300/41412], Loss: 2.3232, Perplexity: 10.2087\n",
      "Epoch [1/3], Step [18400/41412], Loss: 2.7396, Perplexity: 15.4810\n",
      "Epoch [1/3], Step [18500/41412], Loss: 2.1025, Perplexity: 8.18702\n",
      "Epoch [1/3], Step [18600/41412], Loss: 2.4199, Perplexity: 11.2447\n",
      "Epoch [1/3], Step [18700/41412], Loss: 2.5964, Perplexity: 13.4151\n",
      "Epoch [1/3], Step [18800/41412], Loss: 3.1217, Perplexity: 22.6841\n",
      "Epoch [1/3], Step [18900/41412], Loss: 1.8815, Perplexity: 6.56333\n",
      "Epoch [1/3], Step [19000/41412], Loss: 2.9609, Perplexity: 19.3153\n",
      "Epoch [1/3], Step [19100/41412], Loss: 2.3289, Perplexity: 10.2663\n",
      "Epoch [1/3], Step [19200/41412], Loss: 2.0863, Perplexity: 8.05530\n",
      "Epoch [1/3], Step [19300/41412], Loss: 2.3699, Perplexity: 10.6959\n",
      "Epoch [1/3], Step [19400/41412], Loss: 2.2919, Perplexity: 9.89359\n",
      "Epoch [1/3], Step [19500/41412], Loss: 1.9417, Perplexity: 6.97079\n",
      "Epoch [1/3], Step [19600/41412], Loss: 2.1578, Perplexity: 8.65190\n",
      "Epoch [1/3], Step [19700/41412], Loss: 2.2663, Perplexity: 9.64355\n",
      "Epoch [1/3], Step [19800/41412], Loss: 2.7121, Perplexity: 15.0604\n",
      "Epoch [1/3], Step [19900/41412], Loss: 2.0381, Perplexity: 7.67607\n",
      "Epoch [1/3], Step [20000/41412], Loss: 2.4487, Perplexity: 11.5739\n",
      "Epoch [1/3], Step [20100/41412], Loss: 3.0471, Perplexity: 21.0547\n",
      "Epoch [1/3], Step [20200/41412], Loss: 3.4907, Perplexity: 32.8088\n",
      "Epoch [1/3], Step [20300/41412], Loss: 2.3690, Perplexity: 10.6863\n",
      "Epoch [1/3], Step [20400/41412], Loss: 3.0924, Perplexity: 22.0301\n",
      "Epoch [1/3], Step [20500/41412], Loss: 2.0961, Perplexity: 8.13459\n",
      "Epoch [1/3], Step [20600/41412], Loss: 2.1678, Perplexity: 8.73909\n",
      "Epoch [1/3], Step [20700/41412], Loss: 1.8394, Perplexity: 6.29261\n",
      "Epoch [1/3], Step [20800/41412], Loss: 2.9529, Perplexity: 19.1613\n",
      "Epoch [1/3], Step [20900/41412], Loss: 2.2492, Perplexity: 9.48037\n",
      "Epoch [1/3], Step [21000/41412], Loss: 2.1008, Perplexity: 8.17250\n",
      "Epoch [1/3], Step [21100/41412], Loss: 2.4452, Perplexity: 11.5330\n",
      "Epoch [1/3], Step [21200/41412], Loss: 1.8474, Perplexity: 6.34355\n",
      "Epoch [1/3], Step [21300/41412], Loss: 2.1920, Perplexity: 8.95285\n",
      "Epoch [1/3], Step [21400/41412], Loss: 2.5451, Perplexity: 12.7445\n",
      "Epoch [1/3], Step [21500/41412], Loss: 2.2259, Perplexity: 9.26154\n",
      "Epoch [1/3], Step [21600/41412], Loss: 2.2737, Perplexity: 9.71492\n",
      "Epoch [1/3], Step [21700/41412], Loss: 2.7480, Perplexity: 15.6110\n",
      "Epoch [1/3], Step [21800/41412], Loss: 2.4341, Perplexity: 11.4057\n",
      "Epoch [1/3], Step [21900/41412], Loss: 1.7927, Perplexity: 6.00564\n",
      "Epoch [1/3], Step [22000/41412], Loss: 2.3990, Perplexity: 11.0122\n",
      "Epoch [1/3], Step [22100/41412], Loss: 1.9134, Perplexity: 6.77580\n",
      "Epoch [1/3], Step [22200/41412], Loss: 2.3446, Perplexity: 10.4293\n",
      "Epoch [1/3], Step [22300/41412], Loss: 2.1661, Perplexity: 8.72429\n",
      "Epoch [1/3], Step [22400/41412], Loss: 2.2247, Perplexity: 9.25079\n",
      "Epoch [1/3], Step [22500/41412], Loss: 3.5295, Perplexity: 34.1074\n",
      "Epoch [1/3], Step [22600/41412], Loss: 2.0516, Perplexity: 7.78050\n",
      "Epoch [1/3], Step [22700/41412], Loss: 1.8869, Perplexity: 6.59899\n",
      "Epoch [1/3], Step [22800/41412], Loss: 2.7275, Perplexity: 15.2945\n",
      "Epoch [1/3], Step [22900/41412], Loss: 2.2122, Perplexity: 9.13551\n",
      "Epoch [1/3], Step [23000/41412], Loss: 2.1216, Perplexity: 8.34446\n",
      "Epoch [1/3], Step [23100/41412], Loss: 2.1789, Perplexity: 8.83668\n",
      "Epoch [1/3], Step [23200/41412], Loss: 2.6833, Perplexity: 14.6326\n",
      "Epoch [1/3], Step [23300/41412], Loss: 2.4065, Perplexity: 11.0954\n",
      "Epoch [1/3], Step [23400/41412], Loss: 2.3207, Perplexity: 10.1833\n",
      "Epoch [1/3], Step [23500/41412], Loss: 2.4354, Perplexity: 11.4209\n",
      "Epoch [1/3], Step [23600/41412], Loss: 2.2326, Perplexity: 9.32395\n",
      "Epoch [1/3], Step [23700/41412], Loss: 2.7969, Perplexity: 16.3929\n",
      "Epoch [1/3], Step [23800/41412], Loss: 2.3666, Perplexity: 10.6615\n",
      "Epoch [1/3], Step [23900/41412], Loss: 1.8710, Perplexity: 6.49481\n",
      "Epoch [1/3], Step [24000/41412], Loss: 2.0574, Perplexity: 7.82535\n",
      "Epoch [1/3], Step [24100/41412], Loss: 2.0758, Perplexity: 7.970796\n",
      "Epoch [1/3], Step [24200/41412], Loss: 1.7729, Perplexity: 5.88814\n",
      "Epoch [1/3], Step [24300/41412], Loss: 2.4189, Perplexity: 11.2337\n",
      "Epoch [1/3], Step [24400/41412], Loss: 2.4367, Perplexity: 11.4354\n",
      "Epoch [1/3], Step [24500/41412], Loss: 2.1831, Perplexity: 8.87376\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/3], Step [24600/41412], Loss: 2.1880, Perplexity: 8.91732\n",
      "Epoch [1/3], Step [24700/41412], Loss: 2.3610, Perplexity: 10.6015\n",
      "Epoch [1/3], Step [24800/41412], Loss: 2.7011, Perplexity: 14.8963\n",
      "Epoch [1/3], Step [24900/41412], Loss: 2.3581, Perplexity: 10.5709\n",
      "Epoch [1/3], Step [25000/41412], Loss: 2.4173, Perplexity: 11.2154\n",
      "Epoch [1/3], Step [25100/41412], Loss: 2.6927, Perplexity: 14.7718\n",
      "Epoch [1/3], Step [25200/41412], Loss: 2.6351, Perplexity: 13.9450\n",
      "Epoch [1/3], Step [25300/41412], Loss: 2.4624, Perplexity: 11.7334\n",
      "Epoch [1/3], Step [25400/41412], Loss: 2.2155, Perplexity: 9.16647\n",
      "Epoch [1/3], Step [25500/41412], Loss: 2.7026, Perplexity: 14.9178\n",
      "Epoch [1/3], Step [25600/41412], Loss: 2.4894, Perplexity: 12.0542\n",
      "Epoch [1/3], Step [25700/41412], Loss: 1.6753, Perplexity: 5.34030\n",
      "Epoch [1/3], Step [25800/41412], Loss: 2.3380, Perplexity: 10.3605\n",
      "Epoch [1/3], Step [25900/41412], Loss: 2.2751, Perplexity: 9.72906\n",
      "Epoch [1/3], Step [26000/41412], Loss: 2.2879, Perplexity: 9.85438\n",
      "Epoch [1/3], Step [26100/41412], Loss: 2.7428, Perplexity: 15.5308\n",
      "Epoch [1/3], Step [26200/41412], Loss: 2.2532, Perplexity: 9.51801\n",
      "Epoch [1/3], Step [26300/41412], Loss: 2.1816, Perplexity: 8.86066\n",
      "Epoch [1/3], Step [26400/41412], Loss: 2.0283, Perplexity: 7.60116\n",
      "Epoch [1/3], Step [26500/41412], Loss: 2.5676, Perplexity: 13.0348\n",
      "Epoch [1/3], Step [26600/41412], Loss: 1.9822, Perplexity: 7.25854\n",
      "Epoch [1/3], Step [26700/41412], Loss: 2.3716, Perplexity: 10.7150\n",
      "Epoch [1/3], Step [26800/41412], Loss: 2.1692, Perplexity: 8.75139\n",
      "Epoch [1/3], Step [26900/41412], Loss: 2.1509, Perplexity: 8.59244\n",
      "Epoch [1/3], Step [27000/41412], Loss: 2.3668, Perplexity: 10.6635\n",
      "Epoch [1/3], Step [27100/41412], Loss: 2.5719, Perplexity: 13.0909\n",
      "Epoch [1/3], Step [27200/41412], Loss: 2.1522, Perplexity: 8.60359\n",
      "Epoch [1/3], Step [27300/41412], Loss: 2.5771, Perplexity: 13.1584\n",
      "Epoch [1/3], Step [27400/41412], Loss: 2.1900, Perplexity: 8.93495\n",
      "Epoch [1/3], Step [27500/41412], Loss: 2.6149, Perplexity: 13.6661\n",
      "Epoch [1/3], Step [27600/41412], Loss: 3.3645, Perplexity: 28.9186\n",
      "Epoch [1/3], Step [27700/41412], Loss: 2.5844, Perplexity: 13.2554\n",
      "Epoch [1/3], Step [27800/41412], Loss: 2.5844, Perplexity: 13.2556\n",
      "Epoch [1/3], Step [27900/41412], Loss: 1.8321, Perplexity: 6.24715\n",
      "Epoch [1/3], Step [28000/41412], Loss: 2.7804, Perplexity: 16.1253\n",
      "Epoch [1/3], Step [28100/41412], Loss: 2.1310, Perplexity: 8.42328\n",
      "Epoch [1/3], Step [28200/41412], Loss: 2.2711, Perplexity: 9.68990\n",
      "Epoch [1/3], Step [28300/41412], Loss: 2.3306, Perplexity: 10.2840\n",
      "Epoch [1/3], Step [28400/41412], Loss: 2.2747, Perplexity: 9.72535\n",
      "Epoch [1/3], Step [28500/41412], Loss: 2.0375, Perplexity: 7.67115\n",
      "Epoch [1/3], Step [28600/41412], Loss: 2.3872, Perplexity: 10.8826\n",
      "Epoch [1/3], Step [28700/41412], Loss: 2.1905, Perplexity: 8.94014\n",
      "Epoch [1/3], Step [28800/41412], Loss: 2.3218, Perplexity: 10.1940\n",
      "Epoch [1/3], Step [28900/41412], Loss: 2.5251, Perplexity: 12.4927\n",
      "Epoch [1/3], Step [29000/41412], Loss: 2.2512, Perplexity: 9.49872\n",
      "Epoch [1/3], Step [29100/41412], Loss: 1.8864, Perplexity: 6.59556\n",
      "Epoch [1/3], Step [29200/41412], Loss: 1.7872, Perplexity: 5.97271\n",
      "Epoch [1/3], Step [29300/41412], Loss: 2.2110, Perplexity: 9.12445\n",
      "Epoch [1/3], Step [29400/41412], Loss: 3.0739, Perplexity: 21.6269\n",
      "Epoch [1/3], Step [29500/41412], Loss: 1.9469, Perplexity: 7.00676\n",
      "Epoch [1/3], Step [29600/41412], Loss: 2.3034, Perplexity: 10.0077\n",
      "Epoch [1/3], Step [29700/41412], Loss: 2.2771, Perplexity: 9.74835\n",
      "Epoch [1/3], Step [29800/41412], Loss: 2.4082, Perplexity: 11.1140\n",
      "Epoch [1/3], Step [29900/41412], Loss: 2.3215, Perplexity: 10.1907\n",
      "Epoch [1/3], Step [30000/41412], Loss: 1.7053, Perplexity: 5.50310\n",
      "Epoch [1/3], Step [30100/41412], Loss: 2.6306, Perplexity: 13.8817\n",
      "Epoch [1/3], Step [30200/41412], Loss: 1.8920, Perplexity: 6.63236\n",
      "Epoch [1/3], Step [30300/41412], Loss: 2.0119, Perplexity: 7.47779\n",
      "Epoch [1/3], Step [30400/41412], Loss: 2.4510, Perplexity: 11.6002\n",
      "Epoch [1/3], Step [30500/41412], Loss: 2.0067, Perplexity: 7.43864\n",
      "Epoch [1/3], Step [30600/41412], Loss: 1.9081, Perplexity: 6.74011\n",
      "Epoch [1/3], Step [30700/41412], Loss: 1.7559, Perplexity: 5.78879\n",
      "Epoch [1/3], Step [30800/41412], Loss: 1.8172, Perplexity: 6.15446\n",
      "Epoch [1/3], Step [30900/41412], Loss: 2.6542, Perplexity: 14.2137\n",
      "Epoch [1/3], Step [31000/41412], Loss: 2.1175, Perplexity: 8.31079\n",
      "Epoch [1/3], Step [31100/41412], Loss: 2.1167, Perplexity: 8.30383\n",
      "Epoch [1/3], Step [31200/41412], Loss: 2.4202, Perplexity: 11.2485\n",
      "Epoch [1/3], Step [31300/41412], Loss: 2.4595, Perplexity: 11.6985\n",
      "Epoch [1/3], Step [31400/41412], Loss: 2.5444, Perplexity: 12.7360\n",
      "Epoch [1/3], Step [31500/41412], Loss: 2.1029, Perplexity: 8.18982\n",
      "Epoch [1/3], Step [31600/41412], Loss: 2.1862, Perplexity: 8.90138\n",
      "Epoch [1/3], Step [31700/41412], Loss: 1.9054, Perplexity: 6.72232\n",
      "Epoch [1/3], Step [31800/41412], Loss: 2.3504, Perplexity: 10.4898\n",
      "Epoch [1/3], Step [31900/41412], Loss: 2.7410, Perplexity: 15.5032\n",
      "Epoch [1/3], Step [32000/41412], Loss: 1.8599, Perplexity: 6.42284\n",
      "Epoch [1/3], Step [32100/41412], Loss: 2.6256, Perplexity: 13.8129\n",
      "Epoch [1/3], Step [32200/41412], Loss: 1.8957, Perplexity: 6.65700\n",
      "Epoch [1/3], Step [32300/41412], Loss: 2.1053, Perplexity: 8.20988\n",
      "Epoch [1/3], Step [32400/41412], Loss: 2.1001, Perplexity: 8.16699\n",
      "Epoch [1/3], Step [32500/41412], Loss: 1.9917, Perplexity: 7.32775\n",
      "Epoch [1/3], Step [32600/41412], Loss: 2.3619, Perplexity: 10.6116\n",
      "Epoch [1/3], Step [32700/41412], Loss: 2.3236, Perplexity: 10.2120\n",
      "Epoch [1/3], Step [32800/41412], Loss: 2.4380, Perplexity: 11.4496\n",
      "Epoch [1/3], Step [32900/41412], Loss: 2.2235, Perplexity: 9.23920\n",
      "Epoch [1/3], Step [33000/41412], Loss: 2.1977, Perplexity: 9.00456\n",
      "Epoch [1/3], Step [33100/41412], Loss: 2.5521, Perplexity: 12.8336\n",
      "Epoch [1/3], Step [33200/41412], Loss: 2.0784, Perplexity: 7.99164\n",
      "Epoch [1/3], Step [33300/41412], Loss: 2.3605, Perplexity: 10.5963\n",
      "Epoch [1/3], Step [33400/41412], Loss: 2.1462, Perplexity: 8.55278\n",
      "Epoch [1/3], Step [33500/41412], Loss: 2.1623, Perplexity: 8.69138\n",
      "Epoch [1/3], Step [33600/41412], Loss: 2.5314, Perplexity: 12.5705\n",
      "Epoch [1/3], Step [33700/41412], Loss: 2.4595, Perplexity: 11.6985\n",
      "Epoch [1/3], Step [33800/41412], Loss: 2.5831, Perplexity: 13.2386\n",
      "Epoch [1/3], Step [33900/41412], Loss: 2.5343, Perplexity: 12.6076\n",
      "Epoch [1/3], Step [34000/41412], Loss: 2.2968, Perplexity: 9.94183\n",
      "Epoch [1/3], Step [34100/41412], Loss: 2.1865, Perplexity: 8.90404\n",
      "Epoch [1/3], Step [34200/41412], Loss: 2.2053, Perplexity: 9.07303\n",
      "Epoch [1/3], Step [34300/41412], Loss: 1.5301, Perplexity: 4.61866\n",
      "Epoch [1/3], Step [34400/41412], Loss: 2.1986, Perplexity: 9.01260\n",
      "Epoch [1/3], Step [34500/41412], Loss: 2.1906, Perplexity: 8.94018\n",
      "Epoch [1/3], Step [34600/41412], Loss: 1.7344, Perplexity: 5.66578\n",
      "Epoch [1/3], Step [34700/41412], Loss: 2.2216, Perplexity: 9.22245\n",
      "Epoch [1/3], Step [34800/41412], Loss: 2.3109, Perplexity: 10.0830\n",
      "Epoch [1/3], Step [34900/41412], Loss: 2.1036, Perplexity: 8.19530\n",
      "Epoch [1/3], Step [35000/41412], Loss: 2.7268, Perplexity: 15.2846\n",
      "Epoch [1/3], Step [35100/41412], Loss: 2.0572, Perplexity: 7.82398\n",
      "Epoch [1/3], Step [35200/41412], Loss: 2.2847, Perplexity: 9.82262\n",
      "Epoch [1/3], Step [35300/41412], Loss: 1.7664, Perplexity: 5.84982\n",
      "Epoch [1/3], Step [35400/41412], Loss: 1.5432, Perplexity: 4.67943\n",
      "Epoch [1/3], Step [35500/41412], Loss: 2.0310, Perplexity: 7.62163\n",
      "Epoch [1/3], Step [35600/41412], Loss: 2.0222, Perplexity: 7.55462\n",
      "Epoch [1/3], Step [35700/41412], Loss: 2.9381, Perplexity: 18.8799\n",
      "Epoch [1/3], Step [35800/41412], Loss: 1.9018, Perplexity: 6.69798\n",
      "Epoch [1/3], Step [35900/41412], Loss: 1.8808, Perplexity: 6.55852\n",
      "Epoch [1/3], Step [36000/41412], Loss: 2.4745, Perplexity: 11.8755\n",
      "Epoch [1/3], Step [36100/41412], Loss: 2.4147, Perplexity: 11.1870\n",
      "Epoch [1/3], Step [36200/41412], Loss: 1.9155, Perplexity: 6.79013\n",
      "Epoch [1/3], Step [36300/41412], Loss: 2.5291, Perplexity: 12.5425\n",
      "Epoch [1/3], Step [36400/41412], Loss: 1.9551, Perplexity: 7.06449\n",
      "Epoch [1/3], Step [36500/41412], Loss: 2.1203, Perplexity: 8.33360\n",
      "Epoch [1/3], Step [36600/41412], Loss: 2.1362, Perplexity: 8.46768\n",
      "Epoch [1/3], Step [36700/41412], Loss: 1.5689, Perplexity: 4.80123\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/3], Step [36800/41412], Loss: 2.6049, Perplexity: 13.5303\n",
      "Epoch [1/3], Step [36900/41412], Loss: 1.7980, Perplexity: 6.03767\n",
      "Epoch [1/3], Step [37000/41412], Loss: 2.2824, Perplexity: 9.80020\n",
      "Epoch [1/3], Step [37100/41412], Loss: 2.0977, Perplexity: 8.147539\n",
      "Epoch [1/3], Step [37200/41412], Loss: 2.2625, Perplexity: 9.60726\n",
      "Epoch [1/3], Step [37300/41412], Loss: 2.7236, Perplexity: 15.2346\n",
      "Epoch [1/3], Step [37400/41412], Loss: 2.3759, Perplexity: 10.7608\n",
      "Epoch [1/3], Step [37500/41412], Loss: 2.0263, Perplexity: 7.58639\n",
      "Epoch [1/3], Step [37600/41412], Loss: 2.0273, Perplexity: 7.59396\n",
      "Epoch [1/3], Step [37700/41412], Loss: 2.3494, Perplexity: 10.4788\n",
      "Epoch [1/3], Step [37800/41412], Loss: 2.4376, Perplexity: 11.4455\n",
      "Epoch [1/3], Step [37900/41412], Loss: 2.7769, Perplexity: 16.0696\n",
      "Epoch [1/3], Step [38000/41412], Loss: 2.0372, Perplexity: 7.66924\n",
      "Epoch [1/3], Step [38100/41412], Loss: 1.9148, Perplexity: 6.78553\n",
      "Epoch [1/3], Step [38200/41412], Loss: 2.3603, Perplexity: 10.5939\n",
      "Epoch [1/3], Step [38300/41412], Loss: 2.2927, Perplexity: 9.90148\n",
      "Epoch [1/3], Step [38400/41412], Loss: 1.7288, Perplexity: 5.63392\n",
      "Epoch [1/3], Step [38500/41412], Loss: 1.8972, Perplexity: 6.66728\n",
      "Epoch [1/3], Step [38600/41412], Loss: 1.7610, Perplexity: 5.81814\n",
      "Epoch [1/3], Step [38700/41412], Loss: 2.7824, Perplexity: 16.1570\n",
      "Epoch [1/3], Step [38800/41412], Loss: 2.3228, Perplexity: 10.2044\n",
      "Epoch [1/3], Step [38900/41412], Loss: 2.3118, Perplexity: 10.0925\n",
      "Epoch [1/3], Step [39000/41412], Loss: 2.0904, Perplexity: 8.08786\n",
      "Epoch [1/3], Step [39100/41412], Loss: 1.7077, Perplexity: 5.51626\n",
      "Epoch [1/3], Step [39200/41412], Loss: 2.5115, Perplexity: 12.3235\n",
      "Epoch [1/3], Step [39300/41412], Loss: 2.0554, Perplexity: 7.809774\n",
      "Epoch [1/3], Step [39400/41412], Loss: 2.1414, Perplexity: 8.51166\n",
      "Epoch [1/3], Step [39500/41412], Loss: 2.1359, Perplexity: 8.46431\n",
      "Epoch [1/3], Step [39600/41412], Loss: 2.6645, Perplexity: 14.3612\n",
      "Epoch [1/3], Step [39700/41412], Loss: 2.4437, Perplexity: 11.5154\n",
      "Epoch [1/3], Step [39800/41412], Loss: 1.9476, Perplexity: 7.01177\n",
      "Epoch [1/3], Step [39900/41412], Loss: 1.8766, Perplexity: 6.53128\n",
      "Epoch [1/3], Step [40000/41412], Loss: 2.1585, Perplexity: 8.65858\n",
      "Epoch [1/3], Step [40100/41412], Loss: 2.3067, Perplexity: 10.0410\n",
      "Epoch [1/3], Step [40200/41412], Loss: 2.3056, Perplexity: 10.0299\n",
      "Epoch [1/3], Step [40300/41412], Loss: 1.8691, Perplexity: 6.48224\n",
      "Epoch [1/3], Step [40400/41412], Loss: 2.6527, Perplexity: 14.1922\n",
      "Epoch [1/3], Step [40500/41412], Loss: 2.2511, Perplexity: 9.49833\n",
      "Epoch [1/3], Step [40600/41412], Loss: 1.8701, Perplexity: 6.48887\n",
      "Epoch [1/3], Step [40700/41412], Loss: 2.2491, Perplexity: 9.47888\n",
      "Epoch [1/3], Step [40800/41412], Loss: 2.4610, Perplexity: 11.7166\n",
      "Epoch [1/3], Step [40900/41412], Loss: 2.7087, Perplexity: 15.0096\n",
      "Epoch [1/3], Step [41000/41412], Loss: 1.8039, Perplexity: 6.07350\n",
      "Epoch [1/3], Step [41100/41412], Loss: 2.6087, Perplexity: 13.5811\n",
      "Epoch [1/3], Step [41200/41412], Loss: 1.8914, Perplexity: 6.62867\n",
      "Epoch [1/3], Step [41300/41412], Loss: 2.1771, Perplexity: 8.82089\n",
      "Epoch [1/3], Step [41400/41412], Loss: 2.1978, Perplexity: 9.00546\n",
      "Epoch [2/3], Step [100/41412], Loss: 2.2826, Perplexity: 9.8026872\n",
      "Epoch [2/3], Step [200/41412], Loss: 2.3088, Perplexity: 10.0627\n",
      "Epoch [2/3], Step [300/41412], Loss: 1.8228, Perplexity: 6.18900\n",
      "Epoch [2/3], Step [400/41412], Loss: 1.9936, Perplexity: 7.34194\n",
      "Epoch [2/3], Step [500/41412], Loss: 2.3727, Perplexity: 10.7266\n",
      "Epoch [2/3], Step [600/41412], Loss: 2.1736, Perplexity: 8.79020\n",
      "Epoch [2/3], Step [700/41412], Loss: 2.1095, Perplexity: 8.24396\n",
      "Epoch [2/3], Step [800/41412], Loss: 2.8520, Perplexity: 17.3221\n",
      "Epoch [2/3], Step [900/41412], Loss: 3.2846, Perplexity: 26.6981\n",
      "Epoch [2/3], Step [1000/41412], Loss: 1.9439, Perplexity: 6.9860\n",
      "Epoch [2/3], Step [1100/41412], Loss: 2.8072, Perplexity: 16.5636\n",
      "Epoch [2/3], Step [1200/41412], Loss: 2.0209, Perplexity: 7.54501\n",
      "Epoch [2/3], Step [1300/41412], Loss: 2.3123, Perplexity: 10.0976\n",
      "Epoch [2/3], Step [1400/41412], Loss: 1.6142, Perplexity: 5.02376\n",
      "Epoch [2/3], Step [1500/41412], Loss: 2.3292, Perplexity: 10.2696\n",
      "Epoch [2/3], Step [1600/41412], Loss: 1.9329, Perplexity: 6.90942\n",
      "Epoch [2/3], Step [1700/41412], Loss: 2.0982, Perplexity: 8.15119\n",
      "Epoch [2/3], Step [1800/41412], Loss: 2.0256, Perplexity: 7.58079\n",
      "Epoch [2/3], Step [1900/41412], Loss: 2.5144, Perplexity: 12.3592\n",
      "Epoch [2/3], Step [2000/41412], Loss: 2.3821, Perplexity: 10.82805\n",
      "Epoch [2/3], Step [2100/41412], Loss: 1.7540, Perplexity: 5.777722\n",
      "Epoch [2/3], Step [2200/41412], Loss: 2.0432, Perplexity: 7.71515\n",
      "Epoch [2/3], Step [2300/41412], Loss: 1.9052, Perplexity: 6.72052\n",
      "Epoch [2/3], Step [2400/41412], Loss: 2.5477, Perplexity: 12.7775\n",
      "Epoch [2/3], Step [2500/41412], Loss: 1.5642, Perplexity: 4.77873\n",
      "Epoch [2/3], Step [2600/41412], Loss: 2.4019, Perplexity: 11.0445\n",
      "Epoch [2/3], Step [2700/41412], Loss: 2.3762, Perplexity: 10.7641\n",
      "Epoch [2/3], Step [2800/41412], Loss: 2.5692, Perplexity: 13.0552\n",
      "Epoch [2/3], Step [2900/41412], Loss: 1.8901, Perplexity: 6.62008\n",
      "Epoch [2/3], Step [3000/41412], Loss: 2.8693, Perplexity: 17.6254\n",
      "Epoch [2/3], Step [3100/41412], Loss: 2.0953, Perplexity: 8.12793\n",
      "Epoch [2/3], Step [3200/41412], Loss: 2.3672, Perplexity: 10.6679\n",
      "Epoch [2/3], Step [3300/41412], Loss: 2.4180, Perplexity: 11.2230\n",
      "Epoch [2/3], Step [3400/41412], Loss: 2.3798, Perplexity: 10.8031\n",
      "Epoch [2/3], Step [3500/41412], Loss: 1.9909, Perplexity: 7.32195\n",
      "Epoch [2/3], Step [3600/41412], Loss: 1.8352, Perplexity: 6.26640\n",
      "Epoch [2/3], Step [3700/41412], Loss: 2.2133, Perplexity: 9.14615\n",
      "Epoch [2/3], Step [3800/41412], Loss: 1.9224, Perplexity: 6.83721\n",
      "Epoch [2/3], Step [3900/41412], Loss: 1.9695, Perplexity: 7.16742\n",
      "Epoch [2/3], Step [4000/41412], Loss: 2.8012, Perplexity: 16.4639\n",
      "Epoch [2/3], Step [4100/41412], Loss: 2.0846, Perplexity: 8.04179\n",
      "Epoch [2/3], Step [4200/41412], Loss: 1.8068, Perplexity: 6.09114\n",
      "Epoch [2/3], Step [4300/41412], Loss: 1.7686, Perplexity: 5.86294\n",
      "Epoch [2/3], Step [4400/41412], Loss: 2.1855, Perplexity: 8.89518\n",
      "Epoch [2/3], Step [4500/41412], Loss: 2.0113, Perplexity: 7.47324\n",
      "Epoch [2/3], Step [4600/41412], Loss: 2.6166, Perplexity: 13.6889\n",
      "Epoch [2/3], Step [4700/41412], Loss: 3.0831, Perplexity: 21.8265\n",
      "Epoch [2/3], Step [4800/41412], Loss: 2.2725, Perplexity: 9.70332\n",
      "Epoch [2/3], Step [4900/41412], Loss: 2.4212, Perplexity: 11.25918\n",
      "Epoch [2/3], Step [5000/41412], Loss: 2.4029, Perplexity: 11.0556\n",
      "Epoch [2/3], Step [5100/41412], Loss: 2.1426, Perplexity: 8.52158\n",
      "Epoch [2/3], Step [5200/41412], Loss: 2.3533, Perplexity: 10.5204\n",
      "Epoch [2/3], Step [5300/41412], Loss: 2.1383, Perplexity: 8.48520\n",
      "Epoch [2/3], Step [5400/41412], Loss: 2.0031, Perplexity: 7.41217\n",
      "Epoch [2/3], Step [5500/41412], Loss: 2.5316, Perplexity: 12.5733\n",
      "Epoch [2/3], Step [5600/41412], Loss: 2.1453, Perplexity: 8.54496\n",
      "Epoch [2/3], Step [5700/41412], Loss: 2.1198, Perplexity: 8.32921\n",
      "Epoch [2/3], Step [5800/41412], Loss: 2.4460, Perplexity: 11.5422\n",
      "Epoch [2/3], Step [5900/41412], Loss: 2.4214, Perplexity: 11.2621\n",
      "Epoch [2/3], Step [6000/41412], Loss: 2.0895, Perplexity: 8.08098\n",
      "Epoch [2/3], Step [6100/41412], Loss: 2.0431, Perplexity: 7.71450\n",
      "Epoch [2/3], Step [6200/41412], Loss: 2.0376, Perplexity: 7.67224\n",
      "Epoch [2/3], Step [6300/41412], Loss: 2.2213, Perplexity: 9.21946\n",
      "Epoch [2/3], Step [6400/41412], Loss: 2.3213, Perplexity: 10.18912\n",
      "Epoch [2/3], Step [6500/41412], Loss: 2.2872, Perplexity: 9.84762\n",
      "Epoch [2/3], Step [6600/41412], Loss: 1.8924, Perplexity: 6.63541\n",
      "Epoch [2/3], Step [6700/41412], Loss: 1.8747, Perplexity: 6.51863\n",
      "Epoch [2/3], Step [6800/41412], Loss: 2.3289, Perplexity: 10.2663\n",
      "Epoch [2/3], Step [6900/41412], Loss: 2.6309, Perplexity: 13.8863\n",
      "Epoch [2/3], Step [7000/41412], Loss: 2.3848, Perplexity: 10.8568\n",
      "Epoch [2/3], Step [7100/41412], Loss: 1.9335, Perplexity: 6.91386\n",
      "Epoch [2/3], Step [7200/41412], Loss: 2.3686, Perplexity: 10.6828\n",
      "Epoch [2/3], Step [7300/41412], Loss: 2.3565, Perplexity: 10.5540\n",
      "Epoch [2/3], Step [7400/41412], Loss: 1.8924, Perplexity: 6.63527\n",
      "Epoch [2/3], Step [7500/41412], Loss: 2.1213, Perplexity: 8.34234\n",
      "Epoch [2/3], Step [7600/41412], Loss: 2.3295, Perplexity: 10.2727\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/3], Step [7700/41412], Loss: 1.9410, Perplexity: 6.96550\n",
      "Epoch [2/3], Step [7800/41412], Loss: 2.5571, Perplexity: 12.8979\n",
      "Epoch [2/3], Step [7900/41412], Loss: 2.4167, Perplexity: 11.2089\n",
      "Epoch [2/3], Step [8000/41412], Loss: 2.4327, Perplexity: 11.3896\n",
      "Epoch [2/3], Step [8100/41412], Loss: 2.0968, Perplexity: 8.14000\n",
      "Epoch [2/3], Step [8200/41412], Loss: 2.7252, Perplexity: 15.2597\n",
      "Epoch [2/3], Step [8300/41412], Loss: 2.5177, Perplexity: 12.4004\n",
      "Epoch [2/3], Step [8400/41412], Loss: 1.9716, Perplexity: 7.18184\n",
      "Epoch [2/3], Step [8500/41412], Loss: 1.6127, Perplexity: 5.01636\n",
      "Epoch [2/3], Step [8600/41412], Loss: 1.9385, Perplexity: 6.94825\n",
      "Epoch [2/3], Step [8700/41412], Loss: 1.9763, Perplexity: 7.21596\n",
      "Epoch [2/3], Step [8800/41412], Loss: 2.6657, Perplexity: 14.3785\n",
      "Epoch [2/3], Step [8900/41412], Loss: 2.1244, Perplexity: 8.36763\n",
      "Epoch [2/3], Step [9000/41412], Loss: 1.8788, Perplexity: 6.54534\n",
      "Epoch [2/3], Step [9100/41412], Loss: 2.5006, Perplexity: 12.1903\n",
      "Epoch [2/3], Step [9200/41412], Loss: 2.5381, Perplexity: 12.6558\n",
      "Epoch [2/3], Step [9300/41412], Loss: 2.0100, Perplexity: 7.46359\n",
      "Epoch [2/3], Step [9400/41412], Loss: 2.0709, Perplexity: 7.93217\n",
      "Epoch [2/3], Step [9500/41412], Loss: 2.4234, Perplexity: 11.2841\n",
      "Epoch [2/3], Step [9600/41412], Loss: 2.1734, Perplexity: 8.78795\n",
      "Epoch [2/3], Step [9700/41412], Loss: 2.1297, Perplexity: 8.41250\n",
      "Epoch [2/3], Step [9800/41412], Loss: 2.2502, Perplexity: 9.48940\n",
      "Epoch [2/3], Step [9900/41412], Loss: 2.1046, Perplexity: 8.20394\n",
      "Epoch [2/3], Step [10000/41412], Loss: 2.5721, Perplexity: 13.0933\n",
      "Epoch [2/3], Step [10100/41412], Loss: 1.4873, Perplexity: 4.42534\n",
      "Epoch [2/3], Step [10200/41412], Loss: 1.7909, Perplexity: 5.99515\n",
      "Epoch [2/3], Step [10300/41412], Loss: 2.2329, Perplexity: 9.32671\n",
      "Epoch [2/3], Step [10400/41412], Loss: 2.4395, Perplexity: 11.4668\n",
      "Epoch [2/3], Step [10500/41412], Loss: 1.9116, Perplexity: 6.76385\n",
      "Epoch [2/3], Step [10600/41412], Loss: 1.8164, Perplexity: 6.14952\n",
      "Epoch [2/3], Step [10700/41412], Loss: 2.2507, Perplexity: 9.49447\n",
      "Epoch [2/3], Step [10800/41412], Loss: 2.6789, Perplexity: 14.5697\n",
      "Epoch [2/3], Step [10900/41412], Loss: 2.4105, Perplexity: 11.1392\n",
      "Epoch [2/3], Step [11000/41412], Loss: 2.3260, Perplexity: 10.2368\n",
      "Epoch [2/3], Step [11100/41412], Loss: 2.4827, Perplexity: 11.9733\n",
      "Epoch [2/3], Step [11200/41412], Loss: 2.2039, Perplexity: 9.06000\n",
      "Epoch [2/3], Step [11300/41412], Loss: 1.5411, Perplexity: 4.66955\n",
      "Epoch [2/3], Step [11400/41412], Loss: 1.5882, Perplexity: 4.89476\n",
      "Epoch [2/3], Step [11500/41412], Loss: 1.8432, Perplexity: 6.31680\n",
      "Epoch [2/3], Step [11600/41412], Loss: 2.4867, Perplexity: 12.0216\n",
      "Epoch [2/3], Step [11700/41412], Loss: 2.1518, Perplexity: 8.60072\n",
      "Epoch [2/3], Step [11800/41412], Loss: 2.0309, Perplexity: 7.62093\n",
      "Epoch [2/3], Step [11900/41412], Loss: 1.7313, Perplexity: 5.64828\n",
      "Epoch [2/3], Step [12000/41412], Loss: 2.1196, Perplexity: 8.32752\n",
      "Epoch [2/3], Step [12100/41412], Loss: 1.7843, Perplexity: 5.95536\n",
      "Epoch [2/3], Step [12200/41412], Loss: 2.5760, Perplexity: 13.1439\n",
      "Epoch [2/3], Step [12300/41412], Loss: 2.0655, Perplexity: 7.88902\n",
      "Epoch [2/3], Step [12400/41412], Loss: 2.3052, Perplexity: 10.0257\n",
      "Epoch [2/3], Step [12500/41412], Loss: 2.4421, Perplexity: 11.4976\n",
      "Epoch [2/3], Step [12600/41412], Loss: 2.1852, Perplexity: 8.89299\n",
      "Epoch [2/3], Step [12700/41412], Loss: 2.1749, Perplexity: 8.80105\n",
      "Epoch [2/3], Step [12800/41412], Loss: 2.0595, Perplexity: 7.84176\n",
      "Epoch [2/3], Step [12900/41412], Loss: 1.7238, Perplexity: 5.60571\n",
      "Epoch [2/3], Step [13000/41412], Loss: 2.2862, Perplexity: 9.83765\n",
      "Epoch [2/3], Step [13100/41412], Loss: 2.2467, Perplexity: 9.45623\n",
      "Epoch [2/3], Step [13200/41412], Loss: 1.8090, Perplexity: 6.10464\n",
      "Epoch [2/3], Step [13300/41412], Loss: 2.1668, Perplexity: 8.73022\n",
      "Epoch [2/3], Step [13400/41412], Loss: 1.7102, Perplexity: 5.52995\n",
      "Epoch [2/3], Step [13500/41412], Loss: 1.7521, Perplexity: 5.76691\n",
      "Epoch [2/3], Step [13600/41412], Loss: 3.1299, Perplexity: 22.8723\n",
      "Epoch [2/3], Step [13700/41412], Loss: 1.7262, Perplexity: 5.61930\n",
      "Epoch [2/3], Step [13800/41412], Loss: 2.6157, Perplexity: 13.6767\n",
      "Epoch [2/3], Step [13900/41412], Loss: 1.7460, Perplexity: 5.73165\n",
      "Epoch [2/3], Step [14000/41412], Loss: 2.1315, Perplexity: 8.42793\n",
      "Epoch [2/3], Step [14100/41412], Loss: 1.7516, Perplexity: 5.76372\n",
      "Epoch [2/3], Step [14200/41412], Loss: 1.8554, Perplexity: 6.39401\n",
      "Epoch [2/3], Step [14300/41412], Loss: 1.7255, Perplexity: 5.61513\n",
      "Epoch [2/3], Step [14400/41412], Loss: 2.3682, Perplexity: 10.6785\n",
      "Epoch [2/3], Step [14500/41412], Loss: 1.7583, Perplexity: 5.80266\n",
      "Epoch [2/3], Step [14600/41412], Loss: 2.5484, Perplexity: 12.7863\n",
      "Epoch [2/3], Step [14700/41412], Loss: 2.0523, Perplexity: 7.78553\n",
      "Epoch [2/3], Step [14800/41412], Loss: 2.2771, Perplexity: 9.74824\n",
      "Epoch [2/3], Step [14900/41412], Loss: 2.1656, Perplexity: 8.71999\n",
      "Epoch [2/3], Step [15000/41412], Loss: 2.3283, Perplexity: 10.2600\n",
      "Epoch [2/3], Step [15100/41412], Loss: 2.5422, Perplexity: 12.7073\n",
      "Epoch [2/3], Step [15200/41412], Loss: 2.1716, Perplexity: 8.77270\n",
      "Epoch [2/3], Step [15300/41412], Loss: 2.3159, Perplexity: 10.1339\n",
      "Epoch [2/3], Step [15400/41412], Loss: 2.5031, Perplexity: 12.2200\n",
      "Epoch [2/3], Step [15500/41412], Loss: 2.2454, Perplexity: 9.44388\n",
      "Epoch [2/3], Step [15600/41412], Loss: 2.1752, Perplexity: 8.80415\n",
      "Epoch [2/3], Step [15700/41412], Loss: 2.2765, Perplexity: 9.74299\n",
      "Epoch [2/3], Step [15800/41412], Loss: 2.5697, Perplexity: 13.0617\n",
      "Epoch [2/3], Step [15900/41412], Loss: 2.1878, Perplexity: 8.91599\n",
      "Epoch [2/3], Step [16000/41412], Loss: 2.0332, Perplexity: 7.63861\n",
      "Epoch [2/3], Step [16100/41412], Loss: 2.5959, Perplexity: 13.4092\n",
      "Epoch [2/3], Step [16200/41412], Loss: 1.4611, Perplexity: 4.31078\n",
      "Epoch [2/3], Step [16300/41412], Loss: 2.1427, Perplexity: 8.52237\n",
      "Epoch [2/3], Step [16400/41412], Loss: 2.2850, Perplexity: 9.82587\n",
      "Epoch [2/3], Step [16500/41412], Loss: 2.2517, Perplexity: 9.50352\n",
      "Epoch [2/3], Step [16600/41412], Loss: 2.3756, Perplexity: 10.7572\n",
      "Epoch [2/3], Step [16700/41412], Loss: 1.5122, Perplexity: 4.53698\n",
      "Epoch [2/3], Step [16800/41412], Loss: 2.2309, Perplexity: 9.30786\n",
      "Epoch [2/3], Step [16900/41412], Loss: 2.4152, Perplexity: 11.1921\n",
      "Epoch [2/3], Step [17000/41412], Loss: 1.9643, Perplexity: 7.12994\n",
      "Epoch [2/3], Step [17100/41412], Loss: 2.4963, Perplexity: 12.1370\n",
      "Epoch [2/3], Step [17200/41412], Loss: 1.9175, Perplexity: 6.80431\n",
      "Epoch [2/3], Step [17300/41412], Loss: 2.9517, Perplexity: 19.1389\n",
      "Epoch [2/3], Step [17400/41412], Loss: 2.2485, Perplexity: 9.47314\n",
      "Epoch [2/3], Step [17500/41412], Loss: 1.8451, Perplexity: 6.32905\n",
      "Epoch [2/3], Step [17600/41412], Loss: 1.5367, Perplexity: 4.64907\n",
      "Epoch [2/3], Step [17700/41412], Loss: 2.3751, Perplexity: 10.7518\n",
      "Epoch [2/3], Step [17800/41412], Loss: 2.1240, Perplexity: 8.36485\n",
      "Epoch [2/3], Step [17900/41412], Loss: 2.0139, Perplexity: 7.49245\n",
      "Epoch [2/3], Step [18000/41412], Loss: 3.0623, Perplexity: 21.3759\n",
      "Epoch [2/3], Step [18100/41412], Loss: 2.0188, Perplexity: 7.52963\n",
      "Epoch [2/3], Step [18200/41412], Loss: 1.9390, Perplexity: 6.95157\n",
      "Epoch [2/3], Step [18300/41412], Loss: 2.1215, Perplexity: 8.34370\n",
      "Epoch [2/3], Step [18400/41412], Loss: 2.4208, Perplexity: 11.2545\n",
      "Epoch [2/3], Step [18500/41412], Loss: 2.0470, Perplexity: 7.74468\n",
      "Epoch [2/3], Step [18600/41412], Loss: 2.5914, Perplexity: 13.3487\n",
      "Epoch [2/3], Step [18700/41412], Loss: 2.4933, Perplexity: 12.1015\n",
      "Epoch [2/3], Step [18800/41412], Loss: 2.4274, Perplexity: 11.3293\n",
      "Epoch [2/3], Step [18900/41412], Loss: 1.7968, Perplexity: 6.03051\n",
      "Epoch [2/3], Step [19000/41412], Loss: 2.1385, Perplexity: 8.48697\n",
      "Epoch [2/3], Step [19100/41412], Loss: 1.7273, Perplexity: 5.62529\n",
      "Epoch [2/3], Step [19200/41412], Loss: 1.9329, Perplexity: 6.90932\n",
      "Epoch [2/3], Step [19300/41412], Loss: 2.3419, Perplexity: 10.4009\n",
      "Epoch [2/3], Step [19400/41412], Loss: 2.2782, Perplexity: 9.75934\n",
      "Epoch [2/3], Step [19500/41412], Loss: 1.7428, Perplexity: 5.71321\n",
      "Epoch [2/3], Step [19600/41412], Loss: 2.4265, Perplexity: 11.3197\n",
      "Epoch [2/3], Step [19700/41412], Loss: 2.1214, Perplexity: 8.34309\n",
      "Epoch [2/3], Step [19800/41412], Loss: 1.7894, Perplexity: 5.98619\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/3], Step [19900/41412], Loss: 2.0296, Perplexity: 7.61113\n",
      "Epoch [2/3], Step [20000/41412], Loss: 1.4716, Perplexity: 4.35600\n",
      "Epoch [2/3], Step [20100/41412], Loss: 1.4643, Perplexity: 4.32443\n",
      "Epoch [2/3], Step [20200/41412], Loss: 2.1867, Perplexity: 8.90576\n",
      "Epoch [2/3], Step [20300/41412], Loss: 2.4822, Perplexity: 11.9672\n",
      "Epoch [2/3], Step [20400/41412], Loss: 2.1615, Perplexity: 8.68420\n",
      "Epoch [2/3], Step [20500/41412], Loss: 1.6009, Perplexity: 4.95738\n",
      "Epoch [2/3], Step [20600/41412], Loss: 2.8318, Perplexity: 16.9756\n",
      "Epoch [2/3], Step [20700/41412], Loss: 2.0012, Perplexity: 7.39835\n",
      "Epoch [2/3], Step [20800/41412], Loss: 2.2888, Perplexity: 9.86290\n",
      "Epoch [2/3], Step [20900/41412], Loss: 1.6937, Perplexity: 5.43969\n",
      "Epoch [2/3], Step [21000/41412], Loss: 1.8917, Perplexity: 6.63097\n",
      "Epoch [2/3], Step [21100/41412], Loss: 1.9758, Perplexity: 7.21273\n",
      "Epoch [2/3], Step [21200/41412], Loss: 2.0262, Perplexity: 7.58529\n",
      "Epoch [2/3], Step [21300/41412], Loss: 1.9982, Perplexity: 7.37607\n",
      "Epoch [2/3], Step [21400/41412], Loss: 2.3722, Perplexity: 10.7209\n",
      "Epoch [2/3], Step [21500/41412], Loss: 2.4266, Perplexity: 11.3205\n",
      "Epoch [2/3], Step [21600/41412], Loss: 1.9463, Perplexity: 7.00262\n",
      "Epoch [2/3], Step [21700/41412], Loss: 1.7910, Perplexity: 5.99568\n",
      "Epoch [2/3], Step [21800/41412], Loss: 2.0710, Perplexity: 7.93285\n",
      "Epoch [2/3], Step [21900/41412], Loss: 2.3727, Perplexity: 10.7268\n",
      "Epoch [2/3], Step [22000/41412], Loss: 2.6482, Perplexity: 14.1279\n",
      "Epoch [2/3], Step [22100/41412], Loss: 1.8999, Perplexity: 6.68494\n",
      "Epoch [2/3], Step [22200/41412], Loss: 2.2665, Perplexity: 9.64599\n",
      "Epoch [2/3], Step [22300/41412], Loss: 2.8950, Perplexity: 18.0831\n",
      "Epoch [2/3], Step [22400/41412], Loss: 2.1362, Perplexity: 8.46723\n",
      "Epoch [2/3], Step [22500/41412], Loss: 2.0237, Perplexity: 7.56644\n",
      "Epoch [2/3], Step [22600/41412], Loss: 2.4101, Perplexity: 11.1355\n",
      "Epoch [2/3], Step [22700/41412], Loss: 1.3963, Perplexity: 4.04026\n",
      "Epoch [2/3], Step [22800/41412], Loss: 1.8907, Perplexity: 6.62420\n",
      "Epoch [2/3], Step [22900/41412], Loss: 2.1056, Perplexity: 8.21217\n",
      "Epoch [2/3], Step [23000/41412], Loss: 2.3894, Perplexity: 10.9070\n",
      "Epoch [2/3], Step [23100/41412], Loss: 1.8353, Perplexity: 6.26736\n",
      "Epoch [2/3], Step [23200/41412], Loss: 2.2607, Perplexity: 9.58979\n",
      "Epoch [2/3], Step [23300/41412], Loss: 1.7355, Perplexity: 5.67174\n",
      "Epoch [2/3], Step [23400/41412], Loss: 2.3054, Perplexity: 10.0284\n",
      "Epoch [2/3], Step [23500/41412], Loss: 2.3301, Perplexity: 10.2794\n",
      "Epoch [2/3], Step [23600/41412], Loss: 2.6412, Perplexity: 14.0303\n",
      "Epoch [2/3], Step [23700/41412], Loss: 1.9128, Perplexity: 6.77209\n",
      "Epoch [2/3], Step [23800/41412], Loss: 1.8089, Perplexity: 6.10340\n",
      "Epoch [2/3], Step [23900/41412], Loss: 2.3388, Perplexity: 10.3684\n",
      "Epoch [2/3], Step [24000/41412], Loss: 2.2383, Perplexity: 9.37721\n",
      "Epoch [2/3], Step [24100/41412], Loss: 2.5006, Perplexity: 12.1903\n",
      "Epoch [2/3], Step [24200/41412], Loss: 2.1419, Perplexity: 8.51550\n",
      "Epoch [2/3], Step [24300/41412], Loss: 1.7655, Perplexity: 5.84461\n",
      "Epoch [2/3], Step [24400/41412], Loss: 1.6704, Perplexity: 5.31443\n",
      "Epoch [2/3], Step [24500/41412], Loss: 1.9419, Perplexity: 6.97225\n",
      "Epoch [2/3], Step [24600/41412], Loss: 2.0280, Perplexity: 7.59873\n",
      "Epoch [2/3], Step [24700/41412], Loss: 2.4452, Perplexity: 11.5323\n",
      "Epoch [2/3], Step [24800/41412], Loss: 1.5923, Perplexity: 4.91501\n",
      "Epoch [2/3], Step [24900/41412], Loss: 1.5672, Perplexity: 4.79305\n",
      "Epoch [2/3], Step [25000/41412], Loss: 2.3905, Perplexity: 10.9193\n",
      "Epoch [2/3], Step [25100/41412], Loss: 2.4176, Perplexity: 11.2186\n",
      "Epoch [2/3], Step [25200/41412], Loss: 2.6359, Perplexity: 13.9563\n",
      "Epoch [2/3], Step [25300/41412], Loss: 2.3448, Perplexity: 10.4314\n",
      "Epoch [2/3], Step [25400/41412], Loss: 2.2138, Perplexity: 9.15047\n",
      "Epoch [2/3], Step [25500/41412], Loss: 2.9555, Perplexity: 19.2110\n",
      "Epoch [2/3], Step [25600/41412], Loss: 1.6568, Perplexity: 5.24242\n",
      "Epoch [2/3], Step [25700/41412], Loss: 2.4846, Perplexity: 11.9961\n",
      "Epoch [2/3], Step [25800/41412], Loss: 2.0056, Perplexity: 7.43037\n",
      "Epoch [2/3], Step [25900/41412], Loss: 1.7852, Perplexity: 5.96082\n",
      "Epoch [2/3], Step [26000/41412], Loss: 2.2575, Perplexity: 9.55955\n",
      "Epoch [2/3], Step [26100/41412], Loss: 2.5666, Perplexity: 13.0213\n",
      "Epoch [2/3], Step [26200/41412], Loss: 1.5915, Perplexity: 4.91119\n",
      "Epoch [2/3], Step [26300/41412], Loss: 2.8574, Perplexity: 17.4157\n",
      "Epoch [2/3], Step [26400/41412], Loss: 2.3128, Perplexity: 10.1032\n",
      "Epoch [2/3], Step [26500/41412], Loss: 1.6112, Perplexity: 5.00883\n",
      "Epoch [2/3], Step [26600/41412], Loss: 2.1005, Perplexity: 8.17050\n",
      "Epoch [2/3], Step [26700/41412], Loss: 2.2019, Perplexity: 9.04242\n",
      "Epoch [2/3], Step [26800/41412], Loss: 2.1528, Perplexity: 8.60890\n",
      "Epoch [2/3], Step [26900/41412], Loss: 2.0304, Perplexity: 7.61683\n",
      "Epoch [2/3], Step [27000/41412], Loss: 1.8202, Perplexity: 6.17324\n",
      "Epoch [2/3], Step [27100/41412], Loss: 1.6772, Perplexity: 5.35066\n",
      "Epoch [2/3], Step [27200/41412], Loss: 2.2667, Perplexity: 9.64754\n",
      "Epoch [2/3], Step [27300/41412], Loss: 1.8949, Perplexity: 6.65177\n",
      "Epoch [2/3], Step [27400/41412], Loss: 2.1420, Perplexity: 8.51637\n",
      "Epoch [2/3], Step [27500/41412], Loss: 1.9207, Perplexity: 6.82562\n",
      "Epoch [2/3], Step [27600/41412], Loss: 2.0942, Perplexity: 8.11900\n",
      "Epoch [2/3], Step [27700/41412], Loss: 1.5903, Perplexity: 4.90544\n",
      "Epoch [2/3], Step [27800/41412], Loss: 1.9393, Perplexity: 6.95407\n",
      "Epoch [2/3], Step [27900/41412], Loss: 1.7272, Perplexity: 5.62512\n",
      "Epoch [2/3], Step [28000/41412], Loss: 2.3803, Perplexity: 10.8086\n",
      "Epoch [2/3], Step [28100/41412], Loss: 2.2355, Perplexity: 9.35081\n",
      "Epoch [2/3], Step [28200/41412], Loss: 2.1059, Perplexity: 8.21456\n",
      "Epoch [2/3], Step [28300/41412], Loss: 1.9592, Perplexity: 7.09407\n",
      "Epoch [2/3], Step [28400/41412], Loss: 2.2276, Perplexity: 9.27789\n",
      "Epoch [2/3], Step [28500/41412], Loss: 2.3170, Perplexity: 10.1448\n",
      "Epoch [2/3], Step [28600/41412], Loss: 2.2897, Perplexity: 9.87199\n",
      "Epoch [2/3], Step [28700/41412], Loss: 2.5359, Perplexity: 12.62746\n",
      "Epoch [2/3], Step [28800/41412], Loss: 1.7785, Perplexity: 5.92109\n",
      "Epoch [2/3], Step [28900/41412], Loss: 2.2844, Perplexity: 9.81956\n",
      "Epoch [2/3], Step [29000/41412], Loss: 2.2092, Perplexity: 9.10888\n",
      "Epoch [2/3], Step [29100/41412], Loss: 2.1335, Perplexity: 8.44474\n",
      "Epoch [2/3], Step [29200/41412], Loss: 2.3470, Perplexity: 10.4538\n",
      "Epoch [2/3], Step [29300/41412], Loss: 2.2695, Perplexity: 9.67426\n",
      "Epoch [2/3], Step [29400/41412], Loss: 2.2983, Perplexity: 9.95700\n",
      "Epoch [2/3], Step [29500/41412], Loss: 1.5125, Perplexity: 4.53823\n",
      "Epoch [2/3], Step [29600/41412], Loss: 2.0484, Perplexity: 7.75584\n",
      "Epoch [2/3], Step [29700/41412], Loss: 2.0545, Perplexity: 7.80309\n",
      "Epoch [2/3], Step [29800/41412], Loss: 2.4878, Perplexity: 12.0346\n",
      "Epoch [2/3], Step [29900/41412], Loss: 2.3048, Perplexity: 10.0226\n",
      "Epoch [2/3], Step [30000/41412], Loss: 1.9455, Perplexity: 6.99711\n",
      "Epoch [2/3], Step [30100/41412], Loss: 2.0513, Perplexity: 7.77765\n",
      "Epoch [2/3], Step [30200/41412], Loss: 1.8292, Perplexity: 6.22887\n",
      "Epoch [2/3], Step [30300/41412], Loss: 1.8817, Perplexity: 6.56442\n",
      "Epoch [2/3], Step [30400/41412], Loss: 2.1864, Perplexity: 8.90321\n",
      "Epoch [2/3], Step [30500/41412], Loss: 2.1502, Perplexity: 8.58709\n",
      "Epoch [2/3], Step [30600/41412], Loss: 2.2489, Perplexity: 9.47765\n",
      "Epoch [2/3], Step [30700/41412], Loss: 2.2716, Perplexity: 9.69498\n",
      "Epoch [2/3], Step [30800/41412], Loss: 2.3149, Perplexity: 10.1234\n",
      "Epoch [2/3], Step [30900/41412], Loss: 2.1823, Perplexity: 8.86637\n",
      "Epoch [2/3], Step [31000/41412], Loss: 2.0154, Perplexity: 7.50387\n",
      "Epoch [2/3], Step [31100/41412], Loss: 2.0773, Perplexity: 7.98291\n",
      "Epoch [2/3], Step [31200/41412], Loss: 1.5841, Perplexity: 4.87503\n",
      "Epoch [2/3], Step [31300/41412], Loss: 1.9989, Perplexity: 7.38061\n",
      "Epoch [2/3], Step [31400/41412], Loss: 1.8608, Perplexity: 6.42872\n",
      "Epoch [2/3], Step [31500/41412], Loss: 2.5075, Perplexity: 12.2745\n",
      "Epoch [2/3], Step [31600/41412], Loss: 2.1581, Perplexity: 8.65474\n",
      "Epoch [2/3], Step [31700/41412], Loss: 2.3111, Perplexity: 10.0853\n",
      "Epoch [2/3], Step [31800/41412], Loss: 1.8497, Perplexity: 6.35774\n",
      "Epoch [2/3], Step [31900/41412], Loss: 1.9560, Perplexity: 7.07100\n",
      "Epoch [2/3], Step [32000/41412], Loss: 2.4951, Perplexity: 12.1224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/3], Step [32100/41412], Loss: 1.8453, Perplexity: 6.32998\n",
      "Epoch [2/3], Step [32200/41412], Loss: 2.0510, Perplexity: 7.77584\n",
      "Epoch [2/3], Step [32300/41412], Loss: 2.4509, Perplexity: 11.5989\n",
      "Epoch [2/3], Step [32400/41412], Loss: 2.4970, Perplexity: 12.1465\n",
      "Epoch [2/3], Step [32500/41412], Loss: 1.9213, Perplexity: 6.82963\n",
      "Epoch [2/3], Step [32600/41412], Loss: 1.8854, Perplexity: 6.58884\n",
      "Epoch [2/3], Step [32700/41412], Loss: 2.1202, Perplexity: 8.33288\n",
      "Epoch [2/3], Step [32800/41412], Loss: 1.8836, Perplexity: 6.57729\n",
      "Epoch [2/3], Step [32900/41412], Loss: 2.1471, Perplexity: 8.56019\n",
      "Epoch [2/3], Step [33000/41412], Loss: 2.2222, Perplexity: 9.22752\n",
      "Epoch [2/3], Step [33100/41412], Loss: 2.7442, Perplexity: 15.5518\n",
      "Epoch [2/3], Step [33200/41412], Loss: 2.7699, Perplexity: 15.9566\n",
      "Epoch [2/3], Step [33300/41412], Loss: 2.0435, Perplexity: 7.71746\n",
      "Epoch [2/3], Step [33400/41412], Loss: 2.5424, Perplexity: 12.7096\n",
      "Epoch [2/3], Step [33500/41412], Loss: 1.9338, Perplexity: 6.91551\n",
      "Epoch [2/3], Step [33600/41412], Loss: 2.3670, Perplexity: 10.6654\n",
      "Epoch [2/3], Step [33700/41412], Loss: 2.5669, Perplexity: 13.0251\n",
      "Epoch [2/3], Step [33800/41412], Loss: 2.0911, Perplexity: 8.09368\n",
      "Epoch [2/3], Step [33900/41412], Loss: 2.3736, Perplexity: 10.7358\n",
      "Epoch [2/3], Step [34000/41412], Loss: 2.0793, Perplexity: 7.99922\n",
      "Epoch [2/3], Step [34100/41412], Loss: 2.7380, Perplexity: 15.4564\n",
      "Epoch [2/3], Step [34200/41412], Loss: 2.1194, Perplexity: 8.32599\n",
      "Epoch [2/3], Step [34300/41412], Loss: 3.0217, Perplexity: 20.5265\n",
      "Epoch [2/3], Step [34400/41412], Loss: 2.3118, Perplexity: 10.0925\n",
      "Epoch [2/3], Step [34500/41412], Loss: 2.0169, Perplexity: 7.51516\n",
      "Epoch [2/3], Step [34600/41412], Loss: 1.9882, Perplexity: 7.30254\n",
      "Epoch [2/3], Step [34700/41412], Loss: 2.4509, Perplexity: 11.5992\n",
      "Epoch [2/3], Step [34800/41412], Loss: 1.9744, Perplexity: 7.20206\n",
      "Epoch [2/3], Step [34900/41412], Loss: 2.3087, Perplexity: 10.0613\n",
      "Epoch [2/3], Step [35000/41412], Loss: 2.2605, Perplexity: 9.58839\n",
      "Epoch [2/3], Step [35100/41412], Loss: 2.1135, Perplexity: 8.27707\n",
      "Epoch [2/3], Step [35200/41412], Loss: 1.9363, Perplexity: 6.93293\n",
      "Epoch [2/3], Step [35300/41412], Loss: 1.8137, Perplexity: 6.13333\n",
      "Epoch [2/3], Step [35400/41412], Loss: 1.9548, Perplexity: 7.06239\n",
      "Epoch [2/3], Step [35500/41412], Loss: 1.3077, Perplexity: 3.69788\n",
      "Epoch [2/3], Step [35600/41412], Loss: 1.8972, Perplexity: 6.66733\n",
      "Epoch [2/3], Step [35700/41412], Loss: 2.0238, Perplexity: 7.56686\n",
      "Epoch [2/3], Step [35800/41412], Loss: 1.5445, Perplexity: 4.68584\n",
      "Epoch [2/3], Step [35900/41412], Loss: 2.2637, Perplexity: 9.61874\n",
      "Epoch [2/3], Step [36000/41412], Loss: 1.9286, Perplexity: 6.87994\n",
      "Epoch [2/3], Step [36100/41412], Loss: 2.4163, Perplexity: 11.2046\n",
      "Epoch [2/3], Step [36200/41412], Loss: 2.7715, Perplexity: 15.9820\n",
      "Epoch [2/3], Step [36300/41412], Loss: 1.5371, Perplexity: 4.65120\n",
      "Epoch [2/3], Step [36400/41412], Loss: 2.1520, Perplexity: 8.60220\n",
      "Epoch [2/3], Step [36500/41412], Loss: 1.9368, Perplexity: 6.93647\n",
      "Epoch [2/3], Step [36600/41412], Loss: 2.2789, Perplexity: 9.76557\n",
      "Epoch [2/3], Step [36700/41412], Loss: 1.7643, Perplexity: 5.83779\n",
      "Epoch [2/3], Step [36800/41412], Loss: 1.9396, Perplexity: 6.95586\n",
      "Epoch [2/3], Step [36900/41412], Loss: 2.1614, Perplexity: 8.68369\n",
      "Epoch [2/3], Step [37000/41412], Loss: 1.9442, Perplexity: 6.98785\n",
      "Epoch [2/3], Step [37100/41412], Loss: 1.6829, Perplexity: 5.38137\n",
      "Epoch [2/3], Step [37200/41412], Loss: 2.0876, Perplexity: 8.06596\n",
      "Epoch [2/3], Step [37300/41412], Loss: 2.1877, Perplexity: 8.91446\n",
      "Epoch [2/3], Step [37400/41412], Loss: 1.9748, Perplexity: 7.20505\n",
      "Epoch [2/3], Step [37500/41412], Loss: 2.2934, Perplexity: 9.90841\n",
      "Epoch [2/3], Step [37600/41412], Loss: 2.3297, Perplexity: 10.2751\n",
      "Epoch [2/3], Step [37700/41412], Loss: 1.5976, Perplexity: 4.94128\n",
      "Epoch [2/3], Step [37800/41412], Loss: 2.1841, Perplexity: 8.88233\n",
      "Epoch [2/3], Step [37900/41412], Loss: 1.8424, Perplexity: 6.31148\n",
      "Epoch [2/3], Step [38000/41412], Loss: 2.0674, Perplexity: 7.90392\n",
      "Epoch [2/3], Step [38100/41412], Loss: 2.4938, Perplexity: 12.1066\n",
      "Epoch [2/3], Step [38200/41412], Loss: 1.9874, Perplexity: 7.29664\n",
      "Epoch [2/3], Step [38300/41412], Loss: 2.1253, Perplexity: 8.37529\n",
      "Epoch [2/3], Step [38400/41412], Loss: 2.4106, Perplexity: 11.1411\n",
      "Epoch [2/3], Step [38500/41412], Loss: 2.1697, Perplexity: 8.75543\n",
      "Epoch [2/3], Step [38600/41412], Loss: 2.0593, Perplexity: 7.84015\n",
      "Epoch [2/3], Step [38700/41412], Loss: 1.6630, Perplexity: 5.27527\n",
      "Epoch [2/3], Step [38800/41412], Loss: 2.1957, Perplexity: 8.98643\n",
      "Epoch [2/3], Step [38900/41412], Loss: 2.2628, Perplexity: 9.61019\n",
      "Epoch [2/3], Step [39000/41412], Loss: 2.4140, Perplexity: 11.1791\n",
      "Epoch [2/3], Step [39100/41412], Loss: 2.1987, Perplexity: 9.01329\n",
      "Epoch [2/3], Step [39200/41412], Loss: 2.2756, Perplexity: 9.73400\n",
      "Epoch [2/3], Step [39300/41412], Loss: 1.8569, Perplexity: 6.40375\n",
      "Epoch [2/3], Step [39400/41412], Loss: 2.4905, Perplexity: 12.0671\n",
      "Epoch [2/3], Step [39500/41412], Loss: 2.1049, Perplexity: 8.20634\n",
      "Epoch [2/3], Step [39600/41412], Loss: 2.0392, Perplexity: 7.68473\n",
      "Epoch [2/3], Step [39700/41412], Loss: 1.7352, Perplexity: 5.67033\n",
      "Epoch [2/3], Step [39800/41412], Loss: 2.5347, Perplexity: 12.6127\n",
      "Epoch [2/3], Step [39900/41412], Loss: 1.8197, Perplexity: 6.17021\n",
      "Epoch [2/3], Step [40000/41412], Loss: 2.1180, Perplexity: 8.31424\n",
      "Epoch [2/3], Step [40100/41412], Loss: 1.9971, Perplexity: 7.36759\n",
      "Epoch [2/3], Step [40200/41412], Loss: 1.6240, Perplexity: 5.07336\n",
      "Epoch [2/3], Step [40300/41412], Loss: 2.2127, Perplexity: 9.14015\n",
      "Epoch [2/3], Step [40400/41412], Loss: 2.0840, Perplexity: 8.03699\n",
      "Epoch [2/3], Step [40500/41412], Loss: 1.7021, Perplexity: 5.48547\n",
      "Epoch [2/3], Step [40600/41412], Loss: 2.1967, Perplexity: 8.99531\n",
      "Epoch [2/3], Step [40700/41412], Loss: 2.1555, Perplexity: 8.63233\n",
      "Epoch [2/3], Step [40800/41412], Loss: 2.3764, Perplexity: 10.7656\n",
      "Epoch [2/3], Step [40900/41412], Loss: 2.5361, Perplexity: 12.6299\n",
      "Epoch [2/3], Step [41000/41412], Loss: 2.1252, Perplexity: 8.37463\n",
      "Epoch [2/3], Step [41100/41412], Loss: 2.3957, Perplexity: 10.9761\n",
      "Epoch [2/3], Step [41200/41412], Loss: 1.6692, Perplexity: 5.30783\n",
      "Epoch [2/3], Step [41300/41412], Loss: 2.4082, Perplexity: 11.1140\n",
      "Epoch [2/3], Step [41400/41412], Loss: 2.1522, Perplexity: 8.60392\n",
      "Epoch [3/3], Step [100/41412], Loss: 1.8886, Perplexity: 6.6104008\n",
      "Epoch [3/3], Step [200/41412], Loss: 2.3010, Perplexity: 9.98439\n",
      "Epoch [3/3], Step [300/41412], Loss: 1.9192, Perplexity: 6.81520\n",
      "Epoch [3/3], Step [400/41412], Loss: 2.1377, Perplexity: 8.47970\n",
      "Epoch [3/3], Step [500/41412], Loss: 2.5334, Perplexity: 12.5956\n",
      "Epoch [3/3], Step [600/41412], Loss: 2.0599, Perplexity: 7.84522\n",
      "Epoch [3/3], Step [700/41412], Loss: 1.9118, Perplexity: 6.76500\n",
      "Epoch [3/3], Step [800/41412], Loss: 2.0952, Perplexity: 8.12732\n",
      "Epoch [3/3], Step [900/41412], Loss: 2.5422, Perplexity: 12.7082\n",
      "Epoch [3/3], Step [1000/41412], Loss: 1.7677, Perplexity: 5.8572\n",
      "Epoch [3/3], Step [1100/41412], Loss: 1.8670, Perplexity: 6.46875\n",
      "Epoch [3/3], Step [1200/41412], Loss: 1.9207, Perplexity: 6.82588\n",
      "Epoch [3/3], Step [1300/41412], Loss: 2.3031, Perplexity: 10.0055\n",
      "Epoch [3/3], Step [1400/41412], Loss: 2.2416, Perplexity: 9.40869\n",
      "Epoch [3/3], Step [1500/41412], Loss: 2.3464, Perplexity: 10.4478\n",
      "Epoch [3/3], Step [1600/41412], Loss: 2.2123, Perplexity: 9.13711\n",
      "Epoch [3/3], Step [1700/41412], Loss: 1.7196, Perplexity: 5.58264\n",
      "Epoch [3/3], Step [1800/41412], Loss: 2.4186, Perplexity: 11.2300\n",
      "Epoch [3/3], Step [1900/41412], Loss: 2.3764, Perplexity: 10.7659\n",
      "Epoch [3/3], Step [2000/41412], Loss: 1.8029, Perplexity: 6.06701\n",
      "Epoch [3/3], Step [2100/41412], Loss: 1.9632, Perplexity: 7.12217\n",
      "Epoch [3/3], Step [2200/41412], Loss: 1.9014, Perplexity: 6.69522\n",
      "Epoch [3/3], Step [2300/41412], Loss: 2.7793, Perplexity: 16.1071\n",
      "Epoch [3/3], Step [2400/41412], Loss: 2.2073, Perplexity: 9.09115\n",
      "Epoch [3/3], Step [2500/41412], Loss: 2.3929, Perplexity: 10.9446\n",
      "Epoch [3/3], Step [2600/41412], Loss: 2.3069, Perplexity: 10.0433\n",
      "Epoch [3/3], Step [2700/41412], Loss: 2.3038, Perplexity: 10.0118\n",
      "Epoch [3/3], Step [2800/41412], Loss: 2.6078, Perplexity: 13.5695\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/3], Step [2900/41412], Loss: 1.8106, Perplexity: 6.11392\n",
      "Epoch [3/3], Step [3000/41412], Loss: 1.7291, Perplexity: 5.63566\n",
      "Epoch [3/3], Step [3100/41412], Loss: 2.0189, Perplexity: 7.53011\n",
      "Epoch [3/3], Step [3200/41412], Loss: 2.2854, Perplexity: 9.82955\n",
      "Epoch [3/3], Step [3300/41412], Loss: 2.0719, Perplexity: 7.93960\n",
      "Epoch [3/3], Step [3400/41412], Loss: 1.9852, Perplexity: 7.28053\n",
      "Epoch [3/3], Step [3500/41412], Loss: 2.1397, Perplexity: 8.49712\n",
      "Epoch [3/3], Step [3600/41412], Loss: 2.0638, Perplexity: 7.87608\n",
      "Epoch [3/3], Step [3700/41412], Loss: 1.5584, Perplexity: 4.75134\n",
      "Epoch [3/3], Step [3800/41412], Loss: 2.0319, Perplexity: 7.62888\n",
      "Epoch [3/3], Step [3900/41412], Loss: 2.0532, Perplexity: 7.79313\n",
      "Epoch [3/3], Step [4000/41412], Loss: 2.3255, Perplexity: 10.2319\n",
      "Epoch [3/3], Step [4100/41412], Loss: 1.8501, Perplexity: 6.36056\n",
      "Epoch [3/3], Step [4200/41412], Loss: 2.1233, Perplexity: 8.35888\n",
      "Epoch [3/3], Step [4300/41412], Loss: 1.9476, Perplexity: 7.01214\n",
      "Epoch [3/3], Step [4400/41412], Loss: 1.8691, Perplexity: 6.48273\n",
      "Epoch [3/3], Step [4500/41412], Loss: 2.8852, Perplexity: 17.9080\n",
      "Epoch [3/3], Step [4600/41412], Loss: 2.1544, Perplexity: 8.62276\n",
      "Epoch [3/3], Step [4700/41412], Loss: 2.2899, Perplexity: 9.87368\n",
      "Epoch [3/3], Step [4800/41412], Loss: 1.9810, Perplexity: 7.25023\n",
      "Epoch [3/3], Step [4900/41412], Loss: 2.2473, Perplexity: 9.46195\n",
      "Epoch [3/3], Step [5000/41412], Loss: 1.6928, Perplexity: 5.43464\n",
      "Epoch [3/3], Step [5100/41412], Loss: 2.4105, Perplexity: 11.1394\n",
      "Epoch [3/3], Step [5200/41412], Loss: 2.3980, Perplexity: 11.0007\n",
      "Epoch [3/3], Step [5300/41412], Loss: 2.2171, Perplexity: 9.18036\n",
      "Epoch [3/3], Step [5400/41412], Loss: 2.7593, Perplexity: 15.7893\n",
      "Epoch [3/3], Step [5500/41412], Loss: 1.9920, Perplexity: 7.33051\n",
      "Epoch [3/3], Step [5600/41412], Loss: 2.3981, Perplexity: 11.0017\n",
      "Epoch [3/3], Step [5700/41412], Loss: 1.7819, Perplexity: 5.94127\n",
      "Epoch [3/3], Step [5800/41412], Loss: 1.7198, Perplexity: 5.58332\n",
      "Epoch [3/3], Step [5900/41412], Loss: 1.8673, Perplexity: 6.47056\n",
      "Epoch [3/3], Step [6000/41412], Loss: 1.8048, Perplexity: 6.07896\n",
      "Epoch [3/3], Step [6100/41412], Loss: 2.1651, Perplexity: 8.71570\n",
      "Epoch [3/3], Step [6200/41412], Loss: 1.9748, Perplexity: 7.20491\n",
      "Epoch [3/3], Step [6300/41412], Loss: 1.7380, Perplexity: 5.68606\n",
      "Epoch [3/3], Step [6400/41412], Loss: 1.8181, Perplexity: 6.16025\n",
      "Epoch [3/3], Step [6500/41412], Loss: 2.0435, Perplexity: 7.71720\n",
      "Epoch [3/3], Step [6600/41412], Loss: 2.2809, Perplexity: 9.78554\n",
      "Epoch [3/3], Step [6700/41412], Loss: 2.1956, Perplexity: 8.98504\n",
      "Epoch [3/3], Step [6800/41412], Loss: 1.9164, Perplexity: 6.79660\n",
      "Epoch [3/3], Step [6900/41412], Loss: 1.9336, Perplexity: 6.91431\n",
      "Epoch [3/3], Step [7000/41412], Loss: 2.1822, Perplexity: 8.86570\n",
      "Epoch [3/3], Step [7100/41412], Loss: 1.8100, Perplexity: 6.11043\n",
      "Epoch [3/3], Step [7200/41412], Loss: 2.2991, Perplexity: 9.96542\n",
      "Epoch [3/3], Step [7300/41412], Loss: 2.5002, Perplexity: 12.1854\n",
      "Epoch [3/3], Step [7400/41412], Loss: 1.7447, Perplexity: 5.72411\n",
      "Epoch [3/3], Step [7500/41412], Loss: 2.2108, Perplexity: 9.12346\n",
      "Epoch [3/3], Step [7600/41412], Loss: 2.4848, Perplexity: 11.9991\n",
      "Epoch [3/3], Step [7700/41412], Loss: 1.8996, Perplexity: 6.68313\n",
      "Epoch [3/3], Step [7800/41412], Loss: 3.0889, Perplexity: 21.9525\n",
      "Epoch [3/3], Step [7900/41412], Loss: 2.0914, Perplexity: 8.09643\n",
      "Epoch [3/3], Step [8000/41412], Loss: 1.8069, Perplexity: 6.09177\n",
      "Epoch [3/3], Step [8100/41412], Loss: 1.6946, Perplexity: 5.44435\n",
      "Epoch [3/3], Step [8200/41412], Loss: 2.4830, Perplexity: 11.9774\n",
      "Epoch [3/3], Step [8300/41412], Loss: 2.4018, Perplexity: 11.0427\n",
      "Epoch [3/3], Step [8400/41412], Loss: 2.2546, Perplexity: 9.53138\n",
      "Epoch [3/3], Step [8500/41412], Loss: 1.7795, Perplexity: 5.92676\n",
      "Epoch [3/3], Step [8600/41412], Loss: 1.9367, Perplexity: 6.93606\n",
      "Epoch [3/3], Step [8700/41412], Loss: 2.6556, Perplexity: 14.2340\n",
      "Epoch [3/3], Step [8800/41412], Loss: 1.7114, Perplexity: 5.53653\n",
      "Epoch [3/3], Step [8900/41412], Loss: 2.1499, Perplexity: 8.58405\n",
      "Epoch [3/3], Step [9000/41412], Loss: 2.3601, Perplexity: 10.5918\n",
      "Epoch [3/3], Step [9100/41412], Loss: 1.4530, Perplexity: 4.27609\n",
      "Epoch [3/3], Step [9200/41412], Loss: 2.1713, Perplexity: 8.76971\n",
      "Epoch [3/3], Step [9300/41412], Loss: 2.3215, Perplexity: 10.1905\n",
      "Epoch [3/3], Step [9400/41412], Loss: 1.8823, Perplexity: 6.56835\n",
      "Epoch [3/3], Step [9500/41412], Loss: 1.9567, Perplexity: 7.07608\n",
      "Epoch [3/3], Step [9600/41412], Loss: 2.7937, Perplexity: 16.3414\n",
      "Epoch [3/3], Step [9700/41412], Loss: 1.8306, Perplexity: 6.23768\n",
      "Epoch [3/3], Step [9800/41412], Loss: 2.7716, Perplexity: 15.9843\n",
      "Epoch [3/3], Step [9900/41412], Loss: 2.1240, Perplexity: 8.36488\n",
      "Epoch [3/3], Step [10000/41412], Loss: 2.3203, Perplexity: 10.1790\n",
      "Epoch [3/3], Step [10100/41412], Loss: 1.6608, Perplexity: 5.26346\n",
      "Epoch [3/3], Step [10200/41412], Loss: 1.7993, Perplexity: 6.04524\n",
      "Epoch [3/3], Step [10300/41412], Loss: 2.6178, Perplexity: 13.7059\n",
      "Epoch [3/3], Step [10400/41412], Loss: 2.3170, Perplexity: 10.1449\n",
      "Epoch [3/3], Step [10500/41412], Loss: 1.7989, Perplexity: 6.04270\n",
      "Epoch [3/3], Step [10600/41412], Loss: 2.2792, Perplexity: 9.76882\n",
      "Epoch [3/3], Step [10700/41412], Loss: 2.4040, Perplexity: 11.0679\n",
      "Epoch [3/3], Step [10800/41412], Loss: 2.4024, Perplexity: 11.0496\n",
      "Epoch [3/3], Step [10900/41412], Loss: 1.9675, Perplexity: 7.15254\n",
      "Epoch [3/3], Step [11000/41412], Loss: 2.7777, Perplexity: 16.0815\n",
      "Epoch [3/3], Step [11100/41412], Loss: 1.9372, Perplexity: 6.93914\n",
      "Epoch [3/3], Step [11200/41412], Loss: 1.6486, Perplexity: 5.19959\n",
      "Epoch [3/3], Step [11300/41412], Loss: 1.8098, Perplexity: 6.10935\n",
      "Epoch [3/3], Step [11400/41412], Loss: 1.8300, Perplexity: 6.23407\n",
      "Epoch [3/3], Step [11500/41412], Loss: 2.0404, Perplexity: 7.69395\n",
      "Epoch [3/3], Step [11600/41412], Loss: 2.6429, Perplexity: 14.0536\n",
      "Epoch [3/3], Step [11700/41412], Loss: 2.2135, Perplexity: 9.14808\n",
      "Epoch [3/3], Step [11800/41412], Loss: 2.8622, Perplexity: 17.5005\n",
      "Epoch [3/3], Step [11900/41412], Loss: 2.1550, Perplexity: 8.62763\n",
      "Epoch [3/3], Step [12000/41412], Loss: 1.9799, Perplexity: 7.24186\n",
      "Epoch [3/3], Step [12100/41412], Loss: 2.5304, Perplexity: 12.5590\n",
      "Epoch [3/3], Step [12200/41412], Loss: 2.2919, Perplexity: 9.89374\n",
      "Epoch [3/3], Step [12300/41412], Loss: 1.8091, Perplexity: 6.10529\n",
      "Epoch [3/3], Step [12400/41412], Loss: 2.0927, Perplexity: 8.10664\n",
      "Epoch [3/3], Step [12500/41412], Loss: 1.3543, Perplexity: 3.87428\n",
      "Epoch [3/3], Step [12600/41412], Loss: 2.7172, Perplexity: 15.1378\n",
      "Epoch [3/3], Step [12700/41412], Loss: 2.3024, Perplexity: 9.99834\n",
      "Epoch [3/3], Step [12800/41412], Loss: 2.1073, Perplexity: 8.22571\n",
      "Epoch [3/3], Step [12900/41412], Loss: 1.8011, Perplexity: 6.05633\n",
      "Epoch [3/3], Step [13000/41412], Loss: 1.9159, Perplexity: 6.79291\n",
      "Epoch [3/3], Step [13100/41412], Loss: 1.6659, Perplexity: 5.29026\n",
      "Epoch [3/3], Step [13200/41412], Loss: 2.5318, Perplexity: 12.5757\n",
      "Epoch [3/3], Step [13300/41412], Loss: 1.8962, Perplexity: 6.66035\n",
      "Epoch [3/3], Step [13400/41412], Loss: 2.6704, Perplexity: 14.4460\n",
      "Epoch [3/3], Step [13500/41412], Loss: 2.0613, Perplexity: 7.85606\n",
      "Epoch [3/3], Step [13600/41412], Loss: 1.5448, Perplexity: 4.68721\n",
      "Epoch [3/3], Step [13700/41412], Loss: 2.2362, Perplexity: 9.35781\n",
      "Epoch [3/3], Step [13800/41412], Loss: 2.1140, Perplexity: 8.28131\n",
      "Epoch [3/3], Step [13900/41412], Loss: 1.9414, Perplexity: 6.96874\n",
      "Epoch [3/3], Step [14000/41412], Loss: 1.9300, Perplexity: 6.88967\n",
      "Epoch [3/3], Step [14100/41412], Loss: 2.1637, Perplexity: 8.70346\n",
      "Epoch [3/3], Step [14200/41412], Loss: 2.2493, Perplexity: 9.48081\n",
      "Epoch [3/3], Step [14300/41412], Loss: 2.3564, Perplexity: 10.5528\n",
      "Epoch [3/3], Step [14400/41412], Loss: 2.4756, Perplexity: 11.8890\n",
      "Epoch [3/3], Step [14500/41412], Loss: 2.0998, Perplexity: 8.16446\n",
      "Epoch [3/3], Step [14600/41412], Loss: 1.9814, Perplexity: 7.25275\n",
      "Epoch [3/3], Step [14700/41412], Loss: 1.8585, Perplexity: 6.41436\n",
      "Epoch [3/3], Step [14800/41412], Loss: 2.3106, Perplexity: 10.0803\n",
      "Epoch [3/3], Step [14900/41412], Loss: 1.9404, Perplexity: 6.96122\n",
      "Epoch [3/3], Step [15000/41412], Loss: 1.8711, Perplexity: 6.49578\n",
      "Epoch [3/3], Step [15100/41412], Loss: 1.9085, Perplexity: 6.74314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/3], Step [15200/41412], Loss: 1.8156, Perplexity: 6.14487\n",
      "Epoch [3/3], Step [15300/41412], Loss: 1.8570, Perplexity: 6.40459\n",
      "Epoch [3/3], Step [15400/41412], Loss: 1.8713, Perplexity: 6.49672\n",
      "Epoch [3/3], Step [15500/41412], Loss: 1.7394, Perplexity: 5.69410\n",
      "Epoch [3/3], Step [15600/41412], Loss: 1.7547, Perplexity: 5.78142\n",
      "Epoch [3/3], Step [15700/41412], Loss: 1.9302, Perplexity: 6.89053\n",
      "Epoch [3/3], Step [15800/41412], Loss: 2.1992, Perplexity: 9.01750\n",
      "Epoch [3/3], Step [15900/41412], Loss: 2.9705, Perplexity: 19.5013\n",
      "Epoch [3/3], Step [16000/41412], Loss: 1.8992, Perplexity: 6.68095\n",
      "Epoch [3/3], Step [16100/41412], Loss: 1.8256, Perplexity: 6.20679\n",
      "Epoch [3/3], Step [16200/41412], Loss: 1.7900, Perplexity: 5.98941\n",
      "Epoch [3/3], Step [16300/41412], Loss: 1.7762, Perplexity: 5.90757\n",
      "Epoch [3/3], Step [16400/41412], Loss: 2.5496, Perplexity: 12.8020\n",
      "Epoch [3/3], Step [16500/41412], Loss: 1.8346, Perplexity: 6.26241\n",
      "Epoch [3/3], Step [16600/41412], Loss: 1.6691, Perplexity: 5.30779\n",
      "Epoch [3/3], Step [16700/41412], Loss: 1.9018, Perplexity: 6.69772\n",
      "Epoch [3/3], Step [16800/41412], Loss: 2.9060, Perplexity: 18.2835\n",
      "Epoch [3/3], Step [16900/41412], Loss: 1.5553, Perplexity: 4.73658\n",
      "Epoch [3/3], Step [17000/41412], Loss: 2.1669, Perplexity: 8.73155\n",
      "Epoch [3/3], Step [17100/41412], Loss: 2.6014, Perplexity: 13.4831\n",
      "Epoch [3/3], Step [17200/41412], Loss: 2.5226, Perplexity: 12.4616\n",
      "Epoch [3/3], Step [17300/41412], Loss: 1.9867, Perplexity: 7.29176\n",
      "Epoch [3/3], Step [17400/41412], Loss: 2.2050, Perplexity: 9.07026\n",
      "Epoch [3/3], Step [17500/41412], Loss: 2.0277, Perplexity: 7.59683\n",
      "Epoch [3/3], Step [17600/41412], Loss: 2.0378, Perplexity: 7.67405\n",
      "Epoch [3/3], Step [17700/41412], Loss: 2.4598, Perplexity: 11.7029\n",
      "Epoch [3/3], Step [17800/41412], Loss: 1.8705, Perplexity: 6.49148\n",
      "Epoch [3/3], Step [17900/41412], Loss: 1.9736, Perplexity: 7.19658\n",
      "Epoch [3/3], Step [18000/41412], Loss: 2.2640, Perplexity: 9.62100\n",
      "Epoch [3/3], Step [18100/41412], Loss: 1.4919, Perplexity: 4.44564\n",
      "Epoch [3/3], Step [18200/41412], Loss: 1.9235, Perplexity: 6.84493\n",
      "Epoch [3/3], Step [18300/41412], Loss: 2.4731, Perplexity: 11.8591\n",
      "Epoch [3/3], Step [18400/41412], Loss: 1.9879, Perplexity: 7.30046\n",
      "Epoch [3/3], Step [18500/41412], Loss: 1.4888, Perplexity: 4.43161\n",
      "Epoch [3/3], Step [18600/41412], Loss: 1.9042, Perplexity: 6.71421\n",
      "Epoch [3/3], Step [18700/41412], Loss: 2.0614, Perplexity: 7.85677\n",
      "Epoch [3/3], Step [18800/41412], Loss: 1.8842, Perplexity: 6.58105\n",
      "Epoch [3/3], Step [18900/41412], Loss: 2.1799, Perplexity: 8.84533\n",
      "Epoch [3/3], Step [19000/41412], Loss: 1.9137, Perplexity: 6.77805\n",
      "Epoch [3/3], Step [19100/41412], Loss: 1.8720, Perplexity: 6.50117\n",
      "Epoch [3/3], Step [19200/41412], Loss: 2.2357, Perplexity: 9.35300\n",
      "Epoch [3/3], Step [19300/41412], Loss: 2.4374, Perplexity: 11.4429\n",
      "Epoch [3/3], Step [19400/41412], Loss: 2.0385, Perplexity: 7.67891\n",
      "Epoch [3/3], Step [19500/41412], Loss: 2.0165, Perplexity: 7.51210\n",
      "Epoch [3/3], Step [19600/41412], Loss: 2.0179, Perplexity: 7.52298\n",
      "Epoch [3/3], Step [19700/41412], Loss: 1.8653, Perplexity: 6.45767\n",
      "Epoch [3/3], Step [19800/41412], Loss: 2.3595, Perplexity: 10.5859\n",
      "Epoch [3/3], Step [19900/41412], Loss: 2.2441, Perplexity: 9.43155\n",
      "Epoch [3/3], Step [20000/41412], Loss: 1.6741, Perplexity: 5.33418\n",
      "Epoch [3/3], Step [20100/41412], Loss: 2.2705, Perplexity: 9.68427\n",
      "Epoch [3/3], Step [20200/41412], Loss: 2.8366, Perplexity: 17.0584\n",
      "Epoch [3/3], Step [20300/41412], Loss: 2.1580, Perplexity: 8.65388\n",
      "Epoch [3/3], Step [20400/41412], Loss: 2.2956, Perplexity: 9.93093\n",
      "Epoch [3/3], Step [20500/41412], Loss: 1.7226, Perplexity: 5.59929\n",
      "Epoch [3/3], Step [20600/41412], Loss: 1.8123, Perplexity: 6.12475\n",
      "Epoch [3/3], Step [20700/41412], Loss: 2.1578, Perplexity: 8.65215\n",
      "Epoch [3/3], Step [20800/41412], Loss: 1.6021, Perplexity: 4.96369\n",
      "Epoch [3/3], Step [20900/41412], Loss: 1.8860, Perplexity: 6.59270\n",
      "Epoch [3/3], Step [21000/41412], Loss: 2.0898, Perplexity: 8.08358\n",
      "Epoch [3/3], Step [21100/41412], Loss: 2.4089, Perplexity: 11.1222\n",
      "Epoch [3/3], Step [21200/41412], Loss: 2.4138, Perplexity: 11.1768\n",
      "Epoch [3/3], Step [21300/41412], Loss: 2.2114, Perplexity: 9.12890\n",
      "Epoch [3/3], Step [21400/41412], Loss: 2.4524, Perplexity: 11.6161\n",
      "Epoch [3/3], Step [21500/41412], Loss: 1.5728, Perplexity: 4.82004\n",
      "Epoch [3/3], Step [21600/41412], Loss: 2.1854, Perplexity: 8.89383\n",
      "Epoch [3/3], Step [21700/41412], Loss: 1.9392, Perplexity: 6.95297\n",
      "Epoch [3/3], Step [21800/41412], Loss: 2.1136, Perplexity: 8.27792\n",
      "Epoch [3/3], Step [21900/41412], Loss: 2.1186, Perplexity: 8.31950\n",
      "Epoch [3/3], Step [22000/41412], Loss: 1.8240, Perplexity: 6.19664\n",
      "Epoch [3/3], Step [22100/41412], Loss: 1.6923, Perplexity: 5.43179\n",
      "Epoch [3/3], Step [22200/41412], Loss: 1.9849, Perplexity: 7.27854\n",
      "Epoch [3/3], Step [22300/41412], Loss: 2.1391, Perplexity: 8.49200\n",
      "Epoch [3/3], Step [22400/41412], Loss: 2.7142, Perplexity: 15.0922\n",
      "Epoch [3/3], Step [22500/41412], Loss: 2.0103, Perplexity: 7.46596\n",
      "Epoch [3/3], Step [22600/41412], Loss: 2.0644, Perplexity: 7.88087\n",
      "Epoch [3/3], Step [22700/41412], Loss: 1.5720, Perplexity: 4.81623\n",
      "Epoch [3/3], Step [22800/41412], Loss: 1.7206, Perplexity: 5.58786\n",
      "Epoch [3/3], Step [22900/41412], Loss: 1.7861, Perplexity: 5.96625\n",
      "Epoch [3/3], Step [23000/41412], Loss: 1.8476, Perplexity: 6.34434\n",
      "Epoch [3/3], Step [23100/41412], Loss: 2.6716, Perplexity: 14.4632\n",
      "Epoch [3/3], Step [23200/41412], Loss: 2.3093, Perplexity: 10.0674\n",
      "Epoch [3/3], Step [23300/41412], Loss: 2.1921, Perplexity: 8.95425\n",
      "Epoch [3/3], Step [23400/41412], Loss: 3.0088, Perplexity: 20.2632\n",
      "Epoch [3/3], Step [23500/41412], Loss: 2.6974, Perplexity: 14.8412\n",
      "Epoch [3/3], Step [23600/41412], Loss: 2.3242, Perplexity: 10.2183\n",
      "Epoch [3/3], Step [23700/41412], Loss: 2.3556, Perplexity: 10.5441\n",
      "Epoch [3/3], Step [23800/41412], Loss: 1.7865, Perplexity: 5.96846\n",
      "Epoch [3/3], Step [23900/41412], Loss: 1.9971, Perplexity: 7.36794\n",
      "Epoch [3/3], Step [24000/41412], Loss: 2.0281, Perplexity: 7.59987\n",
      "Epoch [3/3], Step [24100/41412], Loss: 1.9071, Perplexity: 6.73333\n",
      "Epoch [3/3], Step [24200/41412], Loss: 1.8016, Perplexity: 6.05931\n",
      "Epoch [3/3], Step [24300/41412], Loss: 1.7269, Perplexity: 5.62319\n",
      "Epoch [3/3], Step [24400/41412], Loss: 2.4877, Perplexity: 12.0336\n",
      "Epoch [3/3], Step [24500/41412], Loss: 1.5953, Perplexity: 4.93000\n",
      "Epoch [3/3], Step [24600/41412], Loss: 2.0279, Perplexity: 7.59839\n",
      "Epoch [3/3], Step [24700/41412], Loss: 2.0635, Perplexity: 7.87349\n",
      "Epoch [3/3], Step [24800/41412], Loss: 2.4931, Perplexity: 12.0987\n",
      "Epoch [3/3], Step [24900/41412], Loss: 2.1616, Perplexity: 8.68483\n",
      "Epoch [3/3], Step [25000/41412], Loss: 1.9008, Perplexity: 6.69131\n",
      "Epoch [3/3], Step [25100/41412], Loss: 1.6479, Perplexity: 5.19619\n",
      "Epoch [3/3], Step [25200/41412], Loss: 2.2355, Perplexity: 9.35148\n",
      "Epoch [3/3], Step [25300/41412], Loss: 1.6224, Perplexity: 5.06539\n",
      "Epoch [3/3], Step [25400/41412], Loss: 1.7668, Perplexity: 5.85239\n",
      "Epoch [3/3], Step [25500/41412], Loss: 2.5602, Perplexity: 12.9388\n",
      "Epoch [3/3], Step [25600/41412], Loss: 2.7631, Perplexity: 15.8494\n",
      "Epoch [3/3], Step [25700/41412], Loss: 2.0205, Perplexity: 7.54183\n",
      "Epoch [3/3], Step [25800/41412], Loss: 1.6641, Perplexity: 5.28081\n",
      "Epoch [3/3], Step [25900/41412], Loss: 1.7571, Perplexity: 5.79556\n",
      "Epoch [3/3], Step [26000/41412], Loss: 2.0084, Perplexity: 7.45150\n",
      "Epoch [3/3], Step [26100/41412], Loss: 1.6865, Perplexity: 5.40033\n",
      "Epoch [3/3], Step [26200/41412], Loss: 1.8444, Perplexity: 6.32445\n",
      "Epoch [3/3], Step [26300/41412], Loss: 2.2149, Perplexity: 9.16082\n",
      "Epoch [3/3], Step [26400/41412], Loss: 2.5893, Perplexity: 13.3199\n",
      "Epoch [3/3], Step [26500/41412], Loss: 2.3196, Perplexity: 10.1713\n",
      "Epoch [3/3], Step [26600/41412], Loss: 2.0583, Perplexity: 7.83254\n",
      "Epoch [3/3], Step [26700/41412], Loss: 1.6678, Perplexity: 5.30039\n",
      "Epoch [3/3], Step [26800/41412], Loss: 2.0326, Perplexity: 7.63368\n",
      "Epoch [3/3], Step [26900/41412], Loss: 2.0866, Perplexity: 8.05752\n",
      "Epoch [3/3], Step [27000/41412], Loss: 1.8893, Perplexity: 6.61448\n",
      "Epoch [3/3], Step [27100/41412], Loss: 2.2449, Perplexity: 9.43920\n",
      "Epoch [3/3], Step [27200/41412], Loss: 1.6951, Perplexity: 5.44742\n",
      "Epoch [3/3], Step [27300/41412], Loss: 2.2812, Perplexity: 9.78856\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/3], Step [27400/41412], Loss: 2.1142, Perplexity: 8.28294\n",
      "Epoch [3/3], Step [27500/41412], Loss: 1.9672, Perplexity: 7.15044\n",
      "Epoch [3/3], Step [27600/41412], Loss: 2.2300, Perplexity: 9.29964\n",
      "Epoch [3/3], Step [27700/41412], Loss: 2.1323, Perplexity: 8.43399\n",
      "Epoch [3/3], Step [27800/41412], Loss: 1.7587, Perplexity: 5.80470\n",
      "Epoch [3/3], Step [27900/41412], Loss: 1.3186, Perplexity: 3.73817\n",
      "Epoch [3/3], Step [28000/41412], Loss: 2.2739, Perplexity: 9.71741\n",
      "Epoch [3/3], Step [28100/41412], Loss: 1.9407, Perplexity: 6.96372\n",
      "Epoch [3/3], Step [28200/41412], Loss: 1.8650, Perplexity: 6.45594\n",
      "Epoch [3/3], Step [28300/41412], Loss: 1.8957, Perplexity: 6.65710\n",
      "Epoch [3/3], Step [28400/41412], Loss: 2.0752, Perplexity: 7.96603\n",
      "Epoch [3/3], Step [28500/41412], Loss: 2.5758, Perplexity: 13.1418\n",
      "Epoch [3/3], Step [28600/41412], Loss: 2.3990, Perplexity: 11.0127\n",
      "Epoch [3/3], Step [28700/41412], Loss: 1.9763, Perplexity: 7.21611\n",
      "Epoch [3/3], Step [28800/41412], Loss: 2.2875, Perplexity: 9.85002\n",
      "Epoch [3/3], Step [28900/41412], Loss: 1.7890, Perplexity: 5.98351\n",
      "Epoch [3/3], Step [29000/41412], Loss: 1.7591, Perplexity: 5.80739\n",
      "Epoch [3/3], Step [29100/41412], Loss: 2.1314, Perplexity: 8.42657\n",
      "Epoch [3/3], Step [29200/41412], Loss: 2.0104, Perplexity: 7.46647\n",
      "Epoch [3/3], Step [29300/41412], Loss: 1.6760, Perplexity: 5.34434\n",
      "Epoch [3/3], Step [29400/41412], Loss: 2.2779, Perplexity: 9.75596\n",
      "Epoch [3/3], Step [29500/41412], Loss: 2.0330, Perplexity: 7.63699\n",
      "Epoch [3/3], Step [29600/41412], Loss: 1.9613, Perplexity: 7.10856\n",
      "Epoch [3/3], Step [29700/41412], Loss: 1.7983, Perplexity: 6.03971\n",
      "Epoch [3/3], Step [29800/41412], Loss: 1.6782, Perplexity: 5.35572\n",
      "Epoch [3/3], Step [29900/41412], Loss: 2.2687, Perplexity: 9.66720\n",
      "Epoch [3/3], Step [30000/41412], Loss: 2.4887, Perplexity: 12.0450\n",
      "Epoch [3/3], Step [30100/41412], Loss: 2.3894, Perplexity: 10.9065\n",
      "Epoch [3/3], Step [30200/41412], Loss: 2.4312, Perplexity: 11.3722\n",
      "Epoch [3/3], Step [30300/41412], Loss: 2.4880, Perplexity: 12.0373\n",
      "Epoch [3/3], Step [30400/41412], Loss: 1.9785, Perplexity: 7.23208\n",
      "Epoch [3/3], Step [30500/41412], Loss: 2.0168, Perplexity: 7.51392\n",
      "Epoch [3/3], Step [30600/41412], Loss: 2.0500, Perplexity: 7.76787\n",
      "Epoch [3/3], Step [30700/41412], Loss: 1.9649, Perplexity: 7.13433\n",
      "Epoch [3/3], Step [30800/41412], Loss: 2.0016, Perplexity: 7.40129\n",
      "Epoch [3/3], Step [30900/41412], Loss: 2.1326, Perplexity: 8.43692\n",
      "Epoch [3/3], Step [31000/41412], Loss: 2.1798, Perplexity: 8.84471\n",
      "Epoch [3/3], Step [31100/41412], Loss: 1.8289, Perplexity: 6.22706\n",
      "Epoch [3/3], Step [31200/41412], Loss: 2.0494, Perplexity: 7.76367\n",
      "Epoch [3/3], Step [31300/41412], Loss: 2.1909, Perplexity: 8.94293\n",
      "Epoch [3/3], Step [31400/41412], Loss: 1.9922, Perplexity: 7.33201\n",
      "Epoch [3/3], Step [31500/41412], Loss: 1.9157, Perplexity: 6.79194\n",
      "Epoch [3/3], Step [31600/41412], Loss: 2.1278, Perplexity: 8.39610\n",
      "Epoch [3/3], Step [31700/41412], Loss: 2.1436, Perplexity: 8.53035\n",
      "Epoch [3/3], Step [31800/41412], Loss: 2.2247, Perplexity: 9.25079\n",
      "Epoch [3/3], Step [31900/41412], Loss: 1.7168, Perplexity: 5.56685\n",
      "Epoch [3/3], Step [32000/41412], Loss: 2.4239, Perplexity: 11.2893\n",
      "Epoch [3/3], Step [32100/41412], Loss: 2.2076, Perplexity: 9.09419\n",
      "Epoch [3/3], Step [32200/41412], Loss: 2.0560, Perplexity: 7.81457\n",
      "Epoch [3/3], Step [32300/41412], Loss: 1.9518, Perplexity: 7.04161\n",
      "Epoch [3/3], Step [32400/41412], Loss: 1.6702, Perplexity: 5.31311\n",
      "Epoch [3/3], Step [32500/41412], Loss: 2.2197, Perplexity: 9.20504\n",
      "Epoch [3/3], Step [32600/41412], Loss: 1.8528, Perplexity: 6.37785\n",
      "Epoch [3/3], Step [32700/41412], Loss: 2.1712, Perplexity: 8.76860\n",
      "Epoch [3/3], Step [32800/41412], Loss: 2.1120, Perplexity: 8.26488\n",
      "Epoch [3/3], Step [32900/41412], Loss: 1.9648, Perplexity: 7.13371\n",
      "Epoch [3/3], Step [33000/41412], Loss: 2.2483, Perplexity: 9.47176\n",
      "Epoch [3/3], Step [33100/41412], Loss: 1.9691, Perplexity: 7.16435\n",
      "Epoch [3/3], Step [33200/41412], Loss: 1.7748, Perplexity: 5.89892\n",
      "Epoch [3/3], Step [33300/41412], Loss: 2.2114, Perplexity: 9.12896\n",
      "Epoch [3/3], Step [33400/41412], Loss: 2.0560, Perplexity: 7.81482\n",
      "Epoch [3/3], Step [33500/41412], Loss: 1.6667, Perplexity: 5.29481\n",
      "Epoch [3/3], Step [33600/41412], Loss: 1.6218, Perplexity: 5.06227\n",
      "Epoch [3/3], Step [33700/41412], Loss: 1.7752, Perplexity: 5.90170\n",
      "Epoch [3/3], Step [33800/41412], Loss: 1.9673, Perplexity: 7.15109\n",
      "Epoch [3/3], Step [33900/41412], Loss: 1.7693, Perplexity: 5.86695\n",
      "Epoch [3/3], Step [34000/41412], Loss: 1.8899, Perplexity: 6.61872\n",
      "Epoch [3/3], Step [34100/41412], Loss: 3.0448, Perplexity: 21.0052\n",
      "Epoch [3/3], Step [34200/41412], Loss: 2.0222, Perplexity: 7.55515\n",
      "Epoch [3/3], Step [34300/41412], Loss: 2.0511, Perplexity: 7.77671\n",
      "Epoch [3/3], Step [34400/41412], Loss: 1.8765, Perplexity: 6.53075\n",
      "Epoch [3/3], Step [34500/41412], Loss: 2.1206, Perplexity: 8.33621\n",
      "Epoch [3/3], Step [34600/41412], Loss: 1.8638, Perplexity: 6.44839\n",
      "Epoch [3/3], Step [34700/41412], Loss: 1.8233, Perplexity: 6.19223\n",
      "Epoch [3/3], Step [34800/41412], Loss: 2.6030, Perplexity: 13.5041\n",
      "Epoch [3/3], Step [34900/41412], Loss: 2.0021, Perplexity: 7.40498\n",
      "Epoch [3/3], Step [35000/41412], Loss: 1.5821, Perplexity: 4.86498\n",
      "Epoch [3/3], Step [35100/41412], Loss: 2.2272, Perplexity: 9.27386\n",
      "Epoch [3/3], Step [35200/41412], Loss: 1.9834, Perplexity: 7.267531\n",
      "Epoch [3/3], Step [35300/41412], Loss: 2.3701, Perplexity: 10.6980\n",
      "Epoch [3/3], Step [35400/41412], Loss: 1.6232, Perplexity: 5.06916\n",
      "Epoch [3/3], Step [35500/41412], Loss: 1.9816, Perplexity: 7.25453\n",
      "Epoch [3/3], Step [35600/41412], Loss: 2.0778, Perplexity: 7.98703\n",
      "Epoch [3/3], Step [35700/41412], Loss: 1.7484, Perplexity: 5.74558\n",
      "Epoch [3/3], Step [35800/41412], Loss: 1.5736, Perplexity: 4.82400\n",
      "Epoch [3/3], Step [35900/41412], Loss: 1.9954, Perplexity: 7.35559\n",
      "Epoch [3/3], Step [36000/41412], Loss: 1.8360, Perplexity: 6.27161\n",
      "Epoch [3/3], Step [36100/41412], Loss: 1.8636, Perplexity: 6.44718\n",
      "Epoch [3/3], Step [36200/41412], Loss: 2.3916, Perplexity: 10.9312\n",
      "Epoch [3/3], Step [36300/41412], Loss: 2.6865, Perplexity: 14.6802\n",
      "Epoch [3/3], Step [36400/41412], Loss: 1.6418, Perplexity: 5.16458\n",
      "Epoch [3/3], Step [36500/41412], Loss: 2.2809, Perplexity: 9.78559\n",
      "Epoch [3/3], Step [36600/41412], Loss: 2.4278, Perplexity: 11.3344\n",
      "Epoch [3/3], Step [36700/41412], Loss: 1.8473, Perplexity: 6.34242\n",
      "Epoch [3/3], Step [36800/41412], Loss: 1.4604, Perplexity: 4.30795\n",
      "Epoch [3/3], Step [36900/41412], Loss: 2.7016, Perplexity: 14.9036\n",
      "Epoch [3/3], Step [37000/41412], Loss: 1.9277, Perplexity: 6.87377\n",
      "Epoch [3/3], Step [37100/41412], Loss: 2.0342, Perplexity: 7.64582\n",
      "Epoch [3/3], Step [37200/41412], Loss: 2.3715, Perplexity: 10.7131\n",
      "Epoch [3/3], Step [37300/41412], Loss: 2.2150, Perplexity: 9.16179\n",
      "Epoch [3/3], Step [37400/41412], Loss: 2.3808, Perplexity: 10.8135\n",
      "Epoch [3/3], Step [37500/41412], Loss: 2.0934, Perplexity: 8.11261\n",
      "Epoch [3/3], Step [37600/41412], Loss: 1.9804, Perplexity: 7.24558\n",
      "Epoch [3/3], Step [37700/41412], Loss: 2.0478, Perplexity: 7.75094\n",
      "Epoch [3/3], Step [37800/41412], Loss: 2.2318, Perplexity: 9.31648\n",
      "Epoch [3/3], Step [37900/41412], Loss: 1.6545, Perplexity: 5.23042\n",
      "Epoch [3/3], Step [38000/41412], Loss: 1.7111, Perplexity: 5.53500\n",
      "Epoch [3/3], Step [38100/41412], Loss: 2.0061, Perplexity: 7.43402\n",
      "Epoch [3/3], Step [38200/41412], Loss: 1.5563, Perplexity: 4.74147\n",
      "Epoch [3/3], Step [38300/41412], Loss: 1.7765, Perplexity: 5.90925\n",
      "Epoch [3/3], Step [38400/41412], Loss: 2.4437, Perplexity: 11.5152\n",
      "Epoch [3/3], Step [38500/41412], Loss: 1.9429, Perplexity: 6.97911\n",
      "Epoch [3/3], Step [38600/41412], Loss: 1.9400, Perplexity: 6.95865\n",
      "Epoch [3/3], Step [38700/41412], Loss: 2.0342, Perplexity: 7.64617\n",
      "Epoch [3/3], Step [38800/41412], Loss: 2.0386, Perplexity: 7.67990\n",
      "Epoch [3/3], Step [38900/41412], Loss: 1.7669, Perplexity: 5.85297\n",
      "Epoch [3/3], Step [39000/41412], Loss: 1.8614, Perplexity: 6.43283\n",
      "Epoch [3/3], Step [39100/41412], Loss: 2.4316, Perplexity: 11.3769\n",
      "Epoch [3/3], Step [39200/41412], Loss: 1.7477, Perplexity: 5.74120\n",
      "Epoch [3/3], Step [39300/41412], Loss: 2.0318, Perplexity: 7.62752\n",
      "Epoch [3/3], Step [39400/41412], Loss: 2.4378, Perplexity: 11.4475\n",
      "Epoch [3/3], Step [39500/41412], Loss: 2.3261, Perplexity: 10.2377\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/3], Step [39600/41412], Loss: 2.0266, Perplexity: 7.58836\n",
      "Epoch [3/3], Step [39700/41412], Loss: 1.9294, Perplexity: 6.88558\n",
      "Epoch [3/3], Step [39800/41412], Loss: 2.2368, Perplexity: 9.36312\n",
      "Epoch [3/3], Step [39900/41412], Loss: 1.8825, Perplexity: 6.57015\n",
      "Epoch [3/3], Step [40000/41412], Loss: 2.4890, Perplexity: 12.0496\n",
      "Epoch [3/3], Step [40100/41412], Loss: 2.0802, Perplexity: 8.00620\n",
      "Epoch [3/3], Step [40200/41412], Loss: 1.5672, Perplexity: 4.79316\n",
      "Epoch [3/3], Step [40300/41412], Loss: 2.1272, Perplexity: 8.39112\n",
      "Epoch [3/3], Step [40400/41412], Loss: 2.2949, Perplexity: 9.92331\n",
      "Epoch [3/3], Step [40500/41412], Loss: 2.3144, Perplexity: 10.1186\n",
      "Epoch [3/3], Step [40600/41412], Loss: 2.2538, Perplexity: 9.52354\n",
      "Epoch [3/3], Step [40700/41412], Loss: 2.2503, Perplexity: 9.49099\n",
      "Epoch [3/3], Step [40800/41412], Loss: 2.5659, Perplexity: 13.0125\n",
      "Epoch [3/3], Step [40900/41412], Loss: 2.5307, Perplexity: 12.5629\n",
      "Epoch [3/3], Step [41000/41412], Loss: 1.9051, Perplexity: 6.71990\n",
      "Epoch [3/3], Step [41100/41412], Loss: 1.9372, Perplexity: 6.93900\n",
      "Epoch [3/3], Step [41200/41412], Loss: 2.0392, Perplexity: 7.68460\n",
      "Epoch [3/3], Step [41300/41412], Loss: 2.5364, Perplexity: 12.6336\n",
      "Epoch [3/3], Step [41400/41412], Loss: 2.1342, Perplexity: 8.45003\n",
      "Epoch [3/3], Step [41412/41412], Loss: 2.1789, Perplexity: 8.83662"
     ]
    }
   ],
   "source": [
    "import torch.utils.data as data\n",
    "import numpy as np\n",
    "import os\n",
    "import requests\n",
    "import time\n",
    "\n",
    "# temporary\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from model import EncoderCNN, DecoderRNN\n",
    "\n",
    "# Open the training log file.\n",
    "f = open(log_file, 'w')\n",
    "\n",
    "## running the training locally\n",
    "#old_time = time.time()\n",
    "#response = requests.request(\"GET\", \n",
    "#                            \"http://metadata.google.internal/computeMetadata/v1/instance/attributes/keep_alive_token\", \n",
    "#                            headers={\"Metadata-Flavor\":\"Google\"})\n",
    "\n",
    "\n",
    "for epoch in range(1, num_epochs+1):\n",
    "\n",
    "    for i_step in range(1, total_step+1):\n",
    "        \n",
    "        ## running the training locally\n",
    "        #if time.time() - old_time > 60:\n",
    "        #    old_time = time.time()\n",
    "        #    requests.request(\"POST\", \n",
    "        #                     \"https://nebula.udacity.com/api/v1/remote/keep-alive\", \n",
    "        #                     headers={'Authorization': \"STAR \" + response.text})\n",
    "\n",
    "        # Randomly sample a caption length, and sample indices with that length.\n",
    "        indices = data_loader.dataset.get_train_indices()\n",
    "        # Create and assign a batch sampler to retrieve a batch with the sampled indices.\n",
    "        new_sampler = data.sampler.SubsetRandomSampler(indices=indices)\n",
    "        data_loader.batch_sampler.sampler = new_sampler\n",
    "\n",
    "        # Obtain the batch.\n",
    "        images, captions = next(iter(data_loader))\n",
    "\n",
    "        # Move batch of images and captions to GPU if CUDA is available.\n",
    "        images = images.to(device)\n",
    "        captions = captions.to(device)\n",
    "\n",
    "        # Zero the gradients.\n",
    "        decoder.zero_grad()\n",
    "        encoder.zero_grad()\n",
    "\n",
    "        # Pass the inputs through the CNN-RNN model.\n",
    "        features = encoder(images)\n",
    "        outputs = decoder(features, captions)\n",
    "\n",
    "        # Calculate the batch loss.\n",
    "        loss = criterion(outputs.contiguous().view(-1, vocab_size), captions.contiguous().view(-1))\n",
    "\n",
    "        # Backward pass.\n",
    "        loss.backward(retain_graph=True)\n",
    "\n",
    "        # Update the parameters in the optimizer.\n",
    "        optimizer.step()\n",
    "\n",
    "        # Get training statistics.\n",
    "        stats = 'Epoch [%d/%d], Step [%d/%d], Loss: %.4f, Perplexity: %5.4f' % (epoch, num_epochs, i_step, total_step, loss.item(), np.exp(loss.item()))\n",
    "\n",
    "        # Print training statistics (on same line).\n",
    "        print('\\r' + stats, end=\"\")\n",
    "        sys.stdout.flush()\n",
    "\n",
    "        # Print training statistics to file.\n",
    "        f.write(stats + '\\n')\n",
    "        f.flush()\n",
    "\n",
    "        # Print training statistics (on different line).\n",
    "        if i_step % print_every == 0:\n",
    "            print('\\r' + stats)\n",
    "\n",
    "    # Save the weights.\n",
    "    if epoch % save_every == 0:\n",
    "        torch.save(decoder.state_dict(), os.path.join('./models', 'decoder-%d.pkl' % epoch))\n",
    "        torch.save(encoder.state_dict(), os.path.join('./models', 'encoder-%d.pkl' % epoch))\n",
    "\n",
    "# Close the training log file.\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step3'></a>\n",
    "## Step 3: (Optional) Validate your Model\n",
    "\n",
    "To assess potential overfitting, one approach is to assess performance on a validation set.  If you decide to do this **optional** task, you are required to first complete all of the steps in the next notebook in the sequence (**3_Inference.ipynb**); as part of that notebook, you will write and test code (specifically, the `sample` method in the `DecoderRNN` class) that uses your RNN decoder to generate captions.  That code will prove incredibly useful here. \n",
    "\n",
    "If you decide to validate your model, please do not edit the data loader in **data_loader.py**.  Instead, create a new file named **data_loader_val.py** containing the code for obtaining the data loader for the validation data.  You can access:\n",
    "- the validation images at filepath `'/opt/cocoapi/images/train2014/'`, and\n",
    "- the validation image caption annotation file at filepath `'/opt/cocoapi/annotations/captions_val2014.json'`.\n",
    "\n",
    "The suggested approach to validating your model involves creating a json file such as [this one](https://github.com/cocodataset/cocoapi/blob/master/results/captions_val2014_fakecap_results.json) containing your model's predicted captions for the validation images.  Then, you can write your own script or use one that you [find online](https://github.com/tylin/coco-caption) to calculate the BLEU score of your model.  You can read more about the BLEU score, along with other evaluation metrics (such as TEOR and Cider) in section 4.1 of [this paper](https://arxiv.org/pdf/1411.4555.pdf).  For more information about how to use the annotation file, check out the [website](http://cocodataset.org/#download) for the COCO dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Optional) TODO: Validate your model."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
