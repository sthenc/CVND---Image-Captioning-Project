{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computer Vision Nanodegree\n",
    "\n",
    "## Project: Image Captioning\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, you will train your CNN-RNN model.  \n",
    "\n",
    "You are welcome and encouraged to try out many different architectures and hyperparameters when searching for a good model.\n",
    "\n",
    "This does have the potential to make the project quite messy!  Before submitting your project, make sure that you clean up:\n",
    "- the code you write in this notebook.  The notebook should describe how to train a single CNN-RNN architecture, corresponding to your final choice of hyperparameters.  You should structure the notebook so that the reviewer can replicate your results by running the code in this notebook.  \n",
    "- the output of the code cell in **Step 2**.  The output should show the output obtained when training the model from scratch.\n",
    "\n",
    "This notebook **will be graded**.  \n",
    "\n",
    "Feel free to use the links below to navigate the notebook:\n",
    "- [Step 1](#step1): Training Setup\n",
    "- [Step 2](#step2): Train your Model\n",
    "- [Step 3](#step3): (Optional) Validate your Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step1'></a>\n",
    "## Step 1: Training Setup\n",
    "\n",
    "In this step of the notebook, you will customize the training of your CNN-RNN model by specifying hyperparameters and setting other options that are important to the training procedure.  The values you set now will be used when training your model in **Step 2** below.\n",
    "\n",
    "You should only amend blocks of code that are preceded by a `TODO` statement.  **Any code blocks that are not preceded by a `TODO` statement should not be modified**.\n",
    "\n",
    "### Task #1\n",
    "\n",
    "Begin by setting the following variables:\n",
    "- `batch_size` - the batch size of each training batch.  It is the number of image-caption pairs used to amend the model weights in each training step. \n",
    "- `vocab_threshold` - the minimum word count threshold.  Note that a larger threshold will result in a smaller vocabulary, whereas a smaller threshold will include rarer words and result in a larger vocabulary.  \n",
    "- `vocab_from_file` - a Boolean that decides whether to load the vocabulary from file. \n",
    "- `embed_size` - the dimensionality of the image and word embeddings.  \n",
    "- `hidden_size` - the number of features in the hidden state of the RNN decoder.  \n",
    "- `num_epochs` - the number of epochs to train the model.  We recommend that you set `num_epochs=3`, but feel free to increase or decrease this number as you wish.  [This paper](https://arxiv.org/pdf/1502.03044.pdf) trained a captioning model on a single state-of-the-art GPU for 3 days, but you'll soon see that you can get reasonable results in a matter of a few hours!  (_But of course, if you want your model to compete with current research, you will have to train for much longer._)\n",
    "- `save_every` - determines how often to save the model weights.  We recommend that you set `save_every=1`, to save the model weights after each epoch.  This way, after the `i`th epoch, the encoder and decoder weights will be saved in the `models/` folder as `encoder-i.pkl` and `decoder-i.pkl`, respectively.\n",
    "- `print_every` - determines how often to print the batch loss to the Jupyter notebook while training.  Note that you **will not** observe a monotonic decrease in the loss function while training - this is perfectly fine and completely expected!  You are encouraged to keep this at its default value of `100` to avoid clogging the notebook, but feel free to change it.\n",
    "- `log_file` - the name of the text file containing - for every step - how the loss and perplexity evolved during training.\n",
    "\n",
    "If you're not sure where to begin to set some of the values above, you can peruse [this paper](https://arxiv.org/pdf/1502.03044.pdf) and [this paper](https://arxiv.org/pdf/1411.4555.pdf) for useful guidance!  **To avoid spending too long on this notebook**, you are encouraged to consult these suggested research papers to obtain a strong initial guess for which hyperparameters are likely to work best.  Then, train a single model, and proceed to the next notebook (**3_Inference.ipynb**).  If you are unhappy with your performance, you can return to this notebook to tweak the hyperparameters (and/or the architecture in **model.py**) and re-train your model.\n",
    "\n",
    "### Question 1\n",
    "\n",
    "**Question:** Describe your CNN-RNN architecture in detail.  With this architecture in mind, how did you select the values of the variables in Task 1?  If you consulted a research paper detailing a successful implementation of an image captioning model, please provide the reference.\n",
    "\n",
    "**Answer:** I based my architecture on the architecture in the google \"Show and Tell\" [paper](https://arxiv.org/pdf/1411.4555.pdf) , with the exception that the encoder is the default ResNet50. I experimented with adding batch normalization on the output but I had other problems so I didn't notice a difference. \n",
    "\n",
    "For the encoder I used an embedding layer, a single LSTM layer, dropout and an output Linear fully connected layer. I tried adding softmax to the output, but it didn't seem to make a difference. I also initialized the embeddings and hidden LSTM values, so I get reasonable results even after the 1. epoch. Embedding and Hidden layer size were set to 512, as this is what is used in the \"Show and tell\" [paper](https://arxiv.org/pdf/1411.4555.pdf).\n",
    "\n",
    "Batch_size was set left at default 10, reducing it to 1 slows down the training drastically, and I had some memory leak when trying batch_size=64 and it didn't seem faster (and had a massive memory leak). But these experiments were done while I still had bugs in my model.py, so now things would probably look different.   \n",
    "\n",
    "I left vocab_treshold and num_epochs at the recommended values.  \n",
    "\n",
    "### (Optional) Task #2\n",
    "\n",
    "Note that we have provided a recommended image transform `transform_train` for pre-processing the training images, but you are welcome (and encouraged!) to modify it as you wish.  When modifying this transform, keep in mind that:\n",
    "- the images in the dataset have varying heights and widths, and \n",
    "- if using a pre-trained model, you must perform the corresponding appropriate normalization.\n",
    "\n",
    "### Question 2\n",
    "\n",
    "**Question:** How did you select the transform in `transform_train`?  If you left the transform at its provided value, why do you think that it is a good choice for your CNN architecture?\n",
    "\n",
    "**Answer:** The default looks reasonable, although I have some doubts as to whether it makes sense to discard the data around the edge in this case, but I didn't investigate that further.\n",
    "\n",
    "### Task #3\n",
    "\n",
    "Next, you will specify a Python list containing the learnable parameters of the model.  For instance, if you decide to make all weights in the decoder trainable, but only want to train the weights in the embedding layer of the encoder, then you should set `params` to something like:\n",
    "```\n",
    "params = list(decoder.parameters()) + list(encoder.embed.parameters()) \n",
    "```\n",
    "\n",
    "### Question 3\n",
    "\n",
    "**Question:** How did you select the trainable parameters of your architecture?  Why do you think this is a good choice?\n",
    "\n",
    "**Answer:** I stayed with the defaults. This is a good choice because while the LSTM is in the initial phases of training it could mess up the CNN coefficients of the pre-trained encoder (one of the papers I read also mentioned this would happen). It also intuitively makes sense to only train the new uninitialized layers. Maybe a performance boost could be obtained if after training the network like this some fine-tuning was done on the CNN encoder. \n",
    "\n",
    "### Task #4\n",
    "\n",
    "Finally, you will select an [optimizer](http://pytorch.org/docs/master/optim.html#torch.optim.Optimizer).\n",
    "\n",
    "### Question 4\n",
    "\n",
    "**Question:** How did you select the optimizer used to train your model?\n",
    "\n",
    "**Answer:** I had a serious bug in my model optimization which caused SGD to not train it very quickly. I tried Adam after that, but for some reason (maybe too low dropout and bugs) it would always output the same sentence irrespective of the input data. So I went back to SGD with a higher learning rate and this succesfully trained the network with ok performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/sthenc/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary successfully loaded from vocab.pkl file!\n",
      "loading annotations into memory...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1851/414113 [00:00<00:45, 9075.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done (t=0.52s)\n",
      "creating index...\n",
      "index created!\n",
      "Obtaining caption lengths...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 414113/414113 [00:42<00:00, 9727.73it/s] \n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "import sys\n",
    "sys.path.append('/opt/cocoapi/PythonAPI')\n",
    "from pycocotools.coco import COCO\n",
    "from data_loader import get_loader\n",
    "from model import EncoderCNN, DecoderRNN\n",
    "import math\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "## TODO #1: Select appropriate values for the Python variables below.\n",
    "#batch_size=1\n",
    "batch_size=10\n",
    "#batch_size = 64          # batch size, we have 10GB of GPU memory, let's use it\n",
    "vocab_threshold = 5        # minimum word count threshold\n",
    "vocab_from_file = True    # if True, load existing vocab file\n",
    "embed_size = 512           # dimensionality of image and word embeddings\n",
    "hidden_size = 512          # number of features in hidden state of the RNN decoder\n",
    "num_epochs = 3             # number of training epochs\n",
    "save_every = 1             # determines frequency of saving model weights\n",
    "print_every = 100          # determines window for printing average loss\n",
    "log_file = 'training_log.txt'       # name of file with saved training loss and perplexity\n",
    "\n",
    "# (Optional) TODO #2: Amend the image transform below.\n",
    "transform_train = transforms.Compose([ \n",
    "    transforms.Resize(256),                          # smaller edge of image resized to 256\n",
    "    transforms.RandomCrop(224),                      # get 224x224 crop from random location\n",
    "    transforms.RandomHorizontalFlip(),               # horizontally flip image with probability=0.5\n",
    "    transforms.ToTensor(),                           # convert the PIL Image to a tensor\n",
    "    transforms.Normalize((0.485, 0.456, 0.406),      # normalize image for pre-trained model\n",
    "                         (0.229, 0.224, 0.225))])\n",
    "\n",
    "# Build data loader.\n",
    "data_loader = get_loader(transform=transform_train,\n",
    "                         mode='train',\n",
    "                         batch_size=batch_size,\n",
    "                         vocab_threshold=vocab_threshold,\n",
    "                         vocab_from_file=vocab_from_file)\n",
    "\n",
    "# The size of the vocabulary.\n",
    "vocab_size = len(data_loader.dataset.vocab)\n",
    "\n",
    "# Initialize the encoder and decoder. \n",
    "encoder = EncoderCNN(embed_size)\n",
    "decoder = DecoderRNN(embed_size, hidden_size, vocab_size, max_batch_size=batch_size)\n",
    "\n",
    "# Move models to GPU if CUDA is available. \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "encoder.to(device)\n",
    "decoder.to(device)\n",
    "\n",
    "# Define the loss function. \n",
    "criterion = nn.CrossEntropyLoss().cuda() if torch.cuda.is_available() else nn.CrossEntropyLoss()\n",
    "\n",
    "# TODO #3: Specify the learnable parameters of the model.\n",
    "params = list(decoder.parameters()) + list(encoder.embed.parameters()) + list(encoder.bn.parameters())\n",
    "\n",
    "# TODO #4: Define the optimizer.\n",
    "optimizer = torch.optim.SGD(params, lr=0.1)\n",
    "\n",
    "# this data is probably pretty sparse, and defaults are probably ok\n",
    "#http://ruder.io/optimizing-gradient-descent/\n",
    "#optimizer = torch.optim.Adam(params, lr=0.001)\n",
    "#optimizer = torch.optim.Adam(params)\n",
    "\n",
    "# Set the total number of training steps per epoch.\n",
    "total_step = math.ceil(len(data_loader.dataset.caption_lengths) / data_loader.batch_sampler.batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step2'></a>\n",
    "## Step 2: Train your Model\n",
    "\n",
    "Once you have executed the code cell in **Step 1**, the training procedure below should run without issue.  \n",
    "\n",
    "It is completely fine to leave the code cell below as-is without modifications to train your model.  However, if you would like to modify the code used to train the model below, you must ensure that your changes are easily parsed by your reviewer.  In other words, make sure to provide appropriate comments to describe how your code works!  \n",
    "\n",
    "You may find it useful to load saved weights to resume training.  In that case, note the names of the files containing the encoder and decoder weights that you'd like to load (`encoder_file` and `decoder_file`).  Then you can load the weights by using the lines below:\n",
    "\n",
    "```python\n",
    "# Load pre-trained weights before resuming training.\n",
    "encoder.load_state_dict(torch.load(os.path.join('./models', encoder_file)))\n",
    "decoder.load_state_dict(torch.load(os.path.join('./models', decoder_file)))\n",
    "```\n",
    "\n",
    "While trying out parameters, make sure to take extensive notes and record the settings that you used in your various training runs.  In particular, you don't want to encounter a situation where you've trained a model for several hours but can't remember what settings you used :).\n",
    "\n",
    "### A Note on Tuning Hyperparameters\n",
    "\n",
    "To figure out how well your model is doing, you can look at how the training loss and perplexity evolve during training - and for the purposes of this project, you are encouraged to amend the hyperparameters based on this information.  \n",
    "\n",
    "However, this will not tell you if your model is overfitting to the training data, and, unfortunately, overfitting is a problem that is commonly encountered when training image captioning models.  \n",
    "\n",
    "For this project, you need not worry about overfitting. **This project does not have strict requirements regarding the performance of your model**, and you just need to demonstrate that your model has learned **_something_** when you generate captions on the test data.  For now, we strongly encourage you to train your model for the suggested 3 epochs without worrying about performance; then, you should immediately transition to the next notebook in the sequence (**3_Inference.ipynb**) to see how your model performs on the test data.  If your model needs to be changed, you can come back to this notebook, amend hyperparameters (if necessary), and re-train the model.\n",
    "\n",
    "That said, if you would like to go above and beyond in this project, you can read about some approaches to minimizing overfitting in section 4.3.1 of [this paper](http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7505636).  In the next (optional) step of this notebook, we provide some guidance for assessing the performance on the validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Epoch [1/3], Step [100/41412], Loss: 5.7521, Perplexity: 314.8569\n",
      "Epoch [1/3], Step [200/41412], Loss: 6.2931, Perplexity: 540.8357\n",
      "Epoch [1/3], Step [300/41412], Loss: 5.6379, Perplexity: 280.86786\n",
      "Epoch [1/3], Step [400/41412], Loss: 5.6829, Perplexity: 293.8073\n",
      "Epoch [1/3], Step [500/41412], Loss: 5.0780, Perplexity: 160.4558\n",
      "Epoch [1/3], Step [600/41412], Loss: 5.3786, Perplexity: 216.7153\n",
      "Epoch [1/3], Step [700/41412], Loss: 4.8912, Perplexity: 133.1118\n",
      "Epoch [1/3], Step [800/41412], Loss: 4.8111, Perplexity: 122.8716\n",
      "Epoch [1/3], Step [900/41412], Loss: 5.4779, Perplexity: 239.3480\n",
      "Epoch [1/3], Step [1000/41412], Loss: 5.0323, Perplexity: 153.2923\n",
      "Epoch [1/3], Step [1100/41412], Loss: 4.6186, Perplexity: 101.3516\n",
      "Epoch [1/3], Step [1200/41412], Loss: 4.5781, Perplexity: 97.32837\n",
      "Epoch [1/3], Step [1300/41412], Loss: 4.5420, Perplexity: 93.87784\n",
      "Epoch [1/3], Step [1400/41412], Loss: 4.7758, Perplexity: 118.6056\n",
      "Epoch [1/3], Step [1500/41412], Loss: 4.8850, Perplexity: 132.2856\n",
      "Epoch [1/3], Step [1600/41412], Loss: 4.9503, Perplexity: 141.2145\n",
      "Epoch [1/3], Step [1700/41412], Loss: 5.0382, Perplexity: 154.1869\n",
      "Epoch [1/3], Step [1800/41412], Loss: 4.6672, Perplexity: 106.4032\n",
      "Epoch [1/3], Step [1900/41412], Loss: 4.1383, Perplexity: 62.69496\n",
      "Epoch [1/3], Step [2000/41412], Loss: 5.0227, Perplexity: 151.8267\n",
      "Epoch [1/3], Step [2100/41412], Loss: 3.8406, Perplexity: 46.55515\n",
      "Epoch [1/3], Step [2200/41412], Loss: 3.8859, Perplexity: 48.71112\n",
      "Epoch [1/3], Step [2300/41412], Loss: 4.3339, Perplexity: 76.24370\n",
      "Epoch [1/3], Step [2400/41412], Loss: 5.1606, Perplexity: 174.2768\n",
      "Epoch [1/3], Step [2500/41412], Loss: 4.0292, Perplexity: 56.21815\n",
      "Epoch [1/3], Step [2600/41412], Loss: 4.0746, Perplexity: 58.82504\n",
      "Epoch [1/3], Step [2700/41412], Loss: 4.3399, Perplexity: 76.69866\n",
      "Epoch [1/3], Step [2800/41412], Loss: 4.5132, Perplexity: 91.21731\n",
      "Epoch [1/3], Step [2900/41412], Loss: 4.3012, Perplexity: 73.79129\n",
      "Epoch [1/3], Step [3000/41412], Loss: 4.1170, Perplexity: 61.37686\n",
      "Epoch [1/3], Step [3100/41412], Loss: 3.9533, Perplexity: 52.10499\n",
      "Epoch [1/3], Step [3200/41412], Loss: 4.5661, Perplexity: 96.16928\n",
      "Epoch [1/3], Step [3300/41412], Loss: 3.4040, Perplexity: 30.08328\n",
      "Epoch [1/3], Step [3400/41412], Loss: 4.6889, Perplexity: 108.7370\n",
      "Epoch [1/3], Step [3500/41412], Loss: 3.9486, Perplexity: 51.86495\n",
      "Epoch [1/3], Step [3600/41412], Loss: 3.8142, Perplexity: 45.34062\n",
      "Epoch [1/3], Step [3700/41412], Loss: 3.9025, Perplexity: 49.52419\n",
      "Epoch [1/3], Step [3800/41412], Loss: 4.5657, Perplexity: 96.13277\n",
      "Epoch [1/3], Step [3900/41412], Loss: 4.2080, Perplexity: 67.22304\n",
      "Epoch [1/3], Step [4000/41412], Loss: 4.1187, Perplexity: 61.47821\n",
      "Epoch [1/3], Step [4100/41412], Loss: 3.6067, Perplexity: 36.84571\n",
      "Epoch [1/3], Step [4200/41412], Loss: 4.7107, Perplexity: 111.1325\n",
      "Epoch [1/3], Step [4300/41412], Loss: 4.2701, Perplexity: 71.53148\n",
      "Epoch [1/3], Step [4400/41412], Loss: 4.2756, Perplexity: 71.92447\n",
      "Epoch [1/3], Step [4500/41412], Loss: 3.4145, Perplexity: 30.40294\n",
      "Epoch [1/3], Step [4600/41412], Loss: 3.7540, Perplexity: 42.69303\n",
      "Epoch [1/3], Step [4700/41412], Loss: 4.1026, Perplexity: 60.49636\n",
      "Epoch [1/3], Step [4800/41412], Loss: 3.3805, Perplexity: 29.38513\n",
      "Epoch [1/3], Step [4900/41412], Loss: 5.0195, Perplexity: 151.3376\n",
      "Epoch [1/3], Step [5000/41412], Loss: 4.1199, Perplexity: 61.55341\n",
      "Epoch [1/3], Step [5100/41412], Loss: 4.1479, Perplexity: 63.29832\n",
      "Epoch [1/3], Step [5200/41412], Loss: 4.0361, Perplexity: 56.60439\n",
      "Epoch [1/3], Step [5300/41412], Loss: 4.1410, Perplexity: 62.86887\n",
      "Epoch [1/3], Step [5400/41412], Loss: 4.1225, Perplexity: 61.71649\n",
      "Epoch [1/3], Step [5500/41412], Loss: 3.4588, Perplexity: 31.78038\n",
      "Epoch [1/3], Step [5600/41412], Loss: 4.6175, Perplexity: 101.2424\n",
      "Epoch [1/3], Step [5700/41412], Loss: 3.7383, Perplexity: 42.02753\n",
      "Epoch [1/3], Step [5800/41412], Loss: 3.5248, Perplexity: 33.94591\n",
      "Epoch [1/3], Step [5900/41412], Loss: 3.8231, Perplexity: 45.74554\n",
      "Epoch [1/3], Step [6000/41412], Loss: 3.0058, Perplexity: 20.20170\n",
      "Epoch [1/3], Step [6100/41412], Loss: 4.4179, Perplexity: 82.92244\n",
      "Epoch [1/3], Step [6200/41412], Loss: 3.8308, Perplexity: 46.09720\n",
      "Epoch [1/3], Step [6300/41412], Loss: 3.8341, Perplexity: 46.24957\n",
      "Epoch [1/3], Step [6400/41412], Loss: 3.5269, Perplexity: 34.01723\n",
      "Epoch [1/3], Step [6500/41412], Loss: 3.4570, Perplexity: 31.72061\n",
      "Epoch [1/3], Step [6600/41412], Loss: 4.0355, Perplexity: 56.57115\n",
      "Epoch [1/3], Step [6700/41412], Loss: 3.2881, Perplexity: 26.79323\n",
      "Epoch [1/3], Step [6800/41412], Loss: 3.8522, Perplexity: 47.09491\n",
      "Epoch [1/3], Step [6900/41412], Loss: 3.7337, Perplexity: 41.83472\n",
      "Epoch [1/3], Step [7000/41412], Loss: 3.8833, Perplexity: 48.58505\n",
      "Epoch [1/3], Step [7100/41412], Loss: 3.4748, Perplexity: 32.29148\n",
      "Epoch [1/3], Step [7200/41412], Loss: 3.4286, Perplexity: 30.83237\n",
      "Epoch [1/3], Step [7300/41412], Loss: 3.5147, Perplexity: 33.60523\n",
      "Epoch [1/3], Step [7400/41412], Loss: 3.7741, Perplexity: 43.55705\n",
      "Epoch [1/3], Step [7500/41412], Loss: 4.6860, Perplexity: 108.4234\n",
      "Epoch [1/3], Step [7600/41412], Loss: 3.5853, Perplexity: 36.06524\n",
      "Epoch [1/3], Step [7700/41412], Loss: 3.0215, Perplexity: 20.52226\n",
      "Epoch [1/3], Step [7800/41412], Loss: 3.7875, Perplexity: 44.14640\n",
      "Epoch [1/3], Step [7900/41412], Loss: 3.8356, Perplexity: 46.32342\n",
      "Epoch [1/3], Step [8000/41412], Loss: 3.5101, Perplexity: 33.45241\n",
      "Epoch [1/3], Step [8100/41412], Loss: 4.8591, Perplexity: 128.9133\n",
      "Epoch [1/3], Step [8200/41412], Loss: 3.3135, Perplexity: 27.48218\n",
      "Epoch [1/3], Step [8300/41412], Loss: 3.8564, Perplexity: 47.29578\n",
      "Epoch [1/3], Step [8400/41412], Loss: 3.7948, Perplexity: 44.46942\n",
      "Epoch [1/3], Step [8500/41412], Loss: 3.4072, Perplexity: 30.18167\n",
      "Epoch [1/3], Step [8600/41412], Loss: 4.0951, Perplexity: 60.04542\n",
      "Epoch [1/3], Step [8700/41412], Loss: 3.3727, Perplexity: 29.15807\n",
      "Epoch [1/3], Step [8800/41412], Loss: 3.6382, Perplexity: 38.02491\n",
      "Epoch [1/3], Step [8900/41412], Loss: 3.0438, Perplexity: 20.9858\n",
      "Epoch [1/3], Step [9000/41412], Loss: 4.0461, Perplexity: 57.17220\n",
      "Epoch [1/3], Step [9100/41412], Loss: 4.3801, Perplexity: 79.84468\n",
      "Epoch [1/3], Step [9200/41412], Loss: 2.7005, Perplexity: 14.88766\n",
      "Epoch [1/3], Step [9300/41412], Loss: 3.7808, Perplexity: 43.84914\n",
      "Epoch [1/3], Step [9400/41412], Loss: 3.7063, Perplexity: 40.70384\n",
      "Epoch [1/3], Step [9500/41412], Loss: 3.8440, Perplexity: 46.71358\n",
      "Epoch [1/3], Step [9600/41412], Loss: 3.4833, Perplexity: 32.56549\n",
      "Epoch [1/3], Step [9700/41412], Loss: 3.2406, Perplexity: 25.54957\n",
      "Epoch [1/3], Step [9800/41412], Loss: 2.7522, Perplexity: 15.67696\n",
      "Epoch [1/3], Step [9900/41412], Loss: 3.6026, Perplexity: 36.69465\n",
      "Epoch [1/3], Step [10000/41412], Loss: 2.8515, Perplexity: 17.3130\n",
      "Epoch [1/3], Step [10100/41412], Loss: 3.8309, Perplexity: 46.10489\n",
      "Epoch [1/3], Step [10200/41412], Loss: 3.8697, Perplexity: 47.92895\n",
      "Epoch [1/3], Step [10300/41412], Loss: 3.6834, Perplexity: 39.7830\n",
      "Epoch [1/3], Step [10400/41412], Loss: 3.8676, Perplexity: 47.82643\n",
      "Epoch [1/3], Step [10500/41412], Loss: 3.3417, Perplexity: 28.2682\n",
      "Epoch [1/3], Step [10600/41412], Loss: 2.8553, Perplexity: 17.37995\n",
      "Epoch [1/3], Step [10700/41412], Loss: 3.7075, Perplexity: 40.7532\n",
      "Epoch [1/3], Step [10800/41412], Loss: 3.1393, Perplexity: 23.08848\n",
      "Epoch [1/3], Step [10900/41412], Loss: 3.5177, Perplexity: 33.70653\n",
      "Epoch [1/3], Step [11000/41412], Loss: 4.0718, Perplexity: 58.66001\n",
      "Epoch [1/3], Step [11100/41412], Loss: 2.8660, Perplexity: 17.5672\n",
      "Epoch [1/3], Step [11200/41412], Loss: 3.4184, Perplexity: 30.52056\n",
      "Epoch [1/3], Step [11300/41412], Loss: 4.6190, Perplexity: 101.3926\n",
      "Epoch [1/3], Step [11400/41412], Loss: 4.1315, Perplexity: 62.27151\n",
      "Epoch [1/3], Step [11500/41412], Loss: 3.6064, Perplexity: 36.8325\n",
      "Epoch [1/3], Step [11600/41412], Loss: 3.4793, Perplexity: 32.43645\n",
      "Epoch [1/3], Step [11700/41412], Loss: 2.9030, Perplexity: 18.2291\n",
      "Epoch [1/3], Step [11800/41412], Loss: 3.5558, Perplexity: 35.01584\n",
      "Epoch [1/3], Step [11900/41412], Loss: 3.7708, Perplexity: 43.4156\n",
      "Epoch [1/3], Step [12000/41412], Loss: 3.3870, Perplexity: 29.57596\n",
      "Epoch [1/3], Step [12100/41412], Loss: 3.4431, Perplexity: 31.28371\n",
      "Epoch [1/3], Step [12200/41412], Loss: 4.3653, Perplexity: 78.67125\n",
      "Epoch [1/3], Step [12300/41412], Loss: 3.8534, Perplexity: 47.15169\n",
      "Epoch [1/3], Step [12400/41412], Loss: 3.4195, Perplexity: 30.55343\n",
      "Epoch [1/3], Step [12500/41412], Loss: 3.7942, Perplexity: 44.4421\n",
      "Epoch [1/3], Step [12600/41412], Loss: 3.3317, Perplexity: 27.98531\n",
      "Epoch [1/3], Step [12700/41412], Loss: 3.9136, Perplexity: 50.0785\n",
      "Epoch [1/3], Step [12800/41412], Loss: 3.5687, Perplexity: 35.46984\n",
      "Epoch [1/3], Step [12900/41412], Loss: 3.7657, Perplexity: 43.19558\n",
      "Epoch [1/3], Step [13000/41412], Loss: 2.6980, Perplexity: 14.8507\n",
      "Epoch [1/3], Step [13100/41412], Loss: 3.7605, Perplexity: 42.96934\n",
      "Epoch [1/3], Step [13200/41412], Loss: 3.9949, Perplexity: 54.3222\n",
      "Epoch [1/3], Step [13300/41412], Loss: 5.1351, Perplexity: 169.8736\n",
      "Epoch [1/3], Step [13400/41412], Loss: 3.1687, Perplexity: 23.77747\n",
      "Epoch [1/3], Step [13500/41412], Loss: 4.2561, Perplexity: 70.53168\n",
      "Epoch [1/3], Step [13600/41412], Loss: 3.7960, Perplexity: 44.5227\n",
      "Epoch [1/3], Step [13700/41412], Loss: 3.5519, Perplexity: 34.88123\n",
      "Epoch [1/3], Step [13800/41412], Loss: 4.0867, Perplexity: 59.5412\n",
      "Epoch [1/3], Step [13900/41412], Loss: 3.3647, Perplexity: 28.92574\n",
      "Epoch [1/3], Step [14000/41412], Loss: 3.2659, Perplexity: 26.2046\n",
      "Epoch [1/3], Step [14100/41412], Loss: 4.2514, Perplexity: 70.20248\n",
      "Epoch [1/3], Step [14200/41412], Loss: 3.2157, Perplexity: 24.9210\n",
      "Epoch [1/3], Step [14300/41412], Loss: 4.7674, Perplexity: 117.6097\n",
      "Epoch [1/3], Step [14400/41412], Loss: 3.3033, Perplexity: 27.20364\n",
      "Epoch [1/3], Step [14500/41412], Loss: 2.6709, Perplexity: 14.4527\n",
      "Epoch [1/3], Step [14600/41412], Loss: 3.3491, Perplexity: 28.47815\n",
      "Epoch [1/3], Step [14700/41412], Loss: 3.8366, Perplexity: 46.3698\n",
      "Epoch [1/3], Step [14800/41412], Loss: 3.5109, Perplexity: 33.47820\n",
      "Epoch [1/3], Step [14900/41412], Loss: 3.3732, Perplexity: 29.17269\n",
      "Epoch [1/3], Step [15000/41412], Loss: 3.5159, Perplexity: 33.64472\n",
      "Epoch [1/3], Step [15100/41412], Loss: 3.3909, Perplexity: 29.6912\n",
      "Epoch [1/3], Step [15200/41412], Loss: 3.9885, Perplexity: 53.9733\n",
      "Epoch [1/3], Step [15300/41412], Loss: 3.4604, Perplexity: 31.8304\n",
      "Epoch [1/3], Step [15400/41412], Loss: 3.7515, Perplexity: 42.58507\n",
      "Epoch [1/3], Step [15500/41412], Loss: 3.0799, Perplexity: 21.75674\n",
      "Epoch [1/3], Step [15600/41412], Loss: 3.1058, Perplexity: 22.3274\n",
      "Epoch [1/3], Step [15700/41412], Loss: 4.0755, Perplexity: 58.87968\n",
      "Epoch [1/3], Step [15800/41412], Loss: 3.2588, Perplexity: 26.0175\n",
      "Epoch [1/3], Step [15900/41412], Loss: 3.1075, Perplexity: 22.3660\n",
      "Epoch [1/3], Step [16000/41412], Loss: 3.4224, Perplexity: 30.64439\n",
      "Epoch [1/3], Step [16100/41412], Loss: 3.3735, Perplexity: 29.18124\n",
      "Epoch [1/3], Step [16200/41412], Loss: 2.7411, Perplexity: 15.50447\n",
      "Epoch [1/3], Step [16300/41412], Loss: 3.1854, Perplexity: 24.17608\n",
      "Epoch [1/3], Step [16400/41412], Loss: 3.1167, Perplexity: 22.5728\n",
      "Epoch [1/3], Step [16500/41412], Loss: 4.4315, Perplexity: 84.05861\n",
      "Epoch [1/3], Step [16600/41412], Loss: 2.6268, Perplexity: 13.8297\n",
      "Epoch [1/3], Step [16700/41412], Loss: 2.9344, Perplexity: 18.8110\n",
      "Epoch [1/3], Step [16800/41412], Loss: 3.5224, Perplexity: 33.8657\n",
      "Epoch [1/3], Step [16900/41412], Loss: 2.9625, Perplexity: 19.3472\n",
      "Epoch [1/3], Step [17000/41412], Loss: 3.3964, Perplexity: 29.85513\n",
      "Epoch [1/3], Step [17100/41412], Loss: 2.8652, Perplexity: 17.55217\n",
      "Epoch [1/3], Step [17200/41412], Loss: 3.2206, Perplexity: 25.0441\n",
      "Epoch [1/3], Step [17300/41412], Loss: 2.9294, Perplexity: 18.7156\n",
      "Epoch [1/3], Step [17400/41412], Loss: 3.4833, Perplexity: 32.56763\n",
      "Epoch [1/3], Step [17500/41412], Loss: 3.2653, Perplexity: 26.18857\n",
      "Epoch [1/3], Step [17600/41412], Loss: 3.6866, Perplexity: 39.9095\n",
      "Epoch [1/3], Step [17700/41412], Loss: 3.0828, Perplexity: 21.8185\n",
      "Epoch [1/3], Step [17800/41412], Loss: 3.3214, Perplexity: 27.6998\n",
      "Epoch [1/3], Step [17900/41412], Loss: 3.8001, Perplexity: 44.70662\n",
      "Epoch [1/3], Step [18000/41412], Loss: 3.3599, Perplexity: 28.7872\n",
      "Epoch [1/3], Step [18100/41412], Loss: 3.2471, Perplexity: 25.71533\n",
      "Epoch [1/3], Step [18200/41412], Loss: 3.4395, Perplexity: 31.17088\n",
      "Epoch [1/3], Step [18300/41412], Loss: 3.4517, Perplexity: 31.5526\n",
      "Epoch [1/3], Step [18400/41412], Loss: 3.4196, Perplexity: 30.55573\n",
      "Epoch [1/3], Step [18500/41412], Loss: 2.9914, Perplexity: 19.91351\n",
      "Epoch [1/3], Step [18600/41412], Loss: 3.3123, Perplexity: 27.44817\n",
      "Epoch [1/3], Step [18700/41412], Loss: 2.9614, Perplexity: 19.3241\n",
      "Epoch [1/3], Step [18800/41412], Loss: 3.8206, Perplexity: 45.63262\n",
      "Epoch [1/3], Step [18900/41412], Loss: 3.0123, Perplexity: 20.33368\n",
      "Epoch [1/3], Step [19000/41412], Loss: 3.2514, Perplexity: 25.8277\n",
      "Epoch [1/3], Step [19100/41412], Loss: 3.2745, Perplexity: 26.4313\n",
      "Epoch [1/3], Step [19200/41412], Loss: 3.5541, Perplexity: 34.9555\n",
      "Epoch [1/3], Step [19300/41412], Loss: 3.1937, Perplexity: 24.3786\n",
      "Epoch [1/3], Step [19400/41412], Loss: 3.6672, Perplexity: 39.1405\n",
      "Epoch [1/3], Step [19500/41412], Loss: 3.4416, Perplexity: 31.23600\n",
      "Epoch [1/3], Step [19600/41412], Loss: 2.8788, Perplexity: 17.7937\n",
      "Epoch [1/3], Step [19700/41412], Loss: 3.9181, Perplexity: 50.30678\n",
      "Epoch [1/3], Step [19800/41412], Loss: 3.5178, Perplexity: 33.70857\n",
      "Epoch [1/3], Step [19900/41412], Loss: 3.1164, Perplexity: 22.5650\n",
      "Epoch [1/3], Step [20000/41412], Loss: 2.7874, Perplexity: 16.23888\n",
      "Epoch [1/3], Step [20100/41412], Loss: 3.0378, Perplexity: 20.85831\n",
      "Epoch [1/3], Step [20200/41412], Loss: 2.9277, Perplexity: 18.6846\n",
      "Epoch [1/3], Step [20300/41412], Loss: 3.2921, Perplexity: 26.9004\n",
      "Epoch [1/3], Step [20400/41412], Loss: 3.7555, Perplexity: 42.75656\n",
      "Epoch [1/3], Step [20500/41412], Loss: 3.2166, Perplexity: 24.9425\n",
      "Epoch [1/3], Step [20600/41412], Loss: 3.2782, Perplexity: 26.5275\n",
      "Epoch [1/3], Step [20700/41412], Loss: 2.5037, Perplexity: 12.2280\n",
      "Epoch [1/3], Step [20800/41412], Loss: 3.5016, Perplexity: 33.1689\n",
      "Epoch [1/3], Step [20900/41412], Loss: 3.0792, Perplexity: 21.7408\n",
      "Epoch [1/3], Step [21000/41412], Loss: 3.0660, Perplexity: 21.4562\n",
      "Epoch [1/3], Step [21100/41412], Loss: 3.6434, Perplexity: 38.2221\n",
      "Epoch [1/3], Step [21200/41412], Loss: 3.4901, Perplexity: 32.7889\n",
      "Epoch [1/3], Step [21300/41412], Loss: 2.9633, Perplexity: 19.36248\n",
      "Epoch [1/3], Step [21400/41412], Loss: 3.1633, Perplexity: 23.64889\n",
      "Epoch [1/3], Step [21500/41412], Loss: 3.3203, Perplexity: 27.6682\n",
      "Epoch [1/3], Step [21600/41412], Loss: 3.5754, Perplexity: 35.7105\n",
      "Epoch [1/3], Step [21700/41412], Loss: 3.1332, Perplexity: 22.9468\n",
      "Epoch [1/3], Step [21800/41412], Loss: 2.8881, Perplexity: 17.95864\n",
      "Epoch [1/3], Step [21900/41412], Loss: 3.3216, Perplexity: 27.70382\n",
      "Epoch [1/3], Step [22000/41412], Loss: 2.9819, Perplexity: 19.72549\n",
      "Epoch [1/3], Step [22100/41412], Loss: 3.9166, Perplexity: 50.22869\n",
      "Epoch [1/3], Step [22200/41412], Loss: 3.7623, Perplexity: 43.0458\n",
      "Epoch [1/3], Step [22300/41412], Loss: 3.3272, Perplexity: 27.8609\n",
      "Epoch [1/3], Step [22400/41412], Loss: 3.2317, Perplexity: 25.32385\n",
      "Epoch [1/3], Step [22500/41412], Loss: 3.0283, Perplexity: 20.66123\n",
      "Epoch [1/3], Step [22600/41412], Loss: 3.7990, Perplexity: 44.65750\n",
      "Epoch [1/3], Step [22700/41412], Loss: 2.7670, Perplexity: 15.91036\n",
      "Epoch [1/3], Step [22800/41412], Loss: 4.0619, Perplexity: 58.0855\n",
      "Epoch [1/3], Step [22900/41412], Loss: 3.4344, Perplexity: 31.0118\n",
      "Epoch [1/3], Step [23000/41412], Loss: 3.0946, Perplexity: 22.07929\n",
      "Epoch [1/3], Step [23100/41412], Loss: 3.3876, Perplexity: 29.5953\n",
      "Epoch [1/3], Step [23200/41412], Loss: 3.9616, Perplexity: 52.5439\n",
      "Epoch [1/3], Step [23300/41412], Loss: 3.9321, Perplexity: 51.01458\n",
      "Epoch [1/3], Step [23400/41412], Loss: 2.9038, Perplexity: 18.2432\n",
      "Epoch [1/3], Step [23500/41412], Loss: 3.5454, Perplexity: 34.6530\n",
      "Epoch [1/3], Step [23600/41412], Loss: 3.4262, Perplexity: 30.7596\n",
      "Epoch [1/3], Step [23700/41412], Loss: 3.5019, Perplexity: 33.17857\n",
      "Epoch [1/3], Step [23800/41412], Loss: 4.6956, Perplexity: 109.4602\n",
      "Epoch [1/3], Step [23900/41412], Loss: 2.9583, Perplexity: 19.26509\n",
      "Epoch [1/3], Step [24000/41412], Loss: 3.2523, Perplexity: 25.8499\n",
      "Epoch [1/3], Step [24100/41412], Loss: 2.5974, Perplexity: 13.4282\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/3], Step [24200/41412], Loss: 3.7556, Perplexity: 42.7611\n",
      "Epoch [1/3], Step [24300/41412], Loss: 2.3806, Perplexity: 10.8110\n",
      "Epoch [1/3], Step [24400/41412], Loss: 3.3599, Perplexity: 28.78547\n",
      "Epoch [1/3], Step [24500/41412], Loss: 3.5174, Perplexity: 33.69757\n",
      "Epoch [1/3], Step [24600/41412], Loss: 3.0778, Perplexity: 21.7097\n",
      "Epoch [1/3], Step [24700/41412], Loss: 3.3670, Perplexity: 28.9926\n",
      "Epoch [1/3], Step [24800/41412], Loss: 3.2877, Perplexity: 26.7815\n",
      "Epoch [1/3], Step [24900/41412], Loss: 3.3052, Perplexity: 27.25334\n",
      "Epoch [1/3], Step [25000/41412], Loss: 3.4461, Perplexity: 31.37779\n",
      "Epoch [1/3], Step [25100/41412], Loss: 3.1457, Perplexity: 23.2357\n",
      "Epoch [1/3], Step [25200/41412], Loss: 3.9032, Perplexity: 49.5624\n",
      "Epoch [1/3], Step [25300/41412], Loss: 2.4940, Perplexity: 12.11028\n",
      "Epoch [1/3], Step [25400/41412], Loss: 3.3570, Perplexity: 28.70153\n",
      "Epoch [1/3], Step [25500/41412], Loss: 3.3195, Perplexity: 27.6466\n",
      "Epoch [1/3], Step [25600/41412], Loss: 2.5289, Perplexity: 12.5395\n",
      "Epoch [1/3], Step [25700/41412], Loss: 3.1714, Perplexity: 23.8401\n",
      "Epoch [1/3], Step [25800/41412], Loss: 2.5230, Perplexity: 12.4661\n",
      "Epoch [1/3], Step [25900/41412], Loss: 3.3757, Perplexity: 29.2436\n",
      "Epoch [1/3], Step [26000/41412], Loss: 3.7561, Perplexity: 42.7806\n",
      "Epoch [1/3], Step [26100/41412], Loss: 2.8697, Perplexity: 17.63139\n",
      "Epoch [1/3], Step [26200/41412], Loss: 2.6816, Perplexity: 14.6079\n",
      "Epoch [1/3], Step [26300/41412], Loss: 2.7159, Perplexity: 15.1187\n",
      "Epoch [1/3], Step [26400/41412], Loss: 2.9341, Perplexity: 18.8043\n",
      "Epoch [1/3], Step [26500/41412], Loss: 3.2903, Perplexity: 26.8510\n",
      "Epoch [1/3], Step [26600/41412], Loss: 3.5107, Perplexity: 33.4712\n",
      "Epoch [1/3], Step [26700/41412], Loss: 2.9786, Perplexity: 19.6610\n",
      "Epoch [1/3], Step [26800/41412], Loss: 4.0704, Perplexity: 58.5795\n",
      "Epoch [1/3], Step [26900/41412], Loss: 2.9602, Perplexity: 19.3014\n",
      "Epoch [1/3], Step [27000/41412], Loss: 3.0307, Perplexity: 20.7110\n",
      "Epoch [1/3], Step [27100/41412], Loss: 3.3204, Perplexity: 27.67203\n",
      "Epoch [1/3], Step [27200/41412], Loss: 3.1730, Perplexity: 23.8789\n",
      "Epoch [1/3], Step [27300/41412], Loss: 3.3170, Perplexity: 27.5773\n",
      "Epoch [1/3], Step [27400/41412], Loss: 2.7336, Perplexity: 15.3878\n",
      "Epoch [1/3], Step [27500/41412], Loss: 2.9994, Perplexity: 20.0740\n",
      "Epoch [1/3], Step [27600/41412], Loss: 3.1785, Perplexity: 24.01100\n",
      "Epoch [1/3], Step [27700/41412], Loss: 3.0106, Perplexity: 20.29930\n",
      "Epoch [1/3], Step [27800/41412], Loss: 2.3464, Perplexity: 10.4475\n",
      "Epoch [1/3], Step [27900/41412], Loss: 3.6176, Perplexity: 37.2470\n",
      "Epoch [1/3], Step [28000/41412], Loss: 3.0511, Perplexity: 21.13947\n",
      "Epoch [1/3], Step [28100/41412], Loss: 2.8127, Perplexity: 16.6540\n",
      "Epoch [1/3], Step [28200/41412], Loss: 3.4163, Perplexity: 30.45609\n",
      "Epoch [1/3], Step [28300/41412], Loss: 3.2506, Perplexity: 25.80532\n",
      "Epoch [1/3], Step [28400/41412], Loss: 3.1122, Perplexity: 22.4714\n",
      "Epoch [1/3], Step [28500/41412], Loss: 2.9554, Perplexity: 19.2101\n",
      "Epoch [1/3], Step [28600/41412], Loss: 3.3294, Perplexity: 27.92262\n",
      "Epoch [1/3], Step [28700/41412], Loss: 3.2543, Perplexity: 25.9016\n",
      "Epoch [1/3], Step [28800/41412], Loss: 2.3814, Perplexity: 10.8202\n",
      "Epoch [1/3], Step [28900/41412], Loss: 4.2700, Perplexity: 71.5220\n",
      "Epoch [1/3], Step [29000/41412], Loss: 4.1508, Perplexity: 63.4828\n",
      "Epoch [1/3], Step [29100/41412], Loss: 2.4105, Perplexity: 11.1396\n",
      "Epoch [1/3], Step [29200/41412], Loss: 3.3564, Perplexity: 28.6862\n",
      "Epoch [1/3], Step [29300/41412], Loss: 3.0226, Perplexity: 20.5445\n",
      "Epoch [1/3], Step [29400/41412], Loss: 3.3740, Perplexity: 29.19385\n",
      "Epoch [1/3], Step [29500/41412], Loss: 4.1367, Perplexity: 62.59504\n",
      "Epoch [1/3], Step [29600/41412], Loss: 3.9378, Perplexity: 51.3079\n",
      "Epoch [1/3], Step [29700/41412], Loss: 2.9039, Perplexity: 18.2445\n",
      "Epoch [1/3], Step [29800/41412], Loss: 2.9860, Perplexity: 19.80724\n",
      "Epoch [1/3], Step [29900/41412], Loss: 3.4848, Perplexity: 32.6154\n",
      "Epoch [1/3], Step [30000/41412], Loss: 2.8564, Perplexity: 17.3988\n",
      "Epoch [1/3], Step [30100/41412], Loss: 3.1845, Perplexity: 24.15601\n",
      "Epoch [1/3], Step [30200/41412], Loss: 2.9639, Perplexity: 19.37273\n",
      "Epoch [1/3], Step [30300/41412], Loss: 2.9488, Perplexity: 19.08306\n",
      "Epoch [1/3], Step [30400/41412], Loss: 3.2186, Perplexity: 24.9926\n",
      "Epoch [1/3], Step [30500/41412], Loss: 3.3204, Perplexity: 27.6722\n",
      "Epoch [1/3], Step [30600/41412], Loss: 2.9784, Perplexity: 19.6559\n",
      "Epoch [1/3], Step [30700/41412], Loss: 3.0384, Perplexity: 20.87246\n",
      "Epoch [1/3], Step [30800/41412], Loss: 2.9732, Perplexity: 19.5552\n",
      "Epoch [1/3], Step [30900/41412], Loss: 2.9479, Perplexity: 19.0662\n",
      "Epoch [1/3], Step [31000/41412], Loss: 3.3767, Perplexity: 29.27462\n",
      "Epoch [1/3], Step [31100/41412], Loss: 2.6600, Perplexity: 14.2960\n",
      "Epoch [1/3], Step [31200/41412], Loss: 2.9468, Perplexity: 19.0455\n",
      "Epoch [1/3], Step [31300/41412], Loss: 2.9375, Perplexity: 18.8683\n",
      "Epoch [1/3], Step [31400/41412], Loss: 3.0678, Perplexity: 21.49549\n",
      "Epoch [1/3], Step [31500/41412], Loss: 3.2100, Perplexity: 24.77946\n",
      "Epoch [1/3], Step [31600/41412], Loss: 3.2408, Perplexity: 25.5533\n",
      "Epoch [1/3], Step [31700/41412], Loss: 2.9789, Perplexity: 19.6660\n",
      "Epoch [1/3], Step [31800/41412], Loss: 2.5958, Perplexity: 13.4078\n",
      "Epoch [1/3], Step [31900/41412], Loss: 3.6053, Perplexity: 36.7917\n",
      "Epoch [1/3], Step [32000/41412], Loss: 2.9959, Perplexity: 20.0037\n",
      "Epoch [1/3], Step [32100/41412], Loss: 3.7259, Perplexity: 41.5069\n",
      "Epoch [1/3], Step [32200/41412], Loss: 2.9394, Perplexity: 18.90404\n",
      "Epoch [1/3], Step [32300/41412], Loss: 3.0501, Perplexity: 21.1183\n",
      "Epoch [1/3], Step [32400/41412], Loss: 3.4792, Perplexity: 32.4342\n",
      "Epoch [1/3], Step [32500/41412], Loss: 4.0582, Perplexity: 57.8699\n",
      "Epoch [1/3], Step [32600/41412], Loss: 3.1800, Perplexity: 24.0461\n",
      "Epoch [1/3], Step [32700/41412], Loss: 2.9149, Perplexity: 18.4471\n",
      "Epoch [1/3], Step [32800/41412], Loss: 3.2278, Perplexity: 25.22368\n",
      "Epoch [1/3], Step [32900/41412], Loss: 2.9733, Perplexity: 19.5563\n",
      "Epoch [1/3], Step [33000/41412], Loss: 3.3676, Perplexity: 29.0077\n",
      "Epoch [1/3], Step [33100/41412], Loss: 3.1785, Perplexity: 24.00979\n",
      "Epoch [1/3], Step [33200/41412], Loss: 5.4276, Perplexity: 227.6038\n",
      "Epoch [1/3], Step [33300/41412], Loss: 2.9107, Perplexity: 18.3692\n",
      "Epoch [1/3], Step [33400/41412], Loss: 3.8245, Perplexity: 45.8098\n",
      "Epoch [1/3], Step [33500/41412], Loss: 3.9613, Perplexity: 52.5274\n",
      "Epoch [1/3], Step [33600/41412], Loss: 2.9432, Perplexity: 18.9761\n",
      "Epoch [1/3], Step [33700/41412], Loss: 3.5526, Perplexity: 34.9048\n",
      "Epoch [1/3], Step [33800/41412], Loss: 3.2057, Perplexity: 24.6726\n",
      "Epoch [1/3], Step [33900/41412], Loss: 3.2600, Perplexity: 26.0506\n",
      "Epoch [1/3], Step [34000/41412], Loss: 3.4617, Perplexity: 31.8723\n",
      "Epoch [1/3], Step [34100/41412], Loss: 2.9812, Perplexity: 19.7107\n",
      "Epoch [1/3], Step [34200/41412], Loss: 3.2486, Perplexity: 25.75313\n",
      "Epoch [1/3], Step [34300/41412], Loss: 3.4273, Perplexity: 30.7932\n",
      "Epoch [1/3], Step [34400/41412], Loss: 3.0751, Perplexity: 21.6519\n",
      "Epoch [1/3], Step [34500/41412], Loss: 3.4352, Perplexity: 31.0366\n",
      "Epoch [1/3], Step [34600/41412], Loss: 3.4211, Perplexity: 30.60418\n",
      "Epoch [1/3], Step [34700/41412], Loss: 3.1133, Perplexity: 22.49445\n",
      "Epoch [1/3], Step [34800/41412], Loss: 2.8059, Perplexity: 16.5419\n",
      "Epoch [1/3], Step [34900/41412], Loss: 3.2530, Perplexity: 25.8670\n",
      "Epoch [1/3], Step [35000/41412], Loss: 2.7615, Perplexity: 15.8242\n",
      "Epoch [1/3], Step [35100/41412], Loss: 2.9760, Perplexity: 19.6091\n",
      "Epoch [1/3], Step [35200/41412], Loss: 2.9577, Perplexity: 19.2528\n",
      "Epoch [1/3], Step [35300/41412], Loss: 3.0176, Perplexity: 20.4430\n",
      "Epoch [1/3], Step [35400/41412], Loss: 2.8911, Perplexity: 18.0138\n",
      "Epoch [1/3], Step [35500/41412], Loss: 3.0657, Perplexity: 21.4486\n",
      "Epoch [1/3], Step [35600/41412], Loss: 3.0120, Perplexity: 20.3281\n",
      "Epoch [1/3], Step [35700/41412], Loss: 3.0883, Perplexity: 21.9391\n",
      "Epoch [1/3], Step [35800/41412], Loss: 2.4794, Perplexity: 11.9338\n",
      "Epoch [1/3], Step [35900/41412], Loss: 3.3309, Perplexity: 27.9632\n",
      "Epoch [1/3], Step [36000/41412], Loss: 3.2218, Perplexity: 25.0738\n",
      "Epoch [1/3], Step [36100/41412], Loss: 3.4922, Perplexity: 32.8587\n",
      "Epoch [1/3], Step [36200/41412], Loss: 2.9436, Perplexity: 18.9840\n",
      "Epoch [1/3], Step [36300/41412], Loss: 3.0737, Perplexity: 21.6217\n",
      "Epoch [1/3], Step [36400/41412], Loss: 3.0422, Perplexity: 20.9513\n",
      "Epoch [1/3], Step [36500/41412], Loss: 2.7685, Perplexity: 15.93523\n",
      "Epoch [1/3], Step [36600/41412], Loss: 3.0965, Perplexity: 22.1196\n",
      "Epoch [1/3], Step [36700/41412], Loss: 2.6391, Perplexity: 14.00105\n",
      "Epoch [1/3], Step [36800/41412], Loss: 2.5712, Perplexity: 13.0809\n",
      "Epoch [1/3], Step [36900/41412], Loss: 3.3785, Perplexity: 29.32790\n",
      "Epoch [1/3], Step [37000/41412], Loss: 3.5787, Perplexity: 35.8266\n",
      "Epoch [1/3], Step [37100/41412], Loss: 2.6595, Perplexity: 14.2892\n",
      "Epoch [1/3], Step [37200/41412], Loss: 3.4565, Perplexity: 31.7072\n",
      "Epoch [1/3], Step [37300/41412], Loss: 3.2537, Perplexity: 25.8864\n",
      "Epoch [1/3], Step [37400/41412], Loss: 2.9077, Perplexity: 18.3146\n",
      "Epoch [1/3], Step [37500/41412], Loss: 3.1335, Perplexity: 22.9547\n",
      "Epoch [1/3], Step [37600/41412], Loss: 2.6549, Perplexity: 14.22361\n",
      "Epoch [1/3], Step [37700/41412], Loss: 2.9941, Perplexity: 19.9683\n",
      "Epoch [1/3], Step [37800/41412], Loss: 3.3516, Perplexity: 28.5497\n",
      "Epoch [1/3], Step [37900/41412], Loss: 3.0799, Perplexity: 21.7558\n",
      "Epoch [1/3], Step [38000/41412], Loss: 3.2828, Perplexity: 26.65058\n",
      "Epoch [1/3], Step [38100/41412], Loss: 3.3495, Perplexity: 28.4873\n",
      "Epoch [1/3], Step [38200/41412], Loss: 2.9256, Perplexity: 18.6456\n",
      "Epoch [1/3], Step [38300/41412], Loss: 3.0732, Perplexity: 21.6110\n",
      "Epoch [1/3], Step [38400/41412], Loss: 3.7550, Perplexity: 42.7344\n",
      "Epoch [1/3], Step [38500/41412], Loss: 2.7910, Perplexity: 16.2967\n",
      "Epoch [1/3], Step [38600/41412], Loss: 2.7360, Perplexity: 15.4253\n",
      "Epoch [1/3], Step [38700/41412], Loss: 2.9741, Perplexity: 19.5728\n",
      "Epoch [1/3], Step [38800/41412], Loss: 3.0030, Perplexity: 20.1463\n",
      "Epoch [1/3], Step [38900/41412], Loss: 3.3576, Perplexity: 28.7209\n",
      "Epoch [1/3], Step [39000/41412], Loss: 2.5217, Perplexity: 12.4494\n",
      "Epoch [1/3], Step [39100/41412], Loss: 2.9467, Perplexity: 19.0421\n",
      "Epoch [1/3], Step [39200/41412], Loss: 3.2450, Perplexity: 25.6628\n",
      "Epoch [1/3], Step [39300/41412], Loss: 2.7539, Perplexity: 15.7034\n",
      "Epoch [1/3], Step [39400/41412], Loss: 2.9776, Perplexity: 19.6415\n",
      "Epoch [1/3], Step [39500/41412], Loss: 2.9247, Perplexity: 18.6284\n",
      "Epoch [1/3], Step [39600/41412], Loss: 2.9322, Perplexity: 18.7680\n",
      "Epoch [1/3], Step [39700/41412], Loss: 3.3078, Perplexity: 27.3257\n",
      "Epoch [1/3], Step [39800/41412], Loss: 3.3073, Perplexity: 27.3119\n",
      "Epoch [1/3], Step [39900/41412], Loss: 2.9787, Perplexity: 19.6624\n",
      "Epoch [1/3], Step [40000/41412], Loss: 3.7210, Perplexity: 41.3038\n",
      "Epoch [1/3], Step [40100/41412], Loss: 3.7150, Perplexity: 41.05745\n",
      "Epoch [1/3], Step [40200/41412], Loss: 3.1933, Perplexity: 24.3693\n",
      "Epoch [1/3], Step [40300/41412], Loss: 3.3294, Perplexity: 27.92249\n",
      "Epoch [1/3], Step [40400/41412], Loss: 3.3053, Perplexity: 27.25678\n",
      "Epoch [1/3], Step [40500/41412], Loss: 3.1606, Perplexity: 23.5838\n",
      "Epoch [1/3], Step [40600/41412], Loss: 3.4266, Perplexity: 30.7720\n",
      "Epoch [1/3], Step [40700/41412], Loss: 2.7296, Perplexity: 15.3268\n",
      "Epoch [1/3], Step [40800/41412], Loss: 3.0682, Perplexity: 21.5040\n",
      "Epoch [1/3], Step [40900/41412], Loss: 2.8370, Perplexity: 17.0641\n",
      "Epoch [1/3], Step [41000/41412], Loss: 3.2712, Perplexity: 26.3432\n",
      "Epoch [1/3], Step [41100/41412], Loss: 2.6795, Perplexity: 14.5775\n",
      "Epoch [1/3], Step [41200/41412], Loss: 2.5482, Perplexity: 12.7841\n",
      "Epoch [1/3], Step [41300/41412], Loss: 3.5935, Perplexity: 36.3624\n",
      "Epoch [1/3], Step [41400/41412], Loss: 1.9865, Perplexity: 7.29000\n",
      "Epoch [2/3], Step [100/41412], Loss: 2.8765, Perplexity: 17.751964\n",
      "Epoch [2/3], Step [200/41412], Loss: 2.7570, Perplexity: 15.7525\n",
      "Epoch [2/3], Step [300/41412], Loss: 2.6249, Perplexity: 13.8028\n",
      "Epoch [2/3], Step [400/41412], Loss: 3.3117, Perplexity: 27.4312\n",
      "Epoch [2/3], Step [500/41412], Loss: 2.6896, Perplexity: 14.7260\n",
      "Epoch [2/3], Step [600/41412], Loss: 2.6538, Perplexity: 14.2078\n",
      "Epoch [2/3], Step [700/41412], Loss: 2.6583, Perplexity: 14.2724\n",
      "Epoch [2/3], Step [800/41412], Loss: 3.2889, Perplexity: 26.8142\n",
      "Epoch [2/3], Step [900/41412], Loss: 3.6017, Perplexity: 36.6592\n",
      "Epoch [2/3], Step [1000/41412], Loss: 3.6479, Perplexity: 38.3950\n",
      "Epoch [2/3], Step [1100/41412], Loss: 2.6342, Perplexity: 13.9327\n",
      "Epoch [2/3], Step [1200/41412], Loss: 3.6875, Perplexity: 39.94641\n",
      "Epoch [2/3], Step [1300/41412], Loss: 2.4996, Perplexity: 12.1772\n",
      "Epoch [2/3], Step [1400/41412], Loss: 3.0662, Perplexity: 21.4595\n",
      "Epoch [2/3], Step [1500/41412], Loss: 3.2666, Perplexity: 26.2232\n",
      "Epoch [2/3], Step [1600/41412], Loss: 2.6271, Perplexity: 13.8330\n",
      "Epoch [2/3], Step [1700/41412], Loss: 3.2887, Perplexity: 26.8083\n",
      "Epoch [2/3], Step [1800/41412], Loss: 2.9486, Perplexity: 19.0801\n",
      "Epoch [2/3], Step [1900/41412], Loss: 3.1696, Perplexity: 23.79872\n",
      "Epoch [2/3], Step [2000/41412], Loss: 2.4098, Perplexity: 11.13173\n",
      "Epoch [2/3], Step [2100/41412], Loss: 3.0076, Perplexity: 20.2383\n",
      "Epoch [2/3], Step [2200/41412], Loss: 2.5896, Perplexity: 13.32425\n",
      "Epoch [2/3], Step [2300/41412], Loss: 3.4282, Perplexity: 30.8201\n",
      "Epoch [2/3], Step [2400/41412], Loss: 2.9595, Perplexity: 19.2876\n",
      "Epoch [2/3], Step [2500/41412], Loss: 3.2613, Perplexity: 26.0836\n",
      "Epoch [2/3], Step [2600/41412], Loss: 2.9450, Perplexity: 19.0110\n",
      "Epoch [2/3], Step [2700/41412], Loss: 4.8109, Perplexity: 122.8372\n",
      "Epoch [2/3], Step [2800/41412], Loss: 1.7989, Perplexity: 6.04283\n",
      "Epoch [2/3], Step [2900/41412], Loss: 3.5525, Perplexity: 34.90031\n",
      "Epoch [2/3], Step [3000/41412], Loss: 2.8147, Perplexity: 16.6874\n",
      "Epoch [2/3], Step [3100/41412], Loss: 3.4721, Perplexity: 32.2028\n",
      "Epoch [2/3], Step [3200/41412], Loss: 3.3710, Perplexity: 29.1065\n",
      "Epoch [2/3], Step [3300/41412], Loss: 3.2339, Perplexity: 25.3775\n",
      "Epoch [2/3], Step [3400/41412], Loss: 2.7667, Perplexity: 15.9056\n",
      "Epoch [2/3], Step [3500/41412], Loss: 2.7520, Perplexity: 15.6737\n",
      "Epoch [2/3], Step [3600/41412], Loss: 3.1495, Perplexity: 23.3253\n",
      "Epoch [2/3], Step [3700/41412], Loss: 3.7010, Perplexity: 40.4878\n",
      "Epoch [2/3], Step [3800/41412], Loss: 2.5761, Perplexity: 13.1462\n",
      "Epoch [2/3], Step [3900/41412], Loss: 3.0260, Perplexity: 20.6153\n",
      "Epoch [2/3], Step [4000/41412], Loss: 2.8335, Perplexity: 17.0049\n",
      "Epoch [2/3], Step [4100/41412], Loss: 2.9071, Perplexity: 18.30391\n",
      "Epoch [2/3], Step [4200/41412], Loss: 3.0162, Perplexity: 20.4135\n",
      "Epoch [2/3], Step [4300/41412], Loss: 3.1714, Perplexity: 23.8401\n",
      "Epoch [2/3], Step [4400/41412], Loss: 2.8544, Perplexity: 17.3639\n",
      "Epoch [2/3], Step [4500/41412], Loss: 3.3144, Perplexity: 27.5049\n",
      "Epoch [2/3], Step [4600/41412], Loss: 2.8476, Perplexity: 17.2459\n",
      "Epoch [2/3], Step [4700/41412], Loss: 3.6391, Perplexity: 38.0584\n",
      "Epoch [2/3], Step [4800/41412], Loss: 3.1742, Perplexity: 23.9066\n",
      "Epoch [2/3], Step [4900/41412], Loss: 3.3066, Perplexity: 27.2908\n",
      "Epoch [2/3], Step [5000/41412], Loss: 2.8060, Perplexity: 16.5444\n",
      "Epoch [2/3], Step [5100/41412], Loss: 2.9378, Perplexity: 18.8746\n",
      "Epoch [2/3], Step [5200/41412], Loss: 2.9183, Perplexity: 18.5094\n",
      "Epoch [2/3], Step [5300/41412], Loss: 3.1163, Perplexity: 22.5636\n",
      "Epoch [2/3], Step [5400/41412], Loss: 3.8633, Perplexity: 47.62286\n",
      "Epoch [2/3], Step [5500/41412], Loss: 3.0574, Perplexity: 21.27314\n",
      "Epoch [2/3], Step [5600/41412], Loss: 2.9710, Perplexity: 19.5112\n",
      "Epoch [2/3], Step [5700/41412], Loss: 3.3092, Perplexity: 27.3643\n",
      "Epoch [2/3], Step [5800/41412], Loss: 3.2940, Perplexity: 26.9492\n",
      "Epoch [2/3], Step [5900/41412], Loss: 2.6727, Perplexity: 14.4789\n",
      "Epoch [2/3], Step [6000/41412], Loss: 2.4281, Perplexity: 11.3375\n",
      "Epoch [2/3], Step [6100/41412], Loss: 3.3595, Perplexity: 28.7745\n",
      "Epoch [2/3], Step [6200/41412], Loss: 2.6635, Perplexity: 14.3469\n",
      "Epoch [2/3], Step [6300/41412], Loss: 3.3226, Perplexity: 27.7317\n",
      "Epoch [2/3], Step [6400/41412], Loss: 2.7838, Perplexity: 16.18067\n",
      "Epoch [2/3], Step [6500/41412], Loss: 2.8034, Perplexity: 16.5009\n",
      "Epoch [2/3], Step [6600/41412], Loss: 3.1360, Perplexity: 23.0114\n",
      "Epoch [2/3], Step [6700/41412], Loss: 3.0181, Perplexity: 20.4531\n",
      "Epoch [2/3], Step [6800/41412], Loss: 2.6626, Perplexity: 14.3332\n",
      "Epoch [2/3], Step [6900/41412], Loss: 2.5461, Perplexity: 12.7566\n",
      "Epoch [2/3], Step [7000/41412], Loss: 3.2230, Perplexity: 25.1027\n",
      "Epoch [2/3], Step [7100/41412], Loss: 2.3619, Perplexity: 10.6112\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/3], Step [7200/41412], Loss: 2.8669, Perplexity: 17.58259\n",
      "Epoch [2/3], Step [7300/41412], Loss: 3.0929, Perplexity: 22.0402\n",
      "Epoch [2/3], Step [7400/41412], Loss: 3.3968, Perplexity: 29.8681\n",
      "Epoch [2/3], Step [7500/41412], Loss: 3.0377, Perplexity: 20.8575\n",
      "Epoch [2/3], Step [7600/41412], Loss: 3.2804, Perplexity: 26.5854\n",
      "Epoch [2/3], Step [7700/41412], Loss: 2.6895, Perplexity: 14.7247\n",
      "Epoch [2/3], Step [7800/41412], Loss: 3.4483, Perplexity: 31.4453\n",
      "Epoch [2/3], Step [7900/41412], Loss: 2.7830, Perplexity: 16.1682\n",
      "Epoch [2/3], Step [8000/41412], Loss: 2.5501, Perplexity: 12.8090\n",
      "Epoch [2/3], Step [8100/41412], Loss: 2.7874, Perplexity: 16.2382\n",
      "Epoch [2/3], Step [8200/41412], Loss: 2.9991, Perplexity: 20.0682\n",
      "Epoch [2/3], Step [8300/41412], Loss: 3.5377, Perplexity: 34.3881\n",
      "Epoch [2/3], Step [8400/41412], Loss: 3.2958, Perplexity: 27.0000\n",
      "Epoch [2/3], Step [8500/41412], Loss: 3.0875, Perplexity: 21.9223\n",
      "Epoch [2/3], Step [8600/41412], Loss: 2.9822, Perplexity: 19.7309\n",
      "Epoch [2/3], Step [8700/41412], Loss: 2.6686, Perplexity: 14.41918\n",
      "Epoch [2/3], Step [8800/41412], Loss: 3.9529, Perplexity: 52.0860\n",
      "Epoch [2/3], Step [8900/41412], Loss: 2.8873, Perplexity: 17.9449\n",
      "Epoch [2/3], Step [9000/41412], Loss: 3.3208, Perplexity: 27.6813\n",
      "Epoch [2/3], Step [9100/41412], Loss: 3.0193, Perplexity: 20.4762\n",
      "Epoch [2/3], Step [9200/41412], Loss: 3.0172, Perplexity: 20.4335\n",
      "Epoch [2/3], Step [9300/41412], Loss: 2.5043, Perplexity: 12.2345\n",
      "Epoch [2/3], Step [9400/41412], Loss: 3.3183, Perplexity: 27.6132\n",
      "Epoch [2/3], Step [9500/41412], Loss: 3.4507, Perplexity: 31.5210\n",
      "Epoch [2/3], Step [9600/41412], Loss: 3.1801, Perplexity: 24.0500\n",
      "Epoch [2/3], Step [9700/41412], Loss: 3.4968, Perplexity: 33.0093\n",
      "Epoch [2/3], Step [9800/41412], Loss: 3.6016, Perplexity: 36.6572\n",
      "Epoch [2/3], Step [9900/41412], Loss: 2.9177, Perplexity: 18.4979\n",
      "Epoch [2/3], Step [10000/41412], Loss: 2.8639, Perplexity: 17.5302\n",
      "Epoch [2/3], Step [10100/41412], Loss: 2.5065, Perplexity: 12.2616\n",
      "Epoch [2/3], Step [10200/41412], Loss: 3.3360, Perplexity: 28.1076\n",
      "Epoch [2/3], Step [10300/41412], Loss: 2.4623, Perplexity: 11.7315\n",
      "Epoch [2/3], Step [10400/41412], Loss: 3.5509, Perplexity: 34.84476\n",
      "Epoch [2/3], Step [10500/41412], Loss: 2.8161, Perplexity: 16.7124\n",
      "Epoch [2/3], Step [10600/41412], Loss: 3.1798, Perplexity: 24.0412\n",
      "Epoch [2/3], Step [10700/41412], Loss: 3.1441, Perplexity: 23.1979\n",
      "Epoch [2/3], Step [10800/41412], Loss: 3.2676, Perplexity: 26.2470\n",
      "Epoch [2/3], Step [10900/41412], Loss: 2.9575, Perplexity: 19.2495\n",
      "Epoch [2/3], Step [11000/41412], Loss: 2.9062, Perplexity: 18.2876\n",
      "Epoch [2/3], Step [11100/41412], Loss: 3.6911, Perplexity: 40.0907\n",
      "Epoch [2/3], Step [11200/41412], Loss: 2.9695, Perplexity: 19.48232\n",
      "Epoch [2/3], Step [11300/41412], Loss: 3.3457, Perplexity: 28.3813\n",
      "Epoch [2/3], Step [11400/41412], Loss: 4.2351, Perplexity: 69.0691\n",
      "Epoch [2/3], Step [11500/41412], Loss: 3.2064, Perplexity: 24.6911\n",
      "Epoch [2/3], Step [11600/41412], Loss: 2.5986, Perplexity: 13.4444\n",
      "Epoch [2/3], Step [11700/41412], Loss: 4.1237, Perplexity: 61.7869\n",
      "Epoch [2/3], Step [11800/41412], Loss: 3.3028, Perplexity: 27.1888\n",
      "Epoch [2/3], Step [11900/41412], Loss: 3.2041, Perplexity: 24.6322\n",
      "Epoch [2/3], Step [12000/41412], Loss: 3.6273, Perplexity: 37.6119\n",
      "Epoch [2/3], Step [12100/41412], Loss: 3.6168, Perplexity: 37.21700\n",
      "Epoch [2/3], Step [12200/41412], Loss: 2.6594, Perplexity: 14.28763\n",
      "Epoch [2/3], Step [12300/41412], Loss: 2.7731, Perplexity: 16.0081\n",
      "Epoch [2/3], Step [12400/41412], Loss: 3.0495, Perplexity: 21.1052\n",
      "Epoch [2/3], Step [12500/41412], Loss: 3.0807, Perplexity: 21.7739\n",
      "Epoch [2/3], Step [12600/41412], Loss: 2.8314, Perplexity: 16.9690\n",
      "Epoch [2/3], Step [12700/41412], Loss: 2.8969, Perplexity: 18.11781\n",
      "Epoch [2/3], Step [12800/41412], Loss: 3.3343, Perplexity: 28.0593\n",
      "Epoch [2/3], Step [12900/41412], Loss: 2.9822, Perplexity: 19.7308\n",
      "Epoch [2/3], Step [13000/41412], Loss: 2.7144, Perplexity: 15.0958\n",
      "Epoch [2/3], Step [13100/41412], Loss: 3.2979, Perplexity: 27.0548\n",
      "Epoch [2/3], Step [13200/41412], Loss: 3.1418, Perplexity: 23.14611\n",
      "Epoch [2/3], Step [13300/41412], Loss: 2.6033, Perplexity: 13.50774\n",
      "Epoch [2/3], Step [13400/41412], Loss: 3.2615, Perplexity: 26.0895\n",
      "Epoch [2/3], Step [13500/41412], Loss: 3.0026, Perplexity: 20.1377\n",
      "Epoch [2/3], Step [13600/41412], Loss: 2.8151, Perplexity: 16.6947\n",
      "Epoch [2/3], Step [13700/41412], Loss: 2.6590, Perplexity: 14.2821\n",
      "Epoch [2/3], Step [13800/41412], Loss: 3.0878, Perplexity: 21.9296\n",
      "Epoch [2/3], Step [13900/41412], Loss: 2.9662, Perplexity: 19.4183\n",
      "Epoch [2/3], Step [14000/41412], Loss: 3.5373, Perplexity: 34.37527\n",
      "Epoch [2/3], Step [14100/41412], Loss: 3.4021, Perplexity: 30.02752\n",
      "Epoch [2/3], Step [14200/41412], Loss: 3.2307, Perplexity: 25.2968\n",
      "Epoch [2/3], Step [14300/41412], Loss: 2.8326, Perplexity: 16.9891\n",
      "Epoch [2/3], Step [14400/41412], Loss: 3.8552, Perplexity: 47.2398\n",
      "Epoch [2/3], Step [14500/41412], Loss: 3.3285, Perplexity: 27.8955\n",
      "Epoch [2/3], Step [14600/41412], Loss: 2.9757, Perplexity: 19.6041\n",
      "Epoch [2/3], Step [14700/41412], Loss: 2.6614, Perplexity: 14.3166\n",
      "Epoch [2/3], Step [14800/41412], Loss: 2.4688, Perplexity: 11.80800\n",
      "Epoch [2/3], Step [14900/41412], Loss: 2.6389, Perplexity: 13.9972\n",
      "Epoch [2/3], Step [15000/41412], Loss: 3.0263, Perplexity: 20.6214\n",
      "Epoch [2/3], Step [15100/41412], Loss: 2.9106, Perplexity: 18.36832\n",
      "Epoch [2/3], Step [15200/41412], Loss: 2.7486, Perplexity: 15.6204\n",
      "Epoch [2/3], Step [15300/41412], Loss: 2.9761, Perplexity: 19.6109\n",
      "Epoch [2/3], Step [15400/41412], Loss: 2.9380, Perplexity: 18.8784\n",
      "Epoch [2/3], Step [15500/41412], Loss: 2.7255, Perplexity: 15.2643\n",
      "Epoch [2/3], Step [15600/41412], Loss: 2.6191, Perplexity: 13.7233\n",
      "Epoch [2/3], Step [15700/41412], Loss: 2.7560, Perplexity: 15.7363\n",
      "Epoch [2/3], Step [15800/41412], Loss: 2.4976, Perplexity: 12.1538\n",
      "Epoch [2/3], Step [15900/41412], Loss: 2.5815, Perplexity: 13.2164\n",
      "Epoch [2/3], Step [16000/41412], Loss: 3.0814, Perplexity: 21.7890\n",
      "Epoch [2/3], Step [16100/41412], Loss: 2.9185, Perplexity: 18.5142\n",
      "Epoch [2/3], Step [16200/41412], Loss: 2.3219, Perplexity: 10.1954\n",
      "Epoch [2/3], Step [16300/41412], Loss: 3.5385, Perplexity: 34.4152\n",
      "Epoch [2/3], Step [16400/41412], Loss: 2.6574, Perplexity: 14.2598\n",
      "Epoch [2/3], Step [16500/41412], Loss: 3.8481, Perplexity: 46.9021\n",
      "Epoch [2/3], Step [16600/41412], Loss: 2.8888, Perplexity: 17.9715\n",
      "Epoch [2/3], Step [16700/41412], Loss: 3.2093, Perplexity: 24.76235\n",
      "Epoch [2/3], Step [16800/41412], Loss: 2.4912, Perplexity: 12.0754\n",
      "Epoch [2/3], Step [16900/41412], Loss: 2.6847, Perplexity: 14.6536\n",
      "Epoch [2/3], Step [17000/41412], Loss: 3.4550, Perplexity: 31.6580\n",
      "Epoch [2/3], Step [17100/41412], Loss: 3.2345, Perplexity: 25.3928\n",
      "Epoch [2/3], Step [17200/41412], Loss: 3.4931, Perplexity: 32.8877\n",
      "Epoch [2/3], Step [17300/41412], Loss: 3.1084, Perplexity: 22.3859\n",
      "Epoch [2/3], Step [17400/41412], Loss: 4.5710, Perplexity: 96.6398\n",
      "Epoch [2/3], Step [17500/41412], Loss: 2.5331, Perplexity: 12.5924\n",
      "Epoch [2/3], Step [17600/41412], Loss: 2.5437, Perplexity: 12.7264\n",
      "Epoch [2/3], Step [17700/41412], Loss: 3.0954, Perplexity: 22.0968\n",
      "Epoch [2/3], Step [17800/41412], Loss: 3.1893, Perplexity: 24.2707\n",
      "Epoch [2/3], Step [17900/41412], Loss: 2.7390, Perplexity: 15.4723\n",
      "Epoch [2/3], Step [18000/41412], Loss: 3.2861, Perplexity: 26.73909\n",
      "Epoch [2/3], Step [18100/41412], Loss: 3.7841, Perplexity: 43.9944\n",
      "Epoch [2/3], Step [18200/41412], Loss: 2.6229, Perplexity: 13.7755\n",
      "Epoch [2/3], Step [18300/41412], Loss: 3.2601, Perplexity: 26.0509\n",
      "Epoch [2/3], Step [18400/41412], Loss: 2.3395, Perplexity: 10.3762\n",
      "Epoch [2/3], Step [18500/41412], Loss: 3.3723, Perplexity: 29.1459\n",
      "Epoch [2/3], Step [18600/41412], Loss: 2.8730, Perplexity: 17.6904\n",
      "Epoch [2/3], Step [18700/41412], Loss: 3.4689, Perplexity: 32.1017\n",
      "Epoch [2/3], Step [18800/41412], Loss: 2.9946, Perplexity: 19.9767\n",
      "Epoch [2/3], Step [18900/41412], Loss: 3.4016, Perplexity: 30.0134\n",
      "Epoch [2/3], Step [19000/41412], Loss: 2.8028, Perplexity: 16.4915\n",
      "Epoch [2/3], Step [19100/41412], Loss: 3.0442, Perplexity: 20.9937\n",
      "Epoch [2/3], Step [19200/41412], Loss: 2.4368, Perplexity: 11.43656\n",
      "Epoch [2/3], Step [19300/41412], Loss: 2.8524, Perplexity: 17.32863\n",
      "Epoch [2/3], Step [19400/41412], Loss: 2.2748, Perplexity: 9.72651\n",
      "Epoch [2/3], Step [19500/41412], Loss: 2.8743, Perplexity: 17.7131\n",
      "Epoch [2/3], Step [19600/41412], Loss: 2.6840, Perplexity: 14.6430\n",
      "Epoch [2/3], Step [19700/41412], Loss: 2.7271, Perplexity: 15.28801\n",
      "Epoch [2/3], Step [19800/41412], Loss: 2.3462, Perplexity: 10.4457\n",
      "Epoch [2/3], Step [19900/41412], Loss: 3.4336, Perplexity: 30.9895\n",
      "Epoch [2/3], Step [20000/41412], Loss: 3.1023, Perplexity: 22.2494\n",
      "Epoch [2/3], Step [20100/41412], Loss: 3.4487, Perplexity: 31.4596\n",
      "Epoch [2/3], Step [20200/41412], Loss: 3.2682, Perplexity: 26.26410\n",
      "Epoch [2/3], Step [20300/41412], Loss: 2.7082, Perplexity: 15.0019\n",
      "Epoch [2/3], Step [20400/41412], Loss: 3.3089, Perplexity: 27.3552\n",
      "Epoch [2/3], Step [20500/41412], Loss: 2.4967, Perplexity: 12.1419\n",
      "Epoch [2/3], Step [20600/41412], Loss: 2.3875, Perplexity: 10.8866\n",
      "Epoch [2/3], Step [20700/41412], Loss: 3.2978, Perplexity: 27.0530\n",
      "Epoch [2/3], Step [20800/41412], Loss: 3.8615, Perplexity: 47.5384\n",
      "Epoch [2/3], Step [20900/41412], Loss: 2.8286, Perplexity: 16.9223\n",
      "Epoch [2/3], Step [21000/41412], Loss: 2.9274, Perplexity: 18.6790\n",
      "Epoch [2/3], Step [21100/41412], Loss: 3.6696, Perplexity: 39.2344\n",
      "Epoch [2/3], Step [21200/41412], Loss: 3.6236, Perplexity: 37.4716\n",
      "Epoch [2/3], Step [21300/41412], Loss: 4.2275, Perplexity: 68.5490\n",
      "Epoch [2/3], Step [21400/41412], Loss: 3.2347, Perplexity: 25.39867\n",
      "Epoch [2/3], Step [21500/41412], Loss: 2.5773, Perplexity: 13.1615\n",
      "Epoch [2/3], Step [21600/41412], Loss: 3.1712, Perplexity: 23.8368\n",
      "Epoch [2/3], Step [21700/41412], Loss: 3.1557, Perplexity: 23.4698\n",
      "Epoch [2/3], Step [21800/41412], Loss: 2.4876, Perplexity: 12.0323\n",
      "Epoch [2/3], Step [21900/41412], Loss: 2.8472, Perplexity: 17.2400\n",
      "Epoch [2/3], Step [22000/41412], Loss: 3.3551, Perplexity: 28.6494\n",
      "Epoch [2/3], Step [22100/41412], Loss: 2.8659, Perplexity: 17.5641\n",
      "Epoch [2/3], Step [22200/41412], Loss: 1.5785, Perplexity: 4.84797\n",
      "Epoch [2/3], Step [22300/41412], Loss: 3.5096, Perplexity: 33.43640\n",
      "Epoch [2/3], Step [22400/41412], Loss: 3.2948, Perplexity: 26.9715\n",
      "Epoch [2/3], Step [22500/41412], Loss: 2.3355, Perplexity: 10.3344\n",
      "Epoch [2/3], Step [22600/41412], Loss: 2.5376, Perplexity: 12.64921\n",
      "Epoch [2/3], Step [22700/41412], Loss: 3.4979, Perplexity: 33.0452\n",
      "Epoch [2/3], Step [22800/41412], Loss: 3.7559, Perplexity: 42.7722\n",
      "Epoch [2/3], Step [22900/41412], Loss: 2.5778, Perplexity: 13.1685\n",
      "Epoch [2/3], Step [23000/41412], Loss: 2.7597, Perplexity: 15.7957\n",
      "Epoch [2/3], Step [23100/41412], Loss: 3.9215, Perplexity: 50.4771\n",
      "Epoch [2/3], Step [23200/41412], Loss: 2.7293, Perplexity: 15.32173\n",
      "Epoch [2/3], Step [23300/41412], Loss: 2.2367, Perplexity: 9.36278\n",
      "Epoch [2/3], Step [23400/41412], Loss: 2.2119, Perplexity: 9.13279\n",
      "Epoch [2/3], Step [23500/41412], Loss: 3.4120, Perplexity: 30.3249\n",
      "Epoch [2/3], Step [23600/41412], Loss: 3.3654, Perplexity: 28.9453\n",
      "Epoch [2/3], Step [23700/41412], Loss: 2.8702, Perplexity: 17.6405\n",
      "Epoch [2/3], Step [23800/41412], Loss: 2.0902, Perplexity: 8.08695\n",
      "Epoch [2/3], Step [23900/41412], Loss: 2.7352, Perplexity: 15.4129\n",
      "Epoch [2/3], Step [24000/41412], Loss: 3.2782, Perplexity: 26.5278\n",
      "Epoch [2/3], Step [24100/41412], Loss: 2.2013, Perplexity: 9.03687\n",
      "Epoch [2/3], Step [24200/41412], Loss: 2.9507, Perplexity: 19.1194\n",
      "Epoch [2/3], Step [24300/41412], Loss: 2.8301, Perplexity: 16.9469\n",
      "Epoch [2/3], Step [24400/41412], Loss: 2.6838, Perplexity: 14.6412\n",
      "Epoch [2/3], Step [24500/41412], Loss: 2.8037, Perplexity: 16.50626\n",
      "Epoch [2/3], Step [24600/41412], Loss: 2.5466, Perplexity: 12.7639\n",
      "Epoch [2/3], Step [24700/41412], Loss: 2.4757, Perplexity: 11.8898\n",
      "Epoch [2/3], Step [24800/41412], Loss: 2.9558, Perplexity: 19.2164\n",
      "Epoch [2/3], Step [24900/41412], Loss: 2.6480, Perplexity: 14.1262\n",
      "Epoch [2/3], Step [25000/41412], Loss: 2.8192, Perplexity: 16.7640\n",
      "Epoch [2/3], Step [25100/41412], Loss: 3.1475, Perplexity: 23.2789\n",
      "Epoch [2/3], Step [25200/41412], Loss: 2.7168, Perplexity: 15.1313\n",
      "Epoch [2/3], Step [25300/41412], Loss: 2.8274, Perplexity: 16.9007\n",
      "Epoch [2/3], Step [25400/41412], Loss: 2.4908, Perplexity: 12.0708\n",
      "Epoch [2/3], Step [25500/41412], Loss: 2.4435, Perplexity: 11.5132\n",
      "Epoch [2/3], Step [25600/41412], Loss: 2.6169, Perplexity: 13.6937\n",
      "Epoch [2/3], Step [25700/41412], Loss: 2.4443, Perplexity: 11.5226\n",
      "Epoch [2/3], Step [25800/41412], Loss: 2.5875, Perplexity: 13.2969\n",
      "Epoch [2/3], Step [25900/41412], Loss: 2.8893, Perplexity: 17.98067\n",
      "Epoch [2/3], Step [26000/41412], Loss: 3.1859, Perplexity: 24.1886\n",
      "Epoch [2/3], Step [26100/41412], Loss: 3.2890, Perplexity: 26.8165\n",
      "Epoch [2/3], Step [26200/41412], Loss: 2.8887, Perplexity: 17.9702\n",
      "Epoch [2/3], Step [26300/41412], Loss: 2.8371, Perplexity: 17.0664\n",
      "Epoch [2/3], Step [26400/41412], Loss: 3.1713, Perplexity: 23.8377\n",
      "Epoch [2/3], Step [26500/41412], Loss: 2.8189, Perplexity: 16.7581\n",
      "Epoch [2/3], Step [26600/41412], Loss: 2.5509, Perplexity: 12.8192\n",
      "Epoch [2/3], Step [26700/41412], Loss: 2.7038, Perplexity: 14.9371\n",
      "Epoch [2/3], Step [26800/41412], Loss: 2.0127, Perplexity: 7.48357\n",
      "Epoch [2/3], Step [26900/41412], Loss: 3.3609, Perplexity: 28.8146\n",
      "Epoch [2/3], Step [27000/41412], Loss: 2.9093, Perplexity: 18.3437\n",
      "Epoch [2/3], Step [27100/41412], Loss: 3.5330, Perplexity: 34.22588\n",
      "Epoch [2/3], Step [27200/41412], Loss: 2.6457, Perplexity: 14.0934\n",
      "Epoch [2/3], Step [27300/41412], Loss: 2.6951, Perplexity: 14.8063\n",
      "Epoch [2/3], Step [27400/41412], Loss: 2.7550, Perplexity: 15.7209\n",
      "Epoch [2/3], Step [27500/41412], Loss: 3.1120, Perplexity: 22.4655\n",
      "Epoch [2/3], Step [27600/41412], Loss: 2.8637, Perplexity: 17.5259\n",
      "Epoch [2/3], Step [27700/41412], Loss: 2.9507, Perplexity: 19.1199\n",
      "Epoch [2/3], Step [27800/41412], Loss: 2.4684, Perplexity: 11.8034\n",
      "Epoch [2/3], Step [27900/41412], Loss: 3.4614, Perplexity: 31.8608\n",
      "Epoch [2/3], Step [28000/41412], Loss: 2.6390, Perplexity: 13.9994\n",
      "Epoch [2/3], Step [28100/41412], Loss: 2.8454, Perplexity: 17.2078\n",
      "Epoch [2/3], Step [28200/41412], Loss: 2.9806, Perplexity: 19.6990\n",
      "Epoch [2/3], Step [28300/41412], Loss: 2.9173, Perplexity: 18.4917\n",
      "Epoch [2/3], Step [28400/41412], Loss: 2.7873, Perplexity: 16.2379\n",
      "Epoch [2/3], Step [28500/41412], Loss: 2.6596, Perplexity: 14.29023\n",
      "Epoch [2/3], Step [28600/41412], Loss: 3.2040, Perplexity: 24.6300\n",
      "Epoch [2/3], Step [28700/41412], Loss: 3.2452, Perplexity: 25.6669\n",
      "Epoch [2/3], Step [28800/41412], Loss: 3.0209, Perplexity: 20.5107\n",
      "Epoch [2/3], Step [28900/41412], Loss: 2.7330, Perplexity: 15.37858\n",
      "Epoch [2/3], Step [29000/41412], Loss: 2.4724, Perplexity: 11.8513\n",
      "Epoch [2/3], Step [29100/41412], Loss: 3.5294, Perplexity: 34.1047\n",
      "Epoch [2/3], Step [29200/41412], Loss: 2.2986, Perplexity: 9.96037\n",
      "Epoch [2/3], Step [29300/41412], Loss: 2.8283, Perplexity: 16.9161\n",
      "Epoch [2/3], Step [29400/41412], Loss: 3.0496, Perplexity: 21.1059\n",
      "Epoch [2/3], Step [29500/41412], Loss: 3.2517, Perplexity: 25.83526\n",
      "Epoch [2/3], Step [29600/41412], Loss: 2.5680, Perplexity: 13.0403\n",
      "Epoch [2/3], Step [29700/41412], Loss: 2.6170, Perplexity: 13.6947\n",
      "Epoch [2/3], Step [29800/41412], Loss: 1.9433, Perplexity: 6.981939\n",
      "Epoch [2/3], Step [29900/41412], Loss: 3.0011, Perplexity: 20.10792\n",
      "Epoch [2/3], Step [30000/41412], Loss: 2.3234, Perplexity: 10.2104\n",
      "Epoch [2/3], Step [30100/41412], Loss: 3.6353, Perplexity: 37.9138\n",
      "Epoch [2/3], Step [30200/41412], Loss: 3.1428, Perplexity: 23.1683\n",
      "Epoch [2/3], Step [30300/41412], Loss: 2.2140, Perplexity: 9.15260\n",
      "Epoch [2/3], Step [30400/41412], Loss: 2.6080, Perplexity: 13.5722\n",
      "Epoch [2/3], Step [30500/41412], Loss: 2.8296, Perplexity: 16.9392\n",
      "Epoch [2/3], Step [30600/41412], Loss: 2.3152, Perplexity: 10.12685\n",
      "Epoch [2/3], Step [30700/41412], Loss: 3.1457, Perplexity: 23.2357\n",
      "Epoch [2/3], Step [30800/41412], Loss: 2.9820, Perplexity: 19.72726\n",
      "Epoch [2/3], Step [30900/41412], Loss: 2.9745, Perplexity: 19.5802\n",
      "Epoch [2/3], Step [31000/41412], Loss: 2.6893, Perplexity: 14.7215\n",
      "Epoch [2/3], Step [31100/41412], Loss: 3.0618, Perplexity: 21.3661\n",
      "Epoch [2/3], Step [31200/41412], Loss: 2.8103, Perplexity: 16.61457\n",
      "Epoch [2/3], Step [31300/41412], Loss: 2.3906, Perplexity: 10.9196\n",
      "Epoch [2/3], Step [31400/41412], Loss: 2.2767, Perplexity: 9.74425\n",
      "Epoch [2/3], Step [31500/41412], Loss: 2.2704, Perplexity: 9.68329\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/3], Step [31600/41412], Loss: 2.8577, Perplexity: 17.4210\n",
      "Epoch [2/3], Step [31700/41412], Loss: 2.6289, Perplexity: 13.8586\n",
      "Epoch [2/3], Step [31800/41412], Loss: 2.8059, Perplexity: 16.5414\n",
      "Epoch [2/3], Step [31900/41412], Loss: 2.8814, Perplexity: 17.8389\n",
      "Epoch [2/3], Step [32000/41412], Loss: 3.5493, Perplexity: 34.7907\n",
      "Epoch [2/3], Step [32100/41412], Loss: 2.7912, Perplexity: 16.3009\n",
      "Epoch [2/3], Step [32200/41412], Loss: 2.4666, Perplexity: 11.7825\n",
      "Epoch [2/3], Step [32300/41412], Loss: 2.5734, Perplexity: 13.11086\n",
      "Epoch [2/3], Step [32400/41412], Loss: 2.8498, Perplexity: 17.2849\n",
      "Epoch [2/3], Step [32500/41412], Loss: 3.0873, Perplexity: 21.9186\n",
      "Epoch [2/3], Step [32600/41412], Loss: 2.7877, Perplexity: 16.2441\n",
      "Epoch [2/3], Step [32700/41412], Loss: 3.0679, Perplexity: 21.49596\n",
      "Epoch [2/3], Step [32800/41412], Loss: 2.9611, Perplexity: 19.3195\n",
      "Epoch [2/3], Step [32900/41412], Loss: 2.6340, Perplexity: 13.9292\n",
      "Epoch [2/3], Step [33000/41412], Loss: 3.7671, Perplexity: 43.2561\n",
      "Epoch [2/3], Step [33100/41412], Loss: 2.6385, Perplexity: 13.9923\n",
      "Epoch [2/3], Step [33200/41412], Loss: 2.4210, Perplexity: 11.2568\n",
      "Epoch [2/3], Step [33300/41412], Loss: 2.9540, Perplexity: 19.1834\n",
      "Epoch [2/3], Step [33400/41412], Loss: 3.3426, Perplexity: 28.2916\n",
      "Epoch [2/3], Step [33500/41412], Loss: 2.7945, Perplexity: 16.3543\n",
      "Epoch [2/3], Step [33600/41412], Loss: 2.8257, Perplexity: 16.8720\n",
      "Epoch [2/3], Step [33700/41412], Loss: 2.7674, Perplexity: 15.9178\n",
      "Epoch [2/3], Step [33800/41412], Loss: 3.6309, Perplexity: 37.7473\n",
      "Epoch [2/3], Step [33900/41412], Loss: 3.6458, Perplexity: 38.3123\n",
      "Epoch [2/3], Step [34000/41412], Loss: 2.9937, Perplexity: 19.9593\n",
      "Epoch [2/3], Step [34100/41412], Loss: 2.4085, Perplexity: 11.1168\n",
      "Epoch [2/3], Step [34200/41412], Loss: 2.6769, Perplexity: 14.53938\n",
      "Epoch [2/3], Step [34300/41412], Loss: 2.5382, Perplexity: 12.6570\n",
      "Epoch [2/3], Step [34400/41412], Loss: 2.9046, Perplexity: 18.25730\n",
      "Epoch [2/3], Step [34500/41412], Loss: 2.8698, Perplexity: 17.6334\n",
      "Epoch [2/3], Step [34600/41412], Loss: 2.6991, Perplexity: 14.86693\n",
      "Epoch [2/3], Step [34700/41412], Loss: 3.0930, Perplexity: 22.0440\n",
      "Epoch [2/3], Step [34800/41412], Loss: 3.0097, Perplexity: 20.2814\n",
      "Epoch [2/3], Step [34900/41412], Loss: 3.3485, Perplexity: 28.4596\n",
      "Epoch [2/3], Step [35000/41412], Loss: 3.1457, Perplexity: 23.2357\n",
      "Epoch [2/3], Step [35100/41412], Loss: 2.9805, Perplexity: 19.6980\n",
      "Epoch [2/3], Step [35200/41412], Loss: 3.0284, Perplexity: 20.6646\n",
      "Epoch [2/3], Step [35300/41412], Loss: 2.9130, Perplexity: 18.4118\n",
      "Epoch [2/3], Step [35400/41412], Loss: 2.6645, Perplexity: 14.3602\n",
      "Epoch [2/3], Step [35500/41412], Loss: 3.0442, Perplexity: 20.9927\n",
      "Epoch [2/3], Step [35600/41412], Loss: 2.7299, Perplexity: 15.3307\n",
      "Epoch [2/3], Step [35700/41412], Loss: 2.4775, Perplexity: 11.9117\n",
      "Epoch [2/3], Step [35800/41412], Loss: 2.6659, Perplexity: 14.3802\n",
      "Epoch [2/3], Step [35900/41412], Loss: 3.0717, Perplexity: 21.5785\n",
      "Epoch [2/3], Step [36000/41412], Loss: 2.7796, Perplexity: 16.1125\n",
      "Epoch [2/3], Step [36100/41412], Loss: 2.7650, Perplexity: 15.8792\n",
      "Epoch [2/3], Step [36200/41412], Loss: 2.8253, Perplexity: 16.8665\n",
      "Epoch [2/3], Step [36300/41412], Loss: 3.2653, Perplexity: 26.1884\n",
      "Epoch [2/3], Step [36400/41412], Loss: 2.3500, Perplexity: 10.4852\n",
      "Epoch [2/3], Step [36500/41412], Loss: 2.6802, Perplexity: 14.5883\n",
      "Epoch [2/3], Step [36600/41412], Loss: 2.4574, Perplexity: 11.6747\n",
      "Epoch [2/3], Step [36700/41412], Loss: 3.1973, Perplexity: 24.46546\n",
      "Epoch [2/3], Step [36800/41412], Loss: 3.5349, Perplexity: 34.2917\n",
      "Epoch [2/3], Step [36900/41412], Loss: 2.9781, Perplexity: 19.6507\n",
      "Epoch [2/3], Step [37000/41412], Loss: 2.4397, Perplexity: 11.4695\n",
      "Epoch [2/3], Step [37100/41412], Loss: 2.8142, Perplexity: 16.6800\n",
      "Epoch [2/3], Step [37200/41412], Loss: 2.6427, Perplexity: 14.0505\n",
      "Epoch [2/3], Step [37300/41412], Loss: 3.0331, Perplexity: 20.7609\n",
      "Epoch [2/3], Step [37400/41412], Loss: 2.4222, Perplexity: 11.27086\n",
      "Epoch [2/3], Step [37500/41412], Loss: 3.1383, Perplexity: 23.0654\n",
      "Epoch [2/3], Step [37600/41412], Loss: 2.3394, Perplexity: 10.37468\n",
      "Epoch [2/3], Step [37700/41412], Loss: 3.4048, Perplexity: 30.1096\n",
      "Epoch [2/3], Step [37800/41412], Loss: 3.8822, Perplexity: 48.5313\n",
      "Epoch [2/3], Step [37900/41412], Loss: 2.6098, Perplexity: 13.5962\n",
      "Epoch [2/3], Step [38000/41412], Loss: 2.7244, Perplexity: 15.2475\n",
      "Epoch [2/3], Step [38100/41412], Loss: 2.7165, Perplexity: 15.1276\n",
      "Epoch [2/3], Step [38200/41412], Loss: 3.1633, Perplexity: 23.6482\n",
      "Epoch [2/3], Step [38300/41412], Loss: 2.3433, Perplexity: 10.4156\n",
      "Epoch [2/3], Step [38400/41412], Loss: 2.4991, Perplexity: 12.17185\n",
      "Epoch [2/3], Step [38500/41412], Loss: 2.7817, Perplexity: 16.1470\n",
      "Epoch [2/3], Step [38600/41412], Loss: 3.2315, Perplexity: 25.3171\n",
      "Epoch [2/3], Step [38700/41412], Loss: 3.0424, Perplexity: 20.9558\n",
      "Epoch [2/3], Step [38800/41412], Loss: 3.1005, Perplexity: 22.2101\n",
      "Epoch [2/3], Step [38900/41412], Loss: 3.5790, Perplexity: 35.8386\n",
      "Epoch [2/3], Step [39000/41412], Loss: 2.4240, Perplexity: 11.2911\n",
      "Epoch [2/3], Step [39100/41412], Loss: 2.9037, Perplexity: 18.2417\n",
      "Epoch [2/3], Step [39200/41412], Loss: 2.8241, Perplexity: 16.8464\n",
      "Epoch [2/3], Step [39300/41412], Loss: 2.9580, Perplexity: 19.2589\n",
      "Epoch [2/3], Step [39400/41412], Loss: 2.7663, Perplexity: 15.8997\n",
      "Epoch [2/3], Step [39500/41412], Loss: 2.8242, Perplexity: 16.8472\n",
      "Epoch [2/3], Step [39600/41412], Loss: 3.0905, Perplexity: 21.9873\n",
      "Epoch [2/3], Step [39700/41412], Loss: 2.7025, Perplexity: 14.9168\n",
      "Epoch [2/3], Step [39800/41412], Loss: 3.5848, Perplexity: 36.0470\n",
      "Epoch [2/3], Step [39900/41412], Loss: 2.6729, Perplexity: 14.4822\n",
      "Epoch [2/3], Step [40000/41412], Loss: 2.4502, Perplexity: 11.5911\n",
      "Epoch [2/3], Step [40100/41412], Loss: 3.0449, Perplexity: 21.0081\n",
      "Epoch [2/3], Step [40200/41412], Loss: 3.1596, Perplexity: 23.5604\n",
      "Epoch [2/3], Step [40300/41412], Loss: 2.9522, Perplexity: 19.1488\n",
      "Epoch [2/3], Step [40400/41412], Loss: 2.6363, Perplexity: 13.96101\n",
      "Epoch [2/3], Step [40500/41412], Loss: 3.7327, Perplexity: 41.7898\n",
      "Epoch [2/3], Step [40600/41412], Loss: 2.9064, Perplexity: 18.29076\n",
      "Epoch [2/3], Step [40700/41412], Loss: 2.9948, Perplexity: 19.9815\n",
      "Epoch [2/3], Step [40800/41412], Loss: 2.4535, Perplexity: 11.6292\n",
      "Epoch [2/3], Step [40900/41412], Loss: 3.3319, Perplexity: 27.9913\n",
      "Epoch [2/3], Step [41000/41412], Loss: 3.0638, Perplexity: 21.4093\n",
      "Epoch [2/3], Step [41100/41412], Loss: 3.1573, Perplexity: 23.5067\n",
      "Epoch [2/3], Step [41200/41412], Loss: 2.5992, Perplexity: 13.4525\n",
      "Epoch [2/3], Step [41300/41412], Loss: 2.9306, Perplexity: 18.7395\n",
      "Epoch [2/3], Step [41400/41412], Loss: 2.9389, Perplexity: 18.8954\n",
      "Epoch [3/3], Step [100/41412], Loss: 2.9814, Perplexity: 19.715395\n",
      "Epoch [3/3], Step [200/41412], Loss: 2.5776, Perplexity: 13.1655\n",
      "Epoch [3/3], Step [300/41412], Loss: 2.7550, Perplexity: 15.7208\n",
      "Epoch [3/3], Step [400/41412], Loss: 2.7112, Perplexity: 15.0474\n",
      "Epoch [3/3], Step [500/41412], Loss: 2.7678, Perplexity: 15.92328\n",
      "Epoch [3/3], Step [600/41412], Loss: 3.3602, Perplexity: 28.7953\n",
      "Epoch [3/3], Step [700/41412], Loss: 2.9897, Perplexity: 19.8798\n",
      "Epoch [3/3], Step [800/41412], Loss: 2.7098, Perplexity: 15.02582\n",
      "Epoch [3/3], Step [900/41412], Loss: 2.2623, Perplexity: 9.60475\n",
      "Epoch [3/3], Step [1000/41412], Loss: 3.4487, Perplexity: 31.4591\n",
      "Epoch [3/3], Step [1100/41412], Loss: 3.1499, Perplexity: 23.3327\n",
      "Epoch [3/3], Step [1200/41412], Loss: 2.2744, Perplexity: 9.72193\n",
      "Epoch [3/3], Step [1300/41412], Loss: 2.9988, Perplexity: 20.0623\n",
      "Epoch [3/3], Step [1400/41412], Loss: 2.7660, Perplexity: 15.8953\n",
      "Epoch [3/3], Step [1500/41412], Loss: 2.7223, Perplexity: 15.2158\n",
      "Epoch [3/3], Step [1600/41412], Loss: 2.9496, Perplexity: 19.0976\n",
      "Epoch [3/3], Step [1700/41412], Loss: 3.2311, Perplexity: 25.30848\n",
      "Epoch [3/3], Step [1800/41412], Loss: 3.4274, Perplexity: 30.7957\n",
      "Epoch [3/3], Step [1900/41412], Loss: 3.5192, Perplexity: 33.7577\n",
      "Epoch [3/3], Step [2000/41412], Loss: 2.8564, Perplexity: 17.3989\n",
      "Epoch [3/3], Step [2100/41412], Loss: 2.8177, Perplexity: 16.7382\n",
      "Epoch [3/3], Step [2200/41412], Loss: 3.1463, Perplexity: 23.2501\n",
      "Epoch [3/3], Step [2300/41412], Loss: 2.4360, Perplexity: 11.4278\n",
      "Epoch [3/3], Step [2400/41412], Loss: 3.3000, Perplexity: 27.1133\n",
      "Epoch [3/3], Step [2500/41412], Loss: 3.2318, Perplexity: 25.3245\n",
      "Epoch [3/3], Step [2600/41412], Loss: 2.7216, Perplexity: 15.2053\n",
      "Epoch [3/3], Step [2700/41412], Loss: 3.3827, Perplexity: 29.4492\n",
      "Epoch [3/3], Step [2800/41412], Loss: 2.6343, Perplexity: 13.93342\n",
      "Epoch [3/3], Step [2900/41412], Loss: 3.1883, Perplexity: 24.2467\n",
      "Epoch [3/3], Step [3000/41412], Loss: 3.0512, Perplexity: 21.1398\n",
      "Epoch [3/3], Step [3100/41412], Loss: 2.5846, Perplexity: 13.2574\n",
      "Epoch [3/3], Step [3200/41412], Loss: 3.2436, Perplexity: 25.6252\n",
      "Epoch [3/3], Step [3300/41412], Loss: 2.6060, Perplexity: 13.5448\n",
      "Epoch [3/3], Step [3400/41412], Loss: 2.7844, Perplexity: 16.1898\n",
      "Epoch [3/3], Step [3500/41412], Loss: 3.0020, Perplexity: 20.1262\n",
      "Epoch [3/3], Step [3600/41412], Loss: 2.8148, Perplexity: 16.6897\n",
      "Epoch [3/3], Step [3700/41412], Loss: 3.0801, Perplexity: 21.7615\n",
      "Epoch [3/3], Step [3800/41412], Loss: 3.1308, Perplexity: 22.8925\n",
      "Epoch [3/3], Step [3900/41412], Loss: 3.0682, Perplexity: 21.5039\n",
      "Epoch [3/3], Step [4000/41412], Loss: 3.0706, Perplexity: 21.5553\n",
      "Epoch [3/3], Step [4100/41412], Loss: 3.2246, Perplexity: 25.1441\n",
      "Epoch [3/3], Step [4200/41412], Loss: 2.2530, Perplexity: 9.51649\n",
      "Epoch [3/3], Step [4300/41412], Loss: 2.8532, Perplexity: 17.3439\n",
      "Epoch [3/3], Step [4400/41412], Loss: 3.6887, Perplexity: 39.9909\n",
      "Epoch [3/3], Step [4500/41412], Loss: 2.4856, Perplexity: 12.0086\n",
      "Epoch [3/3], Step [4600/41412], Loss: 2.4769, Perplexity: 11.9043\n",
      "Epoch [3/3], Step [4700/41412], Loss: 2.5054, Perplexity: 12.2484\n",
      "Epoch [3/3], Step [4800/41412], Loss: 2.6423, Perplexity: 14.0449\n",
      "Epoch [3/3], Step [4900/41412], Loss: 3.7393, Perplexity: 42.0698\n",
      "Epoch [3/3], Step [5000/41412], Loss: 3.8146, Perplexity: 45.3584\n",
      "Epoch [3/3], Step [5100/41412], Loss: 3.5017, Perplexity: 33.1715\n",
      "Epoch [3/3], Step [5200/41412], Loss: 3.1088, Perplexity: 22.39351\n",
      "Epoch [3/3], Step [5300/41412], Loss: 3.4720, Perplexity: 32.2021\n",
      "Epoch [3/3], Step [5400/41412], Loss: 2.5094, Perplexity: 12.29726\n",
      "Epoch [3/3], Step [5500/41412], Loss: 2.5568, Perplexity: 12.8948\n",
      "Epoch [3/3], Step [5600/41412], Loss: 2.7512, Perplexity: 15.6607\n",
      "Epoch [3/3], Step [5700/41412], Loss: 2.8177, Perplexity: 16.7391\n",
      "Epoch [3/3], Step [5800/41412], Loss: 2.9292, Perplexity: 18.7134\n",
      "Epoch [3/3], Step [5900/41412], Loss: 2.6484, Perplexity: 14.1313\n",
      "Epoch [3/3], Step [6000/41412], Loss: 2.1699, Perplexity: 8.75789\n",
      "Epoch [3/3], Step [6100/41412], Loss: 2.7638, Perplexity: 15.8598\n",
      "Epoch [3/3], Step [6200/41412], Loss: 2.5252, Perplexity: 12.4928\n",
      "Epoch [3/3], Step [6300/41412], Loss: 2.5379, Perplexity: 12.6532\n",
      "Epoch [3/3], Step [6400/41412], Loss: 3.1068, Perplexity: 22.3499\n",
      "Epoch [3/3], Step [6500/41412], Loss: 2.2353, Perplexity: 9.34942\n",
      "Epoch [3/3], Step [6600/41412], Loss: 2.5826, Perplexity: 13.23187\n",
      "Epoch [3/3], Step [6700/41412], Loss: 3.2120, Perplexity: 24.8275\n",
      "Epoch [3/3], Step [6800/41412], Loss: 2.7645, Perplexity: 15.8711\n",
      "Epoch [3/3], Step [6900/41412], Loss: 3.1144, Perplexity: 22.5189\n",
      "Epoch [3/3], Step [7000/41412], Loss: 3.3512, Perplexity: 28.5374\n",
      "Epoch [3/3], Step [7100/41412], Loss: 3.1182, Perplexity: 22.6052\n",
      "Epoch [3/3], Step [7200/41412], Loss: 2.7742, Perplexity: 16.0257\n",
      "Epoch [3/3], Step [7300/41412], Loss: 2.8454, Perplexity: 17.2087\n",
      "Epoch [3/3], Step [7400/41412], Loss: 2.8370, Perplexity: 17.0642\n",
      "Epoch [3/3], Step [7500/41412], Loss: 2.5620, Perplexity: 12.96146\n",
      "Epoch [3/3], Step [7600/41412], Loss: 2.0962, Perplexity: 8.13546\n",
      "Epoch [3/3], Step [7700/41412], Loss: 2.0225, Perplexity: 7.55761\n",
      "Epoch [3/3], Step [7800/41412], Loss: 3.3421, Perplexity: 28.2781\n",
      "Epoch [3/3], Step [7900/41412], Loss: 2.9700, Perplexity: 19.49141\n",
      "Epoch [3/3], Step [8000/41412], Loss: 3.0718, Perplexity: 21.5809\n",
      "Epoch [3/3], Step [8100/41412], Loss: 3.2797, Perplexity: 26.5684\n",
      "Epoch [3/3], Step [8200/41412], Loss: 3.1096, Perplexity: 22.4130\n",
      "Epoch [3/3], Step [8300/41412], Loss: 2.6305, Perplexity: 13.8808\n",
      "Epoch [3/3], Step [8400/41412], Loss: 3.1337, Perplexity: 22.95790\n",
      "Epoch [3/3], Step [8500/41412], Loss: 2.4768, Perplexity: 11.9034\n",
      "Epoch [3/3], Step [8600/41412], Loss: 3.0961, Perplexity: 22.1108\n",
      "Epoch [3/3], Step [8700/41412], Loss: 2.6220, Perplexity: 13.7637\n",
      "Epoch [3/3], Step [8800/41412], Loss: 2.3278, Perplexity: 10.2554\n",
      "Epoch [3/3], Step [8900/41412], Loss: 3.0453, Perplexity: 21.0172\n",
      "Epoch [3/3], Step [9000/41412], Loss: 2.2350, Perplexity: 9.34675\n",
      "Epoch [3/3], Step [9100/41412], Loss: 2.2260, Perplexity: 9.26301\n",
      "Epoch [3/3], Step [9200/41412], Loss: 2.3010, Perplexity: 9.98428\n",
      "Epoch [3/3], Step [9300/41412], Loss: 3.5755, Perplexity: 35.7109\n",
      "Epoch [3/3], Step [9400/41412], Loss: 2.5086, Perplexity: 12.28833\n",
      "Epoch [3/3], Step [9500/41412], Loss: 3.4138, Perplexity: 30.3804\n",
      "Epoch [3/3], Step [9600/41412], Loss: 1.9891, Perplexity: 7.30938\n",
      "Epoch [3/3], Step [9700/41412], Loss: 3.1133, Perplexity: 22.49450\n",
      "Epoch [3/3], Step [9800/41412], Loss: 2.7474, Perplexity: 15.6020\n",
      "Epoch [3/3], Step [9900/41412], Loss: 2.7419, Perplexity: 15.5171\n",
      "Epoch [3/3], Step [10000/41412], Loss: 2.4848, Perplexity: 11.9989\n",
      "Epoch [3/3], Step [10100/41412], Loss: 3.2514, Perplexity: 25.8263\n",
      "Epoch [3/3], Step [10200/41412], Loss: 2.7412, Perplexity: 15.50506\n",
      "Epoch [3/3], Step [10300/41412], Loss: 3.2805, Perplexity: 26.5888\n",
      "Epoch [3/3], Step [10400/41412], Loss: 3.1750, Perplexity: 23.9271\n",
      "Epoch [3/3], Step [10500/41412], Loss: 2.9498, Perplexity: 19.1017\n",
      "Epoch [3/3], Step [10600/41412], Loss: 2.7691, Perplexity: 15.94508\n",
      "Epoch [3/3], Step [10700/41412], Loss: 2.9392, Perplexity: 18.9006\n",
      "Epoch [3/3], Step [10800/41412], Loss: 3.1874, Perplexity: 24.2247\n",
      "Epoch [3/3], Step [10900/41412], Loss: 2.7363, Perplexity: 15.4299\n",
      "Epoch [3/3], Step [11000/41412], Loss: 2.6779, Perplexity: 14.5539\n",
      "Epoch [3/3], Step [11100/41412], Loss: 2.4524, Perplexity: 11.6165\n",
      "Epoch [3/3], Step [11200/41412], Loss: 2.3607, Perplexity: 10.5985\n",
      "Epoch [3/3], Step [11300/41412], Loss: 2.8052, Perplexity: 16.5296\n",
      "Epoch [3/3], Step [11400/41412], Loss: 3.3630, Perplexity: 28.8765\n",
      "Epoch [3/3], Step [11500/41412], Loss: 2.0357, Perplexity: 7.65750\n",
      "Epoch [3/3], Step [11600/41412], Loss: 3.0619, Perplexity: 21.3678\n",
      "Epoch [3/3], Step [11700/41412], Loss: 2.6054, Perplexity: 13.5362\n",
      "Epoch [3/3], Step [11800/41412], Loss: 2.4985, Perplexity: 12.1647\n",
      "Epoch [3/3], Step [11900/41412], Loss: 2.3022, Perplexity: 9.99572\n",
      "Epoch [3/3], Step [12000/41412], Loss: 2.7259, Perplexity: 15.2707\n",
      "Epoch [3/3], Step [12100/41412], Loss: 2.3263, Perplexity: 10.2405\n",
      "Epoch [3/3], Step [12200/41412], Loss: 2.9422, Perplexity: 18.9573\n",
      "Epoch [3/3], Step [12300/41412], Loss: 3.2108, Perplexity: 24.7983\n",
      "Epoch [3/3], Step [12400/41412], Loss: 2.7491, Perplexity: 15.6285\n",
      "Epoch [3/3], Step [12500/41412], Loss: 2.3867, Perplexity: 10.8772\n",
      "Epoch [3/3], Step [12600/41412], Loss: 3.1169, Perplexity: 22.57609\n",
      "Epoch [3/3], Step [12700/41412], Loss: 3.0409, Perplexity: 20.9236\n",
      "Epoch [3/3], Step [12800/41412], Loss: 2.6924, Perplexity: 14.7671\n",
      "Epoch [3/3], Step [12900/41412], Loss: 2.8086, Perplexity: 16.5869\n",
      "Epoch [3/3], Step [13000/41412], Loss: 3.4586, Perplexity: 31.7738\n",
      "Epoch [3/3], Step [13100/41412], Loss: 2.5348, Perplexity: 12.6138\n",
      "Epoch [3/3], Step [13200/41412], Loss: 2.8879, Perplexity: 17.95507\n",
      "Epoch [3/3], Step [13300/41412], Loss: 3.9878, Perplexity: 53.9335\n",
      "Epoch [3/3], Step [13400/41412], Loss: 2.4557, Perplexity: 11.6548\n",
      "Epoch [3/3], Step [13500/41412], Loss: 2.8077, Perplexity: 16.5723\n",
      "Epoch [3/3], Step [13600/41412], Loss: 2.3316, Perplexity: 10.2948\n",
      "Epoch [3/3], Step [13700/41412], Loss: 3.0114, Perplexity: 20.3156\n",
      "Epoch [3/3], Step [13800/41412], Loss: 2.4674, Perplexity: 11.7920\n",
      "Epoch [3/3], Step [13900/41412], Loss: 2.8494, Perplexity: 17.2781\n",
      "Epoch [3/3], Step [14000/41412], Loss: 3.8760, Perplexity: 48.2304\n",
      "Epoch [3/3], Step [14100/41412], Loss: 2.9514, Perplexity: 19.1333\n",
      "Epoch [3/3], Step [14200/41412], Loss: 2.9167, Perplexity: 18.4809\n",
      "Epoch [3/3], Step [14300/41412], Loss: 2.6237, Perplexity: 13.7864\n",
      "Epoch [3/3], Step [14400/41412], Loss: 2.9661, Perplexity: 19.4170\n",
      "Epoch [3/3], Step [14500/41412], Loss: 2.1312, Perplexity: 8.42480\n",
      "Epoch [3/3], Step [14600/41412], Loss: 2.9863, Perplexity: 19.8121\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/3], Step [14700/41412], Loss: 2.4601, Perplexity: 11.7063\n",
      "Epoch [3/3], Step [14800/41412], Loss: 2.9063, Perplexity: 18.2882\n",
      "Epoch [3/3], Step [14900/41412], Loss: 3.2531, Perplexity: 25.8703\n",
      "Epoch [3/3], Step [15000/41412], Loss: 3.1927, Perplexity: 24.3544\n",
      "Epoch [3/3], Step [15100/41412], Loss: 2.4703, Perplexity: 11.8261\n",
      "Epoch [3/3], Step [15200/41412], Loss: 2.1224, Perplexity: 8.35159\n",
      "Epoch [3/3], Step [15300/41412], Loss: 2.5045, Perplexity: 12.2380\n",
      "Epoch [3/3], Step [15400/41412], Loss: 2.1379, Perplexity: 8.48186\n",
      "Epoch [3/3], Step [15500/41412], Loss: 2.4397, Perplexity: 11.4698\n",
      "Epoch [3/3], Step [15600/41412], Loss: 2.9859, Perplexity: 19.80513\n",
      "Epoch [3/3], Step [15700/41412], Loss: 2.5928, Perplexity: 13.3676\n",
      "Epoch [3/3], Step [15800/41412], Loss: 2.7637, Perplexity: 15.8586\n",
      "Epoch [3/3], Step [15900/41412], Loss: 2.7654, Perplexity: 15.88567\n",
      "Epoch [3/3], Step [16000/41412], Loss: 3.2054, Perplexity: 24.6650\n",
      "Epoch [3/3], Step [16100/41412], Loss: 2.8205, Perplexity: 16.7860\n",
      "Epoch [3/3], Step [16200/41412], Loss: 2.7053, Perplexity: 14.9592\n",
      "Epoch [3/3], Step [16300/41412], Loss: 2.8824, Perplexity: 17.85757\n",
      "Epoch [3/3], Step [16400/41412], Loss: 2.6698, Perplexity: 14.4374\n",
      "Epoch [3/3], Step [16500/41412], Loss: 2.7757, Perplexity: 16.0497\n",
      "Epoch [3/3], Step [16600/41412], Loss: 2.8696, Perplexity: 17.6292\n",
      "Epoch [3/3], Step [16700/41412], Loss: 3.4295, Perplexity: 30.8615\n",
      "Epoch [3/3], Step [16800/41412], Loss: 2.0974, Perplexity: 8.14476\n",
      "Epoch [3/3], Step [16900/41412], Loss: 2.4145, Perplexity: 11.1841\n",
      "Epoch [3/3], Step [17000/41412], Loss: 3.1378, Perplexity: 23.0530\n",
      "Epoch [3/3], Step [17100/41412], Loss: 2.4100, Perplexity: 11.1338\n",
      "Epoch [3/3], Step [17200/41412], Loss: 1.9741, Perplexity: 7.20057\n",
      "Epoch [3/3], Step [17300/41412], Loss: 3.3456, Perplexity: 28.3778\n",
      "Epoch [3/3], Step [17400/41412], Loss: 2.7295, Perplexity: 15.3246\n",
      "Epoch [3/3], Step [17500/41412], Loss: 2.5446, Perplexity: 12.7385\n",
      "Epoch [3/3], Step [17600/41412], Loss: 2.9064, Perplexity: 18.2903\n",
      "Epoch [3/3], Step [17700/41412], Loss: 2.7930, Perplexity: 16.3298\n",
      "Epoch [3/3], Step [17800/41412], Loss: 3.1152, Perplexity: 22.5390\n",
      "Epoch [3/3], Step [17900/41412], Loss: 2.6848, Perplexity: 14.6551\n",
      "Epoch [3/3], Step [18000/41412], Loss: 2.3121, Perplexity: 10.0958\n",
      "Epoch [3/3], Step [18100/41412], Loss: 2.4977, Perplexity: 12.1547\n",
      "Epoch [3/3], Step [18200/41412], Loss: 2.6613, Perplexity: 14.3143\n",
      "Epoch [3/3], Step [18300/41412], Loss: 2.3825, Perplexity: 10.8319\n",
      "Epoch [3/3], Step [18400/41412], Loss: 2.2745, Perplexity: 9.72294\n",
      "Epoch [3/3], Step [18500/41412], Loss: 3.4255, Perplexity: 30.7385\n",
      "Epoch [3/3], Step [18600/41412], Loss: 3.2868, Perplexity: 26.7564\n",
      "Epoch [3/3], Step [18700/41412], Loss: 3.0169, Perplexity: 20.4282\n",
      "Epoch [3/3], Step [18800/41412], Loss: 2.6520, Perplexity: 14.1827\n",
      "Epoch [3/3], Step [18900/41412], Loss: 3.3370, Perplexity: 28.1338\n",
      "Epoch [3/3], Step [19000/41412], Loss: 2.6646, Perplexity: 14.3628\n",
      "Epoch [3/3], Step [19100/41412], Loss: 2.5368, Perplexity: 12.6391\n",
      "Epoch [3/3], Step [19200/41412], Loss: 2.8755, Perplexity: 17.7335\n",
      "Epoch [3/3], Step [19300/41412], Loss: 2.1156, Perplexity: 8.29475\n",
      "Epoch [3/3], Step [19400/41412], Loss: 3.1082, Perplexity: 22.3809\n",
      "Epoch [3/3], Step [19500/41412], Loss: 3.5390, Perplexity: 34.43272\n",
      "Epoch [3/3], Step [19600/41412], Loss: 3.1734, Perplexity: 23.8885\n",
      "Epoch [3/3], Step [19700/41412], Loss: 2.7061, Perplexity: 14.9715\n",
      "Epoch [3/3], Step [19800/41412], Loss: 2.2932, Perplexity: 9.906504\n",
      "Epoch [3/3], Step [19900/41412], Loss: 2.4562, Perplexity: 11.6607\n",
      "Epoch [3/3], Step [20000/41412], Loss: 2.4539, Perplexity: 11.6336\n",
      "Epoch [3/3], Step [20100/41412], Loss: 2.7508, Perplexity: 15.6552\n",
      "Epoch [3/3], Step [20200/41412], Loss: 2.4055, Perplexity: 11.0835\n",
      "Epoch [3/3], Step [20300/41412], Loss: 3.1402, Perplexity: 23.1092\n",
      "Epoch [3/3], Step [20400/41412], Loss: 2.7240, Perplexity: 15.2406\n",
      "Epoch [3/3], Step [20500/41412], Loss: 2.7391, Perplexity: 15.47348\n",
      "Epoch [3/3], Step [20600/41412], Loss: 3.0304, Perplexity: 20.7060\n",
      "Epoch [3/3], Step [20700/41412], Loss: 2.8976, Perplexity: 18.1306\n",
      "Epoch [3/3], Step [20800/41412], Loss: 2.3895, Perplexity: 10.9076\n",
      "Epoch [3/3], Step [20900/41412], Loss: 2.6577, Perplexity: 14.2639\n",
      "Epoch [3/3], Step [21000/41412], Loss: 2.9644, Perplexity: 19.3832\n",
      "Epoch [3/3], Step [21100/41412], Loss: 2.9588, Perplexity: 19.2750\n",
      "Epoch [3/3], Step [21200/41412], Loss: 2.6356, Perplexity: 13.9523\n",
      "Epoch [3/3], Step [21300/41412], Loss: 3.2698, Perplexity: 26.3065\n",
      "Epoch [3/3], Step [21400/41412], Loss: 2.9731, Perplexity: 19.5515\n",
      "Epoch [3/3], Step [21500/41412], Loss: 2.3823, Perplexity: 10.8301\n",
      "Epoch [3/3], Step [21600/41412], Loss: 2.9860, Perplexity: 19.8062\n",
      "Epoch [3/3], Step [21700/41412], Loss: 3.1690, Perplexity: 23.7835\n",
      "Epoch [3/3], Step [21800/41412], Loss: 2.9863, Perplexity: 19.8119\n",
      "Epoch [3/3], Step [21900/41412], Loss: 2.2145, Perplexity: 9.15681\n",
      "Epoch [3/3], Step [22000/41412], Loss: 2.8381, Perplexity: 17.0829\n",
      "Epoch [3/3], Step [22100/41412], Loss: 2.9072, Perplexity: 18.3047\n",
      "Epoch [3/3], Step [22200/41412], Loss: 2.8014, Perplexity: 16.4679\n",
      "Epoch [3/3], Step [22300/41412], Loss: 2.4493, Perplexity: 11.5805\n",
      "Epoch [3/3], Step [22400/41412], Loss: 3.0837, Perplexity: 21.8390\n",
      "Epoch [3/3], Step [22500/41412], Loss: 2.4460, Perplexity: 11.5417\n",
      "Epoch [3/3], Step [22600/41412], Loss: 2.6716, Perplexity: 14.4635\n",
      "Epoch [3/3], Step [22700/41412], Loss: 3.4549, Perplexity: 31.6549\n",
      "Epoch [3/3], Step [22800/41412], Loss: 2.8224, Perplexity: 16.8179\n",
      "Epoch [3/3], Step [22900/41412], Loss: 2.8138, Perplexity: 16.67311\n",
      "Epoch [3/3], Step [23000/41412], Loss: 3.6230, Perplexity: 37.4495\n",
      "Epoch [3/3], Step [23100/41412], Loss: 2.9411, Perplexity: 18.9371\n",
      "Epoch [3/3], Step [23200/41412], Loss: 3.1801, Perplexity: 24.0489\n",
      "Epoch [3/3], Step [23300/41412], Loss: 2.8484, Perplexity: 17.2598\n",
      "Epoch [3/3], Step [23400/41412], Loss: 2.5864, Perplexity: 13.2815\n",
      "Epoch [3/3], Step [23500/41412], Loss: 2.2585, Perplexity: 9.56874\n",
      "Epoch [3/3], Step [23600/41412], Loss: 2.1492, Perplexity: 8.57798\n",
      "Epoch [3/3], Step [23700/41412], Loss: 2.7376, Perplexity: 15.4495\n",
      "Epoch [3/3], Step [23800/41412], Loss: 2.1943, Perplexity: 8.97357\n",
      "Epoch [3/3], Step [23900/41412], Loss: 2.6615, Perplexity: 14.3177\n",
      "Epoch [3/3], Step [24000/41412], Loss: 2.8853, Perplexity: 17.9086\n",
      "Epoch [3/3], Step [24100/41412], Loss: 3.2913, Perplexity: 26.8778\n",
      "Epoch [3/3], Step [24200/41412], Loss: 2.5811, Perplexity: 13.2110\n",
      "Epoch [3/3], Step [24300/41412], Loss: 2.6348, Perplexity: 13.9404\n",
      "Epoch [3/3], Step [24400/41412], Loss: 3.1481, Perplexity: 23.2916\n",
      "Epoch [3/3], Step [24500/41412], Loss: 2.8695, Perplexity: 17.6279\n",
      "Epoch [3/3], Step [24600/41412], Loss: 2.7146, Perplexity: 15.09806\n",
      "Epoch [3/3], Step [24700/41412], Loss: 2.1538, Perplexity: 8.61722\n",
      "Epoch [3/3], Step [24800/41412], Loss: 3.1615, Perplexity: 23.6050\n",
      "Epoch [3/3], Step [24900/41412], Loss: 2.0850, Perplexity: 8.04492\n",
      "Epoch [3/3], Step [25000/41412], Loss: 2.6586, Perplexity: 14.2760\n",
      "Epoch [3/3], Step [25100/41412], Loss: 2.1214, Perplexity: 8.34260\n",
      "Epoch [3/3], Step [25200/41412], Loss: 2.8549, Perplexity: 17.3732\n",
      "Epoch [3/3], Step [25300/41412], Loss: 2.7879, Perplexity: 16.2467\n",
      "Epoch [3/3], Step [25400/41412], Loss: 2.1174, Perplexity: 8.30988\n",
      "Epoch [3/3], Step [25500/41412], Loss: 2.9793, Perplexity: 19.6737\n",
      "Epoch [3/3], Step [25600/41412], Loss: 3.5416, Perplexity: 34.5225\n",
      "Epoch [3/3], Step [25700/41412], Loss: 3.2595, Perplexity: 26.0373\n",
      "Epoch [3/3], Step [25800/41412], Loss: 2.5607, Perplexity: 12.94536\n",
      "Epoch [3/3], Step [25900/41412], Loss: 2.4977, Perplexity: 12.1548\n",
      "Epoch [3/3], Step [26000/41412], Loss: 2.7099, Perplexity: 15.0271\n",
      "Epoch [3/3], Step [26100/41412], Loss: 2.8746, Perplexity: 17.7178\n",
      "Epoch [3/3], Step [26200/41412], Loss: 2.4610, Perplexity: 11.7162\n",
      "Epoch [3/3], Step [26300/41412], Loss: 2.7325, Perplexity: 15.3707\n",
      "Epoch [3/3], Step [26400/41412], Loss: 3.5245, Perplexity: 33.9384\n",
      "Epoch [3/3], Step [26500/41412], Loss: 3.3221, Perplexity: 27.7185\n",
      "Epoch [3/3], Step [26600/41412], Loss: 2.8880, Perplexity: 17.9579\n",
      "Epoch [3/3], Step [26700/41412], Loss: 2.2546, Perplexity: 9.53192\n",
      "Epoch [3/3], Step [26800/41412], Loss: 2.6146, Perplexity: 13.66173\n",
      "Epoch [3/3], Step [26900/41412], Loss: 3.0344, Perplexity: 20.7883\n",
      "Epoch [3/3], Step [27000/41412], Loss: 2.0608, Perplexity: 7.85191\n",
      "Epoch [3/3], Step [27100/41412], Loss: 2.5653, Perplexity: 13.0049\n",
      "Epoch [3/3], Step [27200/41412], Loss: 3.0338, Perplexity: 20.7758\n",
      "Epoch [3/3], Step [27300/41412], Loss: 2.2866, Perplexity: 9.84110\n",
      "Epoch [3/3], Step [27400/41412], Loss: 2.4094, Perplexity: 11.1275\n",
      "Epoch [3/3], Step [27500/41412], Loss: 2.4483, Perplexity: 11.56884\n",
      "Epoch [3/3], Step [27600/41412], Loss: 2.7982, Perplexity: 16.4143\n",
      "Epoch [3/3], Step [27700/41412], Loss: 3.0599, Perplexity: 21.3263\n",
      "Epoch [3/3], Step [27800/41412], Loss: 2.9029, Perplexity: 18.2261\n",
      "Epoch [3/3], Step [27900/41412], Loss: 2.3289, Perplexity: 10.2666\n",
      "Epoch [3/3], Step [28000/41412], Loss: 2.7201, Perplexity: 15.1817\n",
      "Epoch [3/3], Step [28100/41412], Loss: 2.8604, Perplexity: 17.4690\n",
      "Epoch [3/3], Step [28200/41412], Loss: 2.3083, Perplexity: 10.0572\n",
      "Epoch [3/3], Step [28300/41412], Loss: 2.7512, Perplexity: 15.6611\n",
      "Epoch [3/3], Step [28400/41412], Loss: 3.0388, Perplexity: 20.8795\n",
      "Epoch [3/3], Step [28500/41412], Loss: 2.1923, Perplexity: 8.95578\n",
      "Epoch [3/3], Step [28600/41412], Loss: 2.6383, Perplexity: 13.9890\n",
      "Epoch [3/3], Step [28700/41412], Loss: 2.9827, Perplexity: 19.7414\n",
      "Epoch [3/3], Step [28800/41412], Loss: 3.1321, Perplexity: 22.9210\n",
      "Epoch [3/3], Step [28900/41412], Loss: 2.5424, Perplexity: 12.71038\n",
      "Epoch [3/3], Step [29000/41412], Loss: 2.4128, Perplexity: 11.1656\n",
      "Epoch [3/3], Step [29100/41412], Loss: 2.2055, Perplexity: 9.07508\n",
      "Epoch [3/3], Step [29200/41412], Loss: 2.7427, Perplexity: 15.5283\n",
      "Epoch [3/3], Step [29300/41412], Loss: 2.1483, Perplexity: 8.57065\n",
      "Epoch [3/3], Step [29400/41412], Loss: 2.7907, Perplexity: 16.2924\n",
      "Epoch [3/3], Step [29500/41412], Loss: 2.6715, Perplexity: 14.4614\n",
      "Epoch [3/3], Step [29600/41412], Loss: 2.6992, Perplexity: 14.8671\n",
      "Epoch [3/3], Step [29700/41412], Loss: 4.7268, Perplexity: 112.9391\n",
      "Epoch [3/3], Step [29800/41412], Loss: 3.1384, Perplexity: 23.0661\n",
      "Epoch [3/3], Step [29900/41412], Loss: 3.1583, Perplexity: 23.5300\n",
      "Epoch [3/3], Step [30000/41412], Loss: 2.9701, Perplexity: 19.4934\n",
      "Epoch [3/3], Step [30100/41412], Loss: 2.8143, Perplexity: 16.6811\n",
      "Epoch [3/3], Step [30200/41412], Loss: 3.0945, Perplexity: 22.0762\n",
      "Epoch [3/3], Step [30300/41412], Loss: 2.7015, Perplexity: 14.9021\n",
      "Epoch [3/3], Step [30400/41412], Loss: 2.7348, Perplexity: 15.4065\n",
      "Epoch [3/3], Step [30500/41412], Loss: 2.7035, Perplexity: 14.9319\n",
      "Epoch [3/3], Step [30600/41412], Loss: 2.9452, Perplexity: 19.0153\n",
      "Epoch [3/3], Step [30700/41412], Loss: 2.9515, Perplexity: 19.1346\n",
      "Epoch [3/3], Step [30800/41412], Loss: 2.7947, Perplexity: 16.3573\n",
      "Epoch [3/3], Step [30900/41412], Loss: 2.4152, Perplexity: 11.1923\n",
      "Epoch [3/3], Step [31000/41412], Loss: 2.7373, Perplexity: 15.4455\n",
      "Epoch [3/3], Step [31100/41412], Loss: 2.6469, Perplexity: 14.1105\n",
      "Epoch [3/3], Step [31200/41412], Loss: 2.7781, Perplexity: 16.0882\n",
      "Epoch [3/3], Step [31300/41412], Loss: 2.5964, Perplexity: 13.4153\n",
      "Epoch [3/3], Step [31400/41412], Loss: 2.6382, Perplexity: 13.9876\n",
      "Epoch [3/3], Step [31500/41412], Loss: 2.9527, Perplexity: 19.1569\n",
      "Epoch [3/3], Step [31600/41412], Loss: 3.6468, Perplexity: 38.3508\n",
      "Epoch [3/3], Step [31700/41412], Loss: 2.4211, Perplexity: 11.2579\n",
      "Epoch [3/3], Step [31800/41412], Loss: 3.0574, Perplexity: 21.2720\n",
      "Epoch [3/3], Step [31900/41412], Loss: 2.9029, Perplexity: 18.2263\n",
      "Epoch [3/3], Step [32000/41412], Loss: 2.5456, Perplexity: 12.75126\n",
      "Epoch [3/3], Step [32100/41412], Loss: 2.5995, Perplexity: 13.4566\n",
      "Epoch [3/3], Step [32200/41412], Loss: 2.3716, Perplexity: 10.7150\n",
      "Epoch [3/3], Step [32300/41412], Loss: 3.2673, Perplexity: 26.2402\n",
      "Epoch [3/3], Step [32400/41412], Loss: 2.4537, Perplexity: 11.6317\n",
      "Epoch [3/3], Step [32500/41412], Loss: 2.4482, Perplexity: 11.5678\n",
      "Epoch [3/3], Step [32600/41412], Loss: 2.2792, Perplexity: 9.76846\n",
      "Epoch [3/3], Step [32700/41412], Loss: 2.8710, Perplexity: 17.6547\n",
      "Epoch [3/3], Step [32800/41412], Loss: 2.6193, Perplexity: 13.7261\n",
      "Epoch [3/3], Step [32900/41412], Loss: 2.5912, Perplexity: 13.3453\n",
      "Epoch [3/3], Step [33000/41412], Loss: 2.5851, Perplexity: 13.2644\n",
      "Epoch [3/3], Step [33100/41412], Loss: 3.5193, Perplexity: 33.7611\n",
      "Epoch [3/3], Step [33200/41412], Loss: 2.4022, Perplexity: 11.0478\n",
      "Epoch [3/3], Step [33300/41412], Loss: 2.5095, Perplexity: 12.2990\n",
      "Epoch [3/3], Step [33400/41412], Loss: 3.0414, Perplexity: 20.9346\n",
      "Epoch [3/3], Step [33500/41412], Loss: 2.7575, Perplexity: 15.7607\n",
      "Epoch [3/3], Step [33600/41412], Loss: 2.4140, Perplexity: 11.1789\n",
      "Epoch [3/3], Step [33700/41412], Loss: 2.6955, Perplexity: 14.8124\n",
      "Epoch [3/3], Step [33800/41412], Loss: 3.3791, Perplexity: 29.3458\n",
      "Epoch [3/3], Step [33900/41412], Loss: 2.1901, Perplexity: 8.93610\n",
      "Epoch [3/3], Step [34000/41412], Loss: 2.7082, Perplexity: 15.0028\n",
      "Epoch [3/3], Step [34100/41412], Loss: 2.8392, Perplexity: 17.1014\n",
      "Epoch [3/3], Step [34200/41412], Loss: 2.9699, Perplexity: 19.48913\n",
      "Epoch [3/3], Step [34300/41412], Loss: 2.3954, Perplexity: 10.9725\n",
      "Epoch [3/3], Step [34400/41412], Loss: 3.1285, Perplexity: 22.8403\n",
      "Epoch [3/3], Step [34500/41412], Loss: 3.0983, Perplexity: 22.1596\n",
      "Epoch [3/3], Step [34600/41412], Loss: 2.5069, Perplexity: 12.2673\n",
      "Epoch [3/3], Step [34700/41412], Loss: 2.1808, Perplexity: 8.85336\n",
      "Epoch [3/3], Step [34800/41412], Loss: 2.4839, Perplexity: 11.9881\n",
      "Epoch [3/3], Step [34900/41412], Loss: 2.6067, Perplexity: 13.5540\n",
      "Epoch [3/3], Step [35000/41412], Loss: 2.8739, Perplexity: 17.7058\n",
      "Epoch [3/3], Step [35100/41412], Loss: 3.4162, Perplexity: 30.4541\n",
      "Epoch [3/3], Step [35200/41412], Loss: 2.8210, Perplexity: 16.7942\n",
      "Epoch [3/3], Step [35300/41412], Loss: 3.0889, Perplexity: 21.9539\n",
      "Epoch [3/3], Step [35400/41412], Loss: 3.0410, Perplexity: 20.9256\n",
      "Epoch [3/3], Step [35500/41412], Loss: 3.0698, Perplexity: 21.5366\n",
      "Epoch [3/3], Step [35600/41412], Loss: 2.5432, Perplexity: 12.7207\n",
      "Epoch [3/3], Step [35700/41412], Loss: 3.0948, Perplexity: 22.0829\n",
      "Epoch [3/3], Step [35800/41412], Loss: 2.9624, Perplexity: 19.3443\n",
      "Epoch [3/3], Step [35900/41412], Loss: 2.3383, Perplexity: 10.3640\n",
      "Epoch [3/3], Step [36000/41412], Loss: 3.7590, Perplexity: 42.90352\n",
      "Epoch [3/3], Step [36100/41412], Loss: 3.2867, Perplexity: 26.7543\n",
      "Epoch [3/3], Step [36200/41412], Loss: 2.6814, Perplexity: 14.6059\n",
      "Epoch [3/3], Step [36300/41412], Loss: 1.9925, Perplexity: 7.33407\n",
      "Epoch [3/3], Step [36400/41412], Loss: 1.6803, Perplexity: 5.36729\n",
      "Epoch [3/3], Step [36500/41412], Loss: 2.2917, Perplexity: 9.89197\n",
      "Epoch [3/3], Step [36600/41412], Loss: 3.0060, Perplexity: 20.2064\n",
      "Epoch [3/3], Step [36700/41412], Loss: 2.7445, Perplexity: 15.5569\n",
      "Epoch [3/3], Step [36800/41412], Loss: 3.7248, Perplexity: 41.4636\n",
      "Epoch [3/3], Step [36900/41412], Loss: 2.5515, Perplexity: 12.8267\n",
      "Epoch [3/3], Step [37000/41412], Loss: 2.6012, Perplexity: 13.4798\n",
      "Epoch [3/3], Step [37100/41412], Loss: 2.2676, Perplexity: 9.65590\n",
      "Epoch [3/3], Step [37200/41412], Loss: 2.7004, Perplexity: 14.8852\n",
      "Epoch [3/3], Step [37300/41412], Loss: 2.2866, Perplexity: 9.84170\n",
      "Epoch [3/3], Step [37400/41412], Loss: 3.3616, Perplexity: 28.8347\n",
      "Epoch [3/3], Step [37500/41412], Loss: 2.2810, Perplexity: 9.78687\n",
      "Epoch [3/3], Step [37600/41412], Loss: 2.4215, Perplexity: 11.2622\n",
      "Epoch [3/3], Step [37700/41412], Loss: 2.9580, Perplexity: 19.2602\n",
      "Epoch [3/3], Step [37800/41412], Loss: 2.8590, Perplexity: 17.4449\n",
      "Epoch [3/3], Step [37900/41412], Loss: 2.6292, Perplexity: 13.8627\n",
      "Epoch [3/3], Step [38000/41412], Loss: 2.4793, Perplexity: 11.9326\n",
      "Epoch [3/3], Step [38100/41412], Loss: 2.9654, Perplexity: 19.4015\n",
      "Epoch [3/3], Step [38200/41412], Loss: 2.8431, Perplexity: 17.1682\n",
      "Epoch [3/3], Step [38300/41412], Loss: 2.3500, Perplexity: 10.4854\n",
      "Epoch [3/3], Step [38400/41412], Loss: 2.4160, Perplexity: 11.2007\n",
      "Epoch [3/3], Step [38500/41412], Loss: 2.5049, Perplexity: 12.2419\n",
      "Epoch [3/3], Step [38600/41412], Loss: 2.4241, Perplexity: 11.2925\n",
      "Epoch [3/3], Step [38700/41412], Loss: 2.8565, Perplexity: 17.3998\n",
      "Epoch [3/3], Step [38800/41412], Loss: 2.2297, Perplexity: 9.29699\n",
      "Epoch [3/3], Step [38900/41412], Loss: 3.5819, Perplexity: 35.94196\n",
      "Epoch [3/3], Step [39000/41412], Loss: 3.0590, Perplexity: 21.30607\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/3], Step [39100/41412], Loss: 2.0651, Perplexity: 7.88630\n",
      "Epoch [3/3], Step [39200/41412], Loss: 3.3627, Perplexity: 28.8659\n",
      "Epoch [3/3], Step [39300/41412], Loss: 3.1308, Perplexity: 22.8920\n",
      "Epoch [3/3], Step [39400/41412], Loss: 2.4762, Perplexity: 11.8959\n",
      "Epoch [3/3], Step [39500/41412], Loss: 3.8608, Perplexity: 47.5027\n",
      "Epoch [3/3], Step [39600/41412], Loss: 2.1311, Perplexity: 8.42379\n",
      "Epoch [3/3], Step [39700/41412], Loss: 2.5049, Perplexity: 12.2418\n",
      "Epoch [3/3], Step [39800/41412], Loss: 2.6221, Perplexity: 13.7646\n",
      "Epoch [3/3], Step [39900/41412], Loss: 2.1072, Perplexity: 8.22533\n",
      "Epoch [3/3], Step [40000/41412], Loss: 2.7113, Perplexity: 15.0495\n",
      "Epoch [3/3], Step [40100/41412], Loss: 2.8108, Perplexity: 16.6237\n",
      "Epoch [3/3], Step [40200/41412], Loss: 2.3347, Perplexity: 10.3268\n",
      "Epoch [3/3], Step [40300/41412], Loss: 2.4702, Perplexity: 11.8243\n",
      "Epoch [3/3], Step [40400/41412], Loss: 3.1435, Perplexity: 23.1843\n",
      "Epoch [3/3], Step [40500/41412], Loss: 3.0834, Perplexity: 21.8332\n",
      "Epoch [3/3], Step [40600/41412], Loss: 2.8981, Perplexity: 18.1388\n",
      "Epoch [3/3], Step [40700/41412], Loss: 2.3006, Perplexity: 9.98050\n",
      "Epoch [3/3], Step [40800/41412], Loss: 2.5238, Perplexity: 12.4754\n",
      "Epoch [3/3], Step [40900/41412], Loss: 2.9468, Perplexity: 19.0449\n",
      "Epoch [3/3], Step [41000/41412], Loss: 2.8791, Perplexity: 17.7988\n",
      "Epoch [3/3], Step [41100/41412], Loss: 2.5663, Perplexity: 13.0170\n",
      "Epoch [3/3], Step [41200/41412], Loss: 2.3749, Perplexity: 10.7502\n",
      "Epoch [3/3], Step [41300/41412], Loss: 2.7052, Perplexity: 14.9579\n",
      "Epoch [3/3], Step [41400/41412], Loss: 2.1658, Perplexity: 8.72145\n",
      "Epoch [3/3], Step [41412/41412], Loss: 2.4374, Perplexity: 11.4430"
     ]
    }
   ],
   "source": [
    "import torch.utils.data as data\n",
    "import numpy as np\n",
    "import os\n",
    "import requests\n",
    "import time\n",
    "\n",
    "# temporary\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from model import EncoderCNN, DecoderRNN\n",
    "\n",
    "# Open the training log file.\n",
    "f = open(log_file, 'w')\n",
    "\n",
    "## running the training locally\n",
    "#old_time = time.time()\n",
    "#response = requests.request(\"GET\", \n",
    "#                            \"http://metadata.google.internal/computeMetadata/v1/instance/attributes/keep_alive_token\", \n",
    "#                            headers={\"Metadata-Flavor\":\"Google\"})\n",
    "\n",
    "\n",
    "for epoch in range(1, num_epochs+1):\n",
    "\n",
    "    for i_step in range(1, total_step+1):\n",
    "        \n",
    "        ## running the training locally\n",
    "        #if time.time() - old_time > 60:\n",
    "        #    old_time = time.time()\n",
    "        #    requests.request(\"POST\", \n",
    "        #                     \"https://nebula.udacity.com/api/v1/remote/keep-alive\", \n",
    "        #                     headers={'Authorization': \"STAR \" + response.text})\n",
    "\n",
    "        # Randomly sample a caption length, and sample indices with that length.\n",
    "        indices = data_loader.dataset.get_train_indices()\n",
    "        # Create and assign a batch sampler to retrieve a batch with the sampled indices.\n",
    "        new_sampler = data.sampler.SubsetRandomSampler(indices=indices)\n",
    "        data_loader.batch_sampler.sampler = new_sampler\n",
    "\n",
    "        # Obtain the batch.\n",
    "        images, captions = next(iter(data_loader))\n",
    "\n",
    "        # Move batch of images and captions to GPU if CUDA is available.\n",
    "        images = images.to(device)\n",
    "        captions = captions.to(device)\n",
    "\n",
    "        # Zero the gradients.\n",
    "        decoder.zero_grad()\n",
    "        encoder.zero_grad()\n",
    "\n",
    "        # Pass the inputs through the CNN-RNN model.\n",
    "        features = encoder(images)\n",
    "        outputs = decoder(features, captions)\n",
    "\n",
    "        # Calculate the batch loss.\n",
    "        loss = criterion(outputs.contiguous().view(-1, vocab_size), captions.contiguous().view(-1))\n",
    "\n",
    "        # Backward pass.\n",
    "        loss.backward(retain_graph=True)\n",
    "\n",
    "        # Update the parameters in the optimizer.\n",
    "        optimizer.step()\n",
    "\n",
    "        # Get training statistics.\n",
    "        stats = 'Epoch [%d/%d], Step [%d/%d], Loss: %.4f, Perplexity: %5.4f' % (epoch, num_epochs, i_step, total_step, loss.item(), np.exp(loss.item()))\n",
    "\n",
    "        # Print training statistics (on same line).\n",
    "        print('\\r' + stats, end=\"\")\n",
    "        sys.stdout.flush()\n",
    "\n",
    "        # Print training statistics to file.\n",
    "        f.write(stats + '\\n')\n",
    "        f.flush()\n",
    "\n",
    "        # Print training statistics (on different line).\n",
    "        if i_step % print_every == 0:\n",
    "            print('\\r' + stats)\n",
    "\n",
    "    # Save the weights.\n",
    "    if epoch % save_every == 0:\n",
    "        torch.save(decoder.state_dict(), os.path.join('./models', 'decoder-%d.pkl' % epoch))\n",
    "        torch.save(encoder.state_dict(), os.path.join('./models', 'encoder-%d.pkl' % epoch))\n",
    "\n",
    "# Close the training log file.\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step3'></a>\n",
    "## Step 3: (Optional) Validate your Model\n",
    "\n",
    "To assess potential overfitting, one approach is to assess performance on a validation set.  If you decide to do this **optional** task, you are required to first complete all of the steps in the next notebook in the sequence (**3_Inference.ipynb**); as part of that notebook, you will write and test code (specifically, the `sample` method in the `DecoderRNN` class) that uses your RNN decoder to generate captions.  That code will prove incredibly useful here. \n",
    "\n",
    "If you decide to validate your model, please do not edit the data loader in **data_loader.py**.  Instead, create a new file named **data_loader_val.py** containing the code for obtaining the data loader for the validation data.  You can access:\n",
    "- the validation images at filepath `'/opt/cocoapi/images/train2014/'`, and\n",
    "- the validation image caption annotation file at filepath `'/opt/cocoapi/annotations/captions_val2014.json'`.\n",
    "\n",
    "The suggested approach to validating your model involves creating a json file such as [this one](https://github.com/cocodataset/cocoapi/blob/master/results/captions_val2014_fakecap_results.json) containing your model's predicted captions for the validation images.  Then, you can write your own script or use one that you [find online](https://github.com/tylin/coco-caption) to calculate the BLEU score of your model.  You can read more about the BLEU score, along with other evaluation metrics (such as TEOR and Cider) in section 4.1 of [this paper](https://arxiv.org/pdf/1411.4555.pdf).  For more information about how to use the annotation file, check out the [website](http://cocodataset.org/#download) for the COCO dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Optional) TODO: Validate your model."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
